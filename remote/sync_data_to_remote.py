"""
æ–‡ä»¶å: sync_data_to_remote.py
æ•°æ®åŒæ­¥åˆ°è¿œç¨‹å­˜å‚¨
æ”¯æŒå¤šç§äº‘å­˜å‚¨å¹³å°çš„æ•°æ®ä¸Šä¼ å’Œä¸‹è½½
"""
import os
import json
import logging
import hashlib
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime

import sys

# è®¡ç®—é¡¹ç›®æ ¹ç›®å½•ï¼ˆremote/ çš„ä¸Šä¸€çº§ï¼‰ï¼Œç¡®ä¿å¯ä»¥å¯¼å…¥ src/ å’Œ scripts/
PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))
if str(PROJECT_ROOT / "src") not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT / "src"))
if str(PROJECT_ROOT / "scripts") not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT / "scripts"))

# äº‘å­˜å‚¨å®¢æˆ·ç«¯å¯¼å…¥
try:
    import boto3  # AWS S3
    from botocore.exceptions import ClientError
    AWS_AVAILABLE = True
except ImportError:
    AWS_AVAILABLE = False

try:
    from google.cloud import storage as gcs  # Google Cloud Storage
    GCP_AVAILABLE = True
except ImportError:
    GCP_AVAILABLE = False

try:
    from azure.storage.blob import BlobServiceClient  # Azure Blob Storage
    AZURE_AVAILABLE = True
except ImportError:
    AZURE_AVAILABLE = False

try:
    import oss2  # é˜¿é‡Œäº‘OSS
    ALIYUN_AVAILABLE = True
except ImportError:
    ALIYUN_AVAILABLE = False

from remote.remote_training_config import RemoteTrainingConfig


class CloudStorageManager:
    """äº‘å­˜å‚¨ç®¡ç†å™¨"""
    
    def __init__(self, config: RemoteTrainingConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.client = None
        
        # åˆå§‹åŒ–äº‘å­˜å‚¨å®¢æˆ·ç«¯
        self._initialize_client()
    
    def _initialize_client(self):
        """åˆå§‹åŒ–äº‘å­˜å‚¨å®¢æˆ·ç«¯"""
        provider = self.config.cloud_provider.lower()
        
        try:
            if provider == 'aws' and AWS_AVAILABLE:
                self._init_aws_client()
            elif provider == 'gcp' and GCP_AVAILABLE:
                self._init_gcp_client()
            elif provider == 'azure' and AZURE_AVAILABLE:
                self._init_azure_client()
            elif provider == 'aliyun' and ALIYUN_AVAILABLE:
                self._init_aliyun_client()
            elif provider == 'local':
                self.logger.info("ä½¿ç”¨æœ¬åœ°å­˜å‚¨")
            else:
                self.logger.warning(f"ä¸æ”¯æŒçš„äº‘å­˜å‚¨æä¾›å•†: {provider}")
                
        except Exception as e:
            self.logger.error(f"äº‘å­˜å‚¨å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def _init_aws_client(self):
        """åˆå§‹åŒ–AWS S3å®¢æˆ·ç«¯"""
        self.client = boto3.client(
            's3',
            aws_access_key_id=self.config.cloud_access_key,
            aws_secret_access_key=self.config.cloud_secret_key,
            region_name=self.config.cloud_region
        )
        self.logger.info("AWS S3å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
    
    def _init_gcp_client(self):
        """åˆå§‹åŒ–Google Cloud Storageå®¢æˆ·ç«¯"""
        self.client = gcs.Client()
        self.logger.info("Google Cloud Storageå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
    
    def _init_azure_client(self):
        """åˆå§‹åŒ–Azure Blob Storageå®¢æˆ·ç«¯"""
        connection_string = f"DefaultEndpointsProtocol=https;AccountName={self.config.cloud_access_key};AccountKey={self.config.cloud_secret_key};EndpointSuffix=core.windows.net"
        self.client = BlobServiceClient.from_connection_string(connection_string)
        self.logger.info("Azure Blob Storageå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
    
    def _init_aliyun_client(self):
        """åˆå§‹åŒ–é˜¿é‡Œäº‘OSSå®¢æˆ·ç«¯"""
        auth = oss2.Auth(self.config.cloud_access_key, self.config.cloud_secret_key)
        endpoint = f"https://oss-{self.config.cloud_region}.aliyuncs.com"
        self.client = oss2.Bucket(auth, endpoint, self.config.cloud_bucket)
        self.logger.info("é˜¿é‡Œäº‘OSSå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
    
    def upload_file(self, local_path: str, remote_path: str) -> bool:
        """ä¸Šä¼ æ–‡ä»¶åˆ°äº‘å­˜å‚¨"""
        if self.config.cloud_provider == 'local':
            return self._copy_local_file(local_path, remote_path)
        
        try:
            provider = self.config.cloud_provider.lower()
            
            if provider == 'aws':
                return self._upload_to_s3(local_path, remote_path)
            elif provider == 'gcp':
                return self._upload_to_gcs(local_path, remote_path)
            elif provider == 'azure':
                return self._upload_to_azure(local_path, remote_path)
            elif provider == 'aliyun':
                return self._upload_to_oss(local_path, remote_path)
            
        except Exception as e:
            self.logger.error(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {e}")
            return False
        
        return False
    
    def download_file(self, remote_path: str, local_path: str) -> bool:
        """ä»äº‘å­˜å‚¨ä¸‹è½½æ–‡ä»¶"""
        if self.config.cloud_provider == 'local':
            return self._copy_local_file(remote_path, local_path)
        
        try:
            provider = self.config.cloud_provider.lower()
            
            if provider == 'aws':
                return self._download_from_s3(remote_path, local_path)
            elif provider == 'gcp':
                return self._download_from_gcs(remote_path, local_path)
            elif provider == 'azure':
                return self._download_from_azure(remote_path, local_path)
            elif provider == 'aliyun':
                return self._download_from_oss(remote_path, local_path)
            
        except Exception as e:
            self.logger.error(f"æ–‡ä»¶ä¸‹è½½å¤±è´¥: {e}")
            return False
        
        return False
    
    def _upload_to_s3(self, local_path: str, remote_path: str) -> bool:
        """ä¸Šä¼ åˆ°AWS S3"""
        try:
            self.client.upload_file(local_path, self.config.cloud_bucket, remote_path)
            self.logger.info(f"æ–‡ä»¶ä¸Šä¼ åˆ°S3æˆåŠŸ: {remote_path}")
            return True
        except ClientError as e:
            self.logger.error(f"S3ä¸Šä¼ å¤±è´¥: {e}")
            return False
    
    def _upload_to_gcs(self, local_path: str, remote_path: str) -> bool:
        """ä¸Šä¼ åˆ°Google Cloud Storage"""
        try:
            bucket = self.client.bucket(self.config.cloud_bucket)
            blob = bucket.blob(remote_path)
            blob.upload_from_filename(local_path)
            self.logger.info(f"æ–‡ä»¶ä¸Šä¼ åˆ°GCSæˆåŠŸ: {remote_path}")
            return True
        except Exception as e:
            self.logger.error(f"GCSä¸Šä¼ å¤±è´¥: {e}")
            return False
    
    def _upload_to_azure(self, local_path: str, remote_path: str) -> bool:
        """ä¸Šä¼ åˆ°Azure Blob Storage"""
        try:
            blob_client = self.client.get_blob_client(
                container=self.config.cloud_bucket,
                blob=remote_path
            )
            with open(local_path, 'rb') as data:
                blob_client.upload_blob(data, overwrite=True)
            self.logger.info(f"æ–‡ä»¶ä¸Šä¼ åˆ°AzureæˆåŠŸ: {remote_path}")
            return True
        except Exception as e:
            self.logger.error(f"Azureä¸Šä¼ å¤±è´¥: {e}")
            return False
    
    def _upload_to_oss(self, local_path: str, remote_path: str) -> bool:
        """ä¸Šä¼ åˆ°é˜¿é‡Œäº‘OSS"""
        try:
            self.client.put_object_from_file(remote_path, local_path)
            self.logger.info(f"æ–‡ä»¶ä¸Šä¼ åˆ°OSSæˆåŠŸ: {remote_path}")
            return True
        except Exception as e:
            self.logger.error(f"OSSä¸Šä¼ å¤±è´¥: {e}")
            return False
    
    def _copy_local_file(self, src: str, dst: str) -> bool:
        """æœ¬åœ°æ–‡ä»¶å¤åˆ¶"""
        try:
            import shutil
            os.makedirs(os.path.dirname(dst), exist_ok=True)
            shutil.copy2(src, dst)
            self.logger.info(f"æœ¬åœ°æ–‡ä»¶å¤åˆ¶æˆåŠŸ: {src} -> {dst}")
            return True
        except Exception as e:
            self.logger.error(f"æœ¬åœ°æ–‡ä»¶å¤åˆ¶å¤±è´¥: {e}")
            return False


class DataSyncManager:
    """æ•°æ®åŒæ­¥ç®¡ç†å™¨"""
    
    def __init__(self, config: RemoteTrainingConfig):
        self.config = config
        self.storage_manager = CloudStorageManager(config)
        self.logger = logging.getLogger(__name__)
    
    def sync_training_data(self, force_regenerate: bool = False) -> bool:
        """åŒæ­¥è®­ç»ƒæ•°æ®åˆ°äº‘ç«¯"""
        self.logger.info("ğŸ”„ å¼€å§‹åŒæ­¥è®­ç»ƒæ•°æ®...")
        
        try:
            # æ£€æŸ¥æœ¬åœ°æ•°æ®æ˜¯å¦å­˜åœ¨
            local_train_data = "data/train_level_3_é²æ£’ç‰ˆ.json"
            local_val_data = "data/val_level_3_é²æ£’ç‰ˆ.json"
            
            if force_regenerate or not os.path.exists(local_train_data) or not os.path.exists(local_val_data):
                self.logger.info("ğŸ“Š ç”Ÿæˆæ–°çš„è®­ç»ƒæ•°æ®...")
                self._generate_training_data()
            
            # ä¸Šä¼ è®­ç»ƒæ•°æ®
            remote_train_path = f"data/{self.config.train_data_file}"
            remote_val_path = f"data/{self.config.val_data_file}"
            
            success = True
            success &= self.storage_manager.upload_file(local_train_data, remote_train_path)
            success &= self.storage_manager.upload_file(local_val_data, remote_val_path)
            
            if success:
                self.logger.info("âœ… è®­ç»ƒæ•°æ®åŒæ­¥æˆåŠŸ")
                return True
            else:
                self.logger.error("âŒ è®­ç»ƒæ•°æ®åŒæ­¥å¤±è´¥")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®åŒæ­¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    def _generate_training_data(self):
        """ç”Ÿæˆè®­ç»ƒæ•°æ®"""
        try:
            # å¯¼å…¥å½“å‰æ¨èçš„æ•°æ®ç”Ÿæˆå‡½æ•°ï¼ˆä½äº scripts/ ç›®å½•ï¼‰
            from scripts.generate_robust_dataset import generate_robust_dataset

            # ç›´æ¥ç”Ÿæˆå¤æ‚åº¦ä¸º "complex" çš„é²æ£’æ•°æ®é›†ï¼Œå¯¹åº”åŸæ¥çš„ L3 çº§åˆ«
            train_data = generate_robust_dataset(size=10000, complexity_level="complex")
            val_data = generate_robust_dataset(size=2000, complexity_level="complex")

            # ä¿å­˜æ•°æ®
            os.makedirs("data", exist_ok=True)

            with open("data/train_level_3_é²æ£’ç‰ˆ.json", "w", encoding="utf-8") as f:
                json.dump(train_data, f, ensure_ascii=False, indent=2)

            with open("data/val_level_3_é²æ£’ç‰ˆ.json", "w", encoding="utf-8") as f:
                json.dump(val_data, f, ensure_ascii=False, indent=2)

            self.logger.info(
                f"âœ… ç”Ÿæˆè®­ç»ƒæ•°æ®: {len(train_data)} è®­ç»ƒæ ·æœ¬, {len(val_data)} éªŒè¯æ ·æœ¬"
            )
            
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®ç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    def download_training_data(self) -> bool:
        """ä»äº‘ç«¯ä¸‹è½½è®­ç»ƒæ•°æ®"""
        self.logger.info("â¬‡ï¸ ä»äº‘ç«¯ä¸‹è½½è®­ç»ƒæ•°æ®...")
        
        try:
            # åˆ›å»ºæœ¬åœ°æ•°æ®ç›®å½•
            os.makedirs(self.config.remote_data_path, exist_ok=True)
            
            # ä¸‹è½½æ–‡ä»¶
            remote_train_path = f"data/{self.config.train_data_file}"
            remote_val_path = f"data/{self.config.val_data_file}"
            
            local_train_path = os.path.join(self.config.remote_data_path, self.config.train_data_file)
            local_val_path = os.path.join(self.config.remote_data_path, self.config.val_data_file)
            
            success = True
            success &= self.storage_manager.download_file(remote_train_path, local_train_path)
            success &= self.storage_manager.download_file(remote_val_path, local_val_path)
            
            if success:
                self.logger.info("âœ… è®­ç»ƒæ•°æ®ä¸‹è½½æˆåŠŸ")
                return True
            else:
                self.logger.error("âŒ è®­ç»ƒæ•°æ®ä¸‹è½½å¤±è´¥")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®ä¸‹è½½è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    def sync_model_checkpoints(self, checkpoint_dir: str) -> bool:
        """åŒæ­¥æ¨¡å‹æ£€æŸ¥ç‚¹"""
        self.logger.info("ğŸ’¾ åŒæ­¥æ¨¡å‹æ£€æŸ¥ç‚¹...")
        
        try:
            if not os.path.exists(checkpoint_dir):
                self.logger.warning(f"æ£€æŸ¥ç‚¹ç›®å½•ä¸å­˜åœ¨: {checkpoint_dir}")
                return True
            
            success_count = 0
            total_count = 0
            
            for file_name in os.listdir(checkpoint_dir):
                if file_name.endswith(('.npz', '.json')):
                    local_path = os.path.join(checkpoint_dir, file_name)
                    remote_path = f"checkpoints/{file_name}"
                    
                    total_count += 1
                    if self.storage_manager.upload_file(local_path, remote_path):
                        success_count += 1
            
            self.logger.info(f"ğŸ“Š æ£€æŸ¥ç‚¹åŒæ­¥å®Œæˆ: {success_count}/{total_count}")
            return success_count == total_count
            
        except Exception as e:
            self.logger.error(f"âŒ æ£€æŸ¥ç‚¹åŒæ­¥å¤±è´¥: {e}")
            return False


def main():
    """ä¸»å‡½æ•° - æ•°æ®åŒæ­¥å·¥å…·"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æ•°æ®åŒæ­¥å·¥å…·")
    parser.add_argument('--action', choices=['upload', 'download', 'sync-checkpoints'], 
                       required=True, help="æ“ä½œç±»å‹")
    parser.add_argument('--config', help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument('--force-regenerate', action='store_true', 
                       help="å¼ºåˆ¶é‡æ–°ç”Ÿæˆæ•°æ®")
    parser.add_argument('--checkpoint-dir', default='outputs/checkpoints',
                       help="æ£€æŸ¥ç‚¹ç›®å½•")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    # åˆ›å»ºé…ç½®
    config = RemoteTrainingConfig(args.config)
    
    # åˆ›å»ºæ•°æ®åŒæ­¥ç®¡ç†å™¨
    sync_manager = DataSyncManager(config)
    
    # æ‰§è¡Œæ“ä½œ
    if args.action == 'upload':
        success = sync_manager.sync_training_data(args.force_regenerate)
    elif args.action == 'download':
        success = sync_manager.download_training_data()
    elif args.action == 'sync-checkpoints':
        success = sync_manager.sync_model_checkpoints(args.checkpoint_dir)
    
    if success:
        print("âœ… æ“ä½œæˆåŠŸå®Œæˆ")
    else:
        print("âŒ æ“ä½œå¤±è´¥")
        exit(1)


if __name__ == "__main__":
    main()
