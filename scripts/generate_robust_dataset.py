"""
æ–‡ä»¶å: generate_robust_dataset.py
ç”Ÿæˆ"æ— æ³•ä½œå¼Š"çš„é²æ£’æ•°æ®é›†
å¢åŠ æ•°æ®çš„ç†µå’Œå¤šæ ·æ€§ï¼Œå µæ­»æ‰€æœ‰ä½œå¼Šæ·å¾„
"""

import json
import random
import os
import re
import sys
from pathlib import Path

# ç¡®ä¿å¯ä»¥ä» scripts/ ç›®å½•å¯¼å…¥é¡¹ç›®æ ¹æ¨¡å—
PROJECT_ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(PROJECT_ROOT))

from logic_utils import (
    generate_simple_proposition,
    generate_recursive_implication,
    to_contrapositive,
    verify_equivalence,
)


def add_robust_noise(prop_str: str, num_applications: int = 3) -> str:
    """æ·»åŠ é²æ£’çš„ã€å¤šæ ·åŒ–çš„å™ªå£°ï¼Œç¡®ä¿æ²¡æœ‰ç®€å•çš„å­—ç¬¦ä¸²å¯¹åº”å…³ç³»"""
    result = prop_str

    # å®šä¹‰æ‰€æœ‰å¯ç”¨çš„å™ªå£°å˜æ¢
    noise_functions = [
        add_noise_type1_robust,
        add_noise_type2_robust,
        add_noise_type3_robust,
        add_noise_type4_robust,
        add_noise_type5_new,
        add_noise_type6_new,
    ]

    # éšæœºåº”ç”¨å¤šæ¬¡å™ªå£°
    for _ in range(num_applications):
        noise_func = random.choice(noise_functions)
        try:
            result = noise_func(result)
        except Exception:
            continue  # å¦‚æœæŸä¸ªå™ªå£°å‡½æ•°å¤±è´¥ï¼Œç»§ç»­å°è¯•å…¶ä»–çš„

    return result


def add_noise_type1_robust(prop_str: str) -> str:
    """é²æ£’ç‰ˆå™ªå£°ç±»å‹1ï¼šè•´å«è½¬æå–ï¼Œå¢åŠ éšæœºæ€§"""
    # éšæœºå†³å®šæ˜¯å¦åº”ç”¨
    if random.random() < 0.7:
        # æ‰¾åˆ°ä¸»è•´å«ç¬¦å¹¶è½¬æ¢
        if " -> " in prop_str:
            parts = prop_str.split(" -> ", 1)
            if len(parts) == 2:
                antecedent = parts[0].strip()
                consequent = parts[1].strip()

                # éšæœºé€‰æ‹©å¦å®šæ–¹å¼
                if random.random() < 0.5:
                    neg_antecedent = (
                        f"~({antecedent})" if " " in antecedent else f"~{antecedent}"
                    )
                else:
                    neg_antecedent = f"~{antecedent}"

                return f"({neg_antecedent} | {consequent})"

    return prop_str


def add_noise_type2_robust(prop_str: str) -> str:
    """é²æ£’ç‰ˆå™ªå£°ç±»å‹2ï¼šåŒé‡å¦å®šï¼Œéšæœºåº”ç”¨åˆ°ä¸åŒä½ç½®"""
    variables = re.findall(r"\b[pqrst]\b", prop_str)
    if variables and random.random() < 0.6:
        # éšæœºé€‰æ‹©1-2ä¸ªå˜é‡
        num_vars = random.randint(1, min(2, len(variables)))
        selected_vars = random.sample(variables, num_vars)

        for var in selected_vars:
            # éšæœºé€‰æ‹©åŒé‡å¦å®šçš„å½¢å¼
            if random.random() < 0.5:
                replacement = f"~~{var}"
            else:
                replacement = f"~(~{var})"

            prop_str = re.sub(rf"\b{var}\b", replacement, prop_str, count=1)

    return prop_str


def add_noise_type3_robust(prop_str: str) -> str:
    """é²æ£’ç‰ˆå™ªå£°ç±»å‹3ï¼šå†—ä½™æ‹¬å·ï¼Œéšæœºåº”ç”¨"""
    variables = re.findall(r"\b[pqrst]\b", prop_str)
    if variables and random.random() < 0.5:
        var = random.choice(variables)
        # éšæœºé€‰æ‹©æ‹¬å·çš„å½¢å¼
        if random.random() < 0.5:
            replacement = f"({var})"
        else:
            replacement = f"(({var}))"

        prop_str = re.sub(rf"\b{var}\b", replacement, prop_str, count=1)

    return prop_str


def add_noise_type4_robust(prop_str: str) -> str:
    """é²æ£’ç‰ˆå™ªå£°ç±»å‹4ï¼šäº¤æ¢å¾‹ï¼Œéšæœºåº”ç”¨"""
    # æŸ¥æ‰¾å¯äº¤æ¢çš„è¡¨è¾¾å¼
    patterns = [
        (r"\(([^()]+)\s*&\s*([^()]+)\)", r"(\2 & \1)"),
        (r"\(([^()]+)\s*\|\s*([^()]+)\)", r"(\2 | \1)"),
    ]

    if random.random() < 0.4:
        for pattern, replacement in patterns:
            if re.search(pattern, prop_str):
                prop_str = re.sub(pattern, replacement, prop_str, count=1)
                break

    return prop_str


def add_noise_type5_new(prop_str: str) -> str:
    """æ–°å™ªå£°ç±»å‹5ï¼šæ·»åŠ æ’çœŸ/æ’å‡è¡¨è¾¾å¼"""
    if random.random() < 0.3:
        variables = re.findall(r"\b[pqrst]\b", prop_str)
        if variables:
            var = random.choice(variables)
            # æ·»åŠ æ’çœŸæˆ–æ’å‡è¡¨è¾¾å¼
            if random.random() < 0.5:
                tautology = f"({var} | ~{var})"  # æ’çœŸ
                prop_str = f"({prop_str} & {tautology})"
            else:
                contradiction = f"({var} & ~{var})"  # æ’å‡
                prop_str = f"({prop_str} | {contradiction})"

    return prop_str


def add_noise_type6_new(prop_str: str) -> str:
    """æ–°å™ªå£°ç±»å‹6ï¼šå¾·æ‘©æ ¹å®šå¾‹å˜æ¢"""
    if random.random() < 0.3:
        # æŸ¥æ‰¾å¯ä»¥åº”ç”¨å¾·æ‘©æ ¹å®šå¾‹çš„æ¨¡å¼
        patterns = [
            (r"~\(([^()]+)\s*&\s*([^()]+)\)", r"(~\1 | ~\2)"),
            (r"~\(([^()]+)\s*\|\s*([^()]+)\)", r"(~\1 & ~\2)"),
        ]

        for pattern, replacement in patterns:
            if re.search(pattern, prop_str):
                prop_str = re.sub(pattern, replacement, prop_str, count=1)
                break

    return prop_str


def generate_diverse_proposition(complexity_level: str = "medium") -> str:
    """ç”Ÿæˆå¤šæ ·åŒ–çš„å‘½é¢˜ï¼Œç¡®ä¿ç»“æ„å¤šæ ·æ€§"""
    if complexity_level == "simple":
        return generate_simple_proposition()
    elif complexity_level == "medium":
        # 50%æ¦‚ç‡ç”Ÿæˆç®€å•å‘½é¢˜ï¼Œ50%ç”Ÿæˆä¸­ç­‰å¤æ‚åº¦
        if random.random() < 0.5:
            return generate_simple_proposition()
        else:
            return generate_recursive_implication(max_depth=2)
    else:  # complex
        # 30%ç®€å•ï¼Œ40%ä¸­ç­‰ï¼Œ30%å¤æ‚
        rand = random.random()
        if rand < 0.3:
            return generate_simple_proposition()
        elif rand < 0.7:
            return generate_recursive_implication(max_depth=2)
        else:
            return generate_recursive_implication(max_depth=3)


def generate_robust_sample(complexity_level: str = "medium") -> dict:
    """ç”Ÿæˆä¸€ä¸ªé²æ£’çš„è®­ç»ƒæ ·æœ¬"""
    # ç”ŸæˆåŸå§‹å‘½é¢˜
    original_prop = generate_diverse_proposition(complexity_level)

    # ç”Ÿæˆé€†å¦å‘½é¢˜
    target_contrapositive = to_contrapositive(original_prop)

    # åº”ç”¨é²æ£’å™ªå£°
    noise_applications = random.randint(2, 4)  # éšæœº2-4æ¬¡å™ªå£°
    noisy_prop = add_robust_noise(original_prop, noise_applications)

    return {
        "original_prop": original_prop,
        "noisy_prop": noisy_prop,
        "target_contrapositive": target_contrapositive,
        "complexity_level": complexity_level,
        "noise_applications": noise_applications,
    }


def generate_robust_dataset(num_samples: int, complexity_level: str = "medium") -> list:
    """ç”Ÿæˆé²æ£’æ•°æ®é›†"""
    dataset = []
    successful_samples = 0
    attempts = 0
    max_attempts = num_samples * 5

    print(f"ç”Ÿæˆé²æ£’æ•°æ®é›†: {complexity_level} çº§åˆ«, {num_samples} æ ·æœ¬")

    while successful_samples < num_samples and attempts < max_attempts:
        attempts += 1

        try:
            sample = generate_robust_sample(complexity_level)

            # éªŒè¯æ ·æœ¬è´¨é‡
            if (
                sample["noisy_prop"].strip()
                and sample["target_contrapositive"].strip()
                and sample["noisy_prop"] != sample["target_contrapositive"]
                and len(sample["noisy_prop"]) > 5
            ):

                dataset.append(sample)
                successful_samples += 1

                if successful_samples % 500 == 0:
                    print(f"  å·²ç”Ÿæˆ {successful_samples}/{num_samples} ä¸ªæ ·æœ¬...")

        except Exception:
            continue

    print(f"  âœ… æˆåŠŸç”Ÿæˆ {successful_samples} ä¸ªæ ·æœ¬")
    return dataset


def save_robust_dataset(dataset: list, filename: str):
    """ä¿å­˜é²æ£’æ•°æ®é›†"""
    os.makedirs(os.path.dirname(filename), exist_ok=True)

    with open(filename, "w", encoding="utf-8") as f:
        for sample in dataset:
            f.write(json.dumps(sample, ensure_ascii=False) + "\n")

    print(f"  âœ… æ•°æ®é›†å·²ä¿å­˜åˆ° {filename}")


def analyze_robust_dataset(dataset: list, name: str):
    """åˆ†æé²æ£’æ•°æ®é›†çš„è´¨é‡"""
    if not dataset:
        return

    print(f"\nğŸ“Š {name} è´¨é‡åˆ†æ:")

    # é•¿åº¦åˆ†æ
    input_lengths = [len(sample["noisy_prop"]) for sample in dataset]
    target_lengths = [len(sample["target_contrapositive"]) for sample in dataset]

    print(f"  æ ·æœ¬æ•°é‡: {len(dataset)}")
    print(f"  å¹³å‡è¾“å…¥é•¿åº¦: {sum(input_lengths)/len(input_lengths):.1f}")
    print(f"  å¹³å‡ç›®æ ‡é•¿åº¦: {sum(target_lengths)/len(target_lengths):.1f}")
    print(
        f"  é•¿åº¦èŒƒå›´: è¾“å…¥({min(input_lengths)}-{max(input_lengths)}), "
        f"ç›®æ ‡({min(target_lengths)}-{max(target_lengths)})"
    )

    # å™ªå£°åº”ç”¨åˆ†æ
    noise_counts = {}
    for sample in dataset:
        count = sample.get("noise_applications", 1)
        noise_counts[count] = noise_counts.get(count, 0) + 1

    print(f"  å™ªå£°åº”ç”¨åˆ†å¸ƒ: {noise_counts}")

    # æ˜¾ç¤ºæ ·æœ¬ç¤ºä¾‹
    print("  æ ·æœ¬ç¤ºä¾‹:")
    for i, sample in enumerate(dataset[:3]):
        print(f"    {i+1}. åŸå§‹: {sample['original_prop']}")
        print(f"       å™ªå£°: {sample['noisy_prop']}")
        print(f"       ç›®æ ‡: {sample['target_contrapositive']}")


def main():
    """ä¸»å‡½æ•°ï¼šç”Ÿæˆé²æ£’æ•°æ®é›†"""
    print("ğŸ›¡ï¸ ç”Ÿæˆé²æ£’çš„ã€æ— æ³•ä½œå¼Šçš„æ•°æ®é›†")
    print("=" * 60)

    random.seed(42)

    # ç”Ÿæˆä¸‰ä¸ªçº§åˆ«çš„é²æ£’æ•°æ®é›†
    datasets_config = [
        {
            "name": "Level 1 é²æ£’ç‰ˆ",
            "complexity": "simple",
            "train_size": 3000,
            "val_size": 500,
        },
        {
            "name": "Level 2 é²æ£’ç‰ˆ",
            "complexity": "medium",
            "train_size": 2500,
            "val_size": 400,
        },
        {
            "name": "Level 3 é²æ£’ç‰ˆ",
            "complexity": "complex",
            "train_size": 2000,
            "val_size": 300,
        },
    ]

    for config in datasets_config:
        print(f"\nğŸ”§ ç”Ÿæˆ {config['name']}")
        print("-" * 40)

        # ç”Ÿæˆè®­ç»ƒé›†
        train_dataset = generate_robust_dataset(
            config["train_size"], config["complexity"]
        )
        train_filename = f"data/train_{config['name'].replace(' ', '_').lower()}.json"
        save_robust_dataset(train_dataset, train_filename)
        analyze_robust_dataset(train_dataset, f"{config['name']} è®­ç»ƒé›†")

        # ç”ŸæˆéªŒè¯é›†
        val_dataset = generate_robust_dataset(config["val_size"], config["complexity"])
        val_filename = f"data/val_{config['name'].replace(' ', '_').lower()}.json"
        save_robust_dataset(val_dataset, val_filename)
        analyze_robust_dataset(val_dataset, f"{config['name']} éªŒè¯é›†")

    print(f"\nğŸ‰ æ‰€æœ‰é²æ£’æ•°æ®é›†ç”Ÿæˆå®Œæˆï¼")
    print(f"\nğŸ“‹ ç”Ÿæˆçš„æ–‡ä»¶:")
    for config in datasets_config:
        train_file = f"data/train_{config['name'].replace(' ', '_').lower()}.json"
        val_file = f"data/val_{config['name'].replace(' ', '_').lower()}.json"
        print(f"  ğŸ“Š {train_file}")
        print(f"  ğŸ“Š {val_file}")


if __name__ == "__main__":
    main()
