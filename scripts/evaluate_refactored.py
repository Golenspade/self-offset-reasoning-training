"""
é‡æ„åçš„è¯„ä¼°è„šæœ¬
ä½¿ç”¨é…ç½®æ–‡ä»¶å’Œæ–°çš„åŒ…ç»“æ„
"""

import json
import sys
import os
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from logic_transformer.data_utils import Tokenizer, load_dataset
from logic_transformer.models.hybrid_model import HybridModel


def load_config(config_path="configs/default_config.json"):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_file = project_root / config_path

    try:
        with open(config_file, "r", encoding="utf-8") as f:
            config = json.load(f)
        print(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_file}")
        return config
    except FileNotFoundError:
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        return None
    except json.JSONDecodeError as e:
        print(f"âŒ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
        return None


def comprehensive_evaluation(config):
    """ç»¼åˆè¯„ä¼°å‡½æ•°"""
    print("=== è‡ªåç§»æ¨ç†è®­ç»ƒ - ç»¼åˆè¯„ä¼° (é‡æ„ç‰ˆ) ===\n")

    # åˆå§‹åŒ–tokenizer
    tokenizer = Tokenizer()

    # åˆ›å»ºæ··åˆæ¨¡å‹
    model_path = project_root / config["paths"]["model_save_path"]
    hybrid_model = HybridModel(
        vocab_size=tokenizer.vocab_size,
        hidden_size=config["model"]["hidden_size"],
        model_path=str(model_path),
    )

    # åŠ è½½è¯„ä¼°æ•°æ®
    print("åŠ è½½è¯„ä¼°æ•°æ®...")
    val_path = project_root / config["data"]["val_path"]
    val_data = load_dataset(
        str(val_path), tokenizer, config["evaluation"]["max_samples"]
    )

    if not val_data:
        print("âŒ è¯„ä¼°æ•°æ®åŠ è½½å¤±è´¥")
        return None

    print(f"è¯„ä¼°æ•°æ®: {len(val_data)} æ ·æœ¬")

    # è¿›è¡Œè¯„ä¼°
    print("\n=== å¼€å§‹è¯„ä¼° ===")
    results = hybrid_model.evaluate_on_dataset(val_data)

    # æ˜¾ç¤ºç»“æœ
    print(f"\n=== è¯„ä¼°ç»“æœ ===")
    print(
        f"ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡: {results['exact_accuracy']:.2%} ({results['correct_samples']}/{results['total_samples']})"
    )

    stats = results["prediction_statistics"]
    print(f"\n=== æ–¹æ³•ä½¿ç”¨ç»Ÿè®¡ ===")
    print(f"è§„åˆ™æ–¹æ³•æˆåŠŸ: {stats['rule_success']} ({stats['rule_success_rate']:.1%})")
    print(
        f"ç¥ç»ç½‘ç»œå›é€€: {stats['neural_fallback']} ({stats['neural_fallback_rate']:.1%})"
    )
    print(f"æ€»é¢„æµ‹æ¬¡æ•°: {stats['total']}")

    print(f"\n=== æ–¹æ³•åˆ†å¸ƒ ===")
    for method, count in results["method_distribution"].items():
        percentage = count / results["total_samples"] * 100
        print(f"{method}: {count} ({percentage:.1f}%)")

    # ä¿å­˜è¯„ä¼°æŠ¥å‘Š
    save_evaluation_report(results, config)

    return results


def save_evaluation_report(results, config):
    """ä¿å­˜è¯„ä¼°æŠ¥å‘Š"""
    report_path = project_root / config["paths"]["evaluation_report_path"]

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(report_path), exist_ok=True)

    # å‡†å¤‡æŠ¥å‘Šæ•°æ®
    report = {
        "evaluation_summary": {
            "exact_accuracy": results["exact_accuracy"],
            "total_samples": results["total_samples"],
            "correct_samples": results["correct_samples"],
        },
        "method_statistics": results["prediction_statistics"],
        "method_distribution": results["method_distribution"],
        "config_used": config,
    }

    # ä¿å­˜æŠ¥å‘Š
    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    print(f"âœ… è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")


def show_sample_predictions(hybrid_model, val_data, num_samples=10):
    """æ˜¾ç¤ºæ ·æœ¬é¢„æµ‹ç»“æœ"""
    print(f"\n=== æ ·æœ¬é¢„æµ‹å±•ç¤º (å‰{num_samples}ä¸ª) ===")

    for i, sample in enumerate(val_data[:num_samples]):
        input_text = sample["input_text"]
        target_text = sample["target_text"]

        predicted_text, method = hybrid_model.predict(input_text)
        is_correct = predicted_text.strip() == target_text.strip()

        print(f"\næ ·æœ¬ {i+1}:")
        print(f"  è¾“å…¥: {input_text}")
        print(f"  ç›®æ ‡: {target_text}")
        print(f"  é¢„æµ‹: {predicted_text}")
        print(f"  æ–¹æ³•: {method}")
        print(f"  æ­£ç¡®: {'âœ“' if is_correct else 'âœ—'}")


def main():
    """ä¸»è¯„ä¼°å‡½æ•°"""
    # åŠ è½½é…ç½®
    config = load_config()
    if config is None:
        print("âŒ æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ï¼Œé€€å‡ºè¯„ä¼°")
        return

    # è¿›è¡Œç»¼åˆè¯„ä¼°
    results = comprehensive_evaluation(config)

    if results is None:
        print("âŒ è¯„ä¼°å¤±è´¥")
        return

    # åˆ›å»ºæ··åˆæ¨¡å‹ç”¨äºæ ·æœ¬å±•ç¤º
    tokenizer = Tokenizer()
    model_path = project_root / config["paths"]["model_save_path"]
    hybrid_model = HybridModel(
        vocab_size=tokenizer.vocab_size,
        hidden_size=config["model"]["hidden_size"],
        model_path=str(model_path),
    )

    # åŠ è½½æ•°æ®ç”¨äºæ ·æœ¬å±•ç¤º
    val_path = project_root / config["data"]["val_path"]
    val_data = load_dataset(str(val_path), tokenizer, 20)  # åªåŠ è½½20ä¸ªæ ·æœ¬ç”¨äºå±•ç¤º

    # æ˜¾ç¤ºæ ·æœ¬é¢„æµ‹
    show_sample_predictions(hybrid_model, val_data, 10)

    # æœ€ç»ˆæ€»ç»“
    print(f"\n=== è¯„ä¼°æ€»ç»“ ===")
    if results["exact_accuracy"] >= 0.95:
        print(f"ğŸ‰ è¯„ä¼°æˆåŠŸï¼è¾¾åˆ°äº† {results['exact_accuracy']:.1%} çš„ç²¾ç¡®å‡†ç¡®ç‡ï¼")
        print(f"è¿™è¯æ˜äº†'è‡ªåç§»æ¨ç†è®­ç»ƒ'æ¦‚å¿µçš„å¯è¡Œæ€§ã€‚")
    else:
        print(f"ğŸ“Š å½“å‰å‡†ç¡®ç‡: {results['exact_accuracy']:.1%}")
        print(f"è¿˜æœ‰æ”¹è¿›ç©ºé—´ï¼Œå¯ä»¥è€ƒè™‘ä¼˜åŒ–æ¨¡å‹æˆ–å¢åŠ è®­ç»ƒæ•°æ®ã€‚")


if __name__ == "__main__":
    main()
