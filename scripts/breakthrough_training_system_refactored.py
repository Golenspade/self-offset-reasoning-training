"""
æ–‡ä»¶å: breakthrough_training_system_refactored.py
é‡æ„åçš„çªç ´æ€§è®­ç»ƒç³»ç»Ÿ
ä¿®å¤äº†åŸç³»ç»Ÿä¸­çš„å…³é”®Bugå’Œè®¾è®¡ç¼ºé™·

ä¸»è¦æ”¹è¿›ï¼š
1. ä¿®å¤éªŒè¯é›†è®­ç»ƒBug - åˆ†ç¦»è®­ç»ƒå’Œè¯„ä¼°é€»è¾‘
2. ç»Ÿä¸€è®­ç»ƒå¾ªç¯ - ç§»é™¤å†—ä½™çš„è¿œç¨‹è®­ç»ƒä»£ç 
3. æ”¹è¿›æ¨¡å‹æ¥å£ - ä½¿ç”¨å®‰å…¨çš„æƒé‡ç®¡ç†
4. é‡æ„é…ç½®ç³»ç»Ÿ - ç»Ÿä¸€ä½¿ç”¨åµŒå¥—é…ç½®
5. æ”¹è¿›æ•°æ®æµ - é‡æ–°è®¾è®¡ç»éªŒå›æ”¾å’Œæ–°æ•°æ®ç”Ÿæˆ
6. å¢å¼ºå¼‚å¸¸å¤„ç† - é¿å…åæ‰æ‰€æœ‰å¼‚å¸¸
"""

import sys
import os
import numpy as np
import time
import json
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from collections import deque
import random

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„ï¼ˆè„šæœ¬å·²ç§»åŠ¨åˆ° scripts/ ä¸‹ï¼Œå› æ­¤è¿™é‡Œå–ä¸Šä¸€çº§ä½œä¸ºé¡¹ç›®æ ¹ç›®å½•ï¼‰
project_root = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(project_root / "src"))

from logic_transformer.data_utils import Tokenizer, load_dataset
from logic_transformer.models.base_model import ImprovedSimpleModel

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class ExperienceReplayBuffer:
    """æ”¹è¿›çš„ç»éªŒå›æ”¾ç¼“å†²åŒº"""
    
    def __init__(self, capacity: int = 15000):
        self.capacity = capacity
        self.buffer = deque(maxlen=capacity)
        self.priorities = deque(maxlen=capacity)
    
    def push(self, sample: Dict, priority: float = 1.0):
        """æ·»åŠ å•ä¸ªæ ·æœ¬"""
        self.buffer.append(sample)
        self.priorities.append(priority)
    
    def push_batch(self, samples: List[Dict], priorities: Optional[List[float]] = None):
        """æ‰¹é‡æ·»åŠ æ ·æœ¬"""
        if priorities is None:
            priorities = [1.0] * len(samples)
        
        for sample, priority in zip(samples, priorities):
            self.push(sample, priority)
    
    def sample(self, batch_size: int) -> List[Dict]:
        """é‡‡æ ·æ‰¹æ¬¡æ•°æ®"""
        if len(self.buffer) < batch_size:
            return list(self.buffer)
        
        # åŸºäºä¼˜å…ˆçº§çš„é‡‡æ ·
        priorities = np.array(self.priorities)
        probabilities = priorities / np.sum(priorities)
        
        indices = np.random.choice(len(self.buffer), size=batch_size, p=probabilities, replace=False)
        return [self.buffer[i] for i in indices]
    
    def update_priority(self, index: int, priority: float):
        """æ›´æ–°æ ·æœ¬ä¼˜å…ˆçº§"""
        if 0 <= index < len(self.priorities):
            self.priorities[index] = priority
    
    def __len__(self):
        return len(self.buffer)
    
    def utilization(self) -> float:
        """ç¼“å†²åŒºåˆ©ç”¨ç‡"""
        return len(self.buffer) / self.capacity


class AdaptiveLearningRateScheduler:
    """è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    
    def __init__(self, initial_lr: float = 0.001, patience: int = 3, factor: float = 0.7, min_lr: float = 1e-6):
        self.initial_lr = initial_lr
        self.current_lr = initial_lr
        self.patience = patience
        self.factor = factor
        self.min_lr = min_lr
        
        self.best_loss = float('inf')
        self.wait_count = 0
        self.adjustments = 0
    
    def step(self, val_loss: float) -> bool:
        """æ›´æ–°å­¦ä¹ ç‡ï¼Œè¿”å›æ˜¯å¦è¿›è¡Œäº†è°ƒæ•´"""
        if val_loss < self.best_loss:
            self.best_loss = val_loss
            self.wait_count = 0
            return False
        
        self.wait_count += 1
        
        if self.wait_count >= self.patience:
            old_lr = self.current_lr
            self.current_lr = max(self.current_lr * self.factor, self.min_lr)
            self.wait_count = 0
            self.adjustments += 1
            
            if self.current_lr != old_lr:
                logger.info(f"å­¦ä¹ ç‡è°ƒæ•´: {old_lr:.6f} -> {self.current_lr:.6f}")
                return True
        
        return False


class BreakthroughTrainingSystem:
    """é‡æ„åçš„çªç ´æ€§è®­ç»ƒç³»ç»Ÿ"""
    
    def __init__(self, config: Dict):
        """
        åˆå§‹åŒ–è®­ç»ƒç³»ç»Ÿ
        
        Args:
            config: åµŒå¥—é…ç½®å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰è®­ç»ƒå‚æ•°
        """
        self.config = config
        self.tokenizer = Tokenizer()
        
        # ä»é…ç½®ä¸­æå–å‚æ•°
        model_config = config.get('model', {})
        training_config = config.get('training', {})
        precision_config = config.get('precision', {})
        
        # åˆ›å»ºæ¨¡å‹
        self.model = ImprovedSimpleModel(
            vocab_size=self.tokenizer.vocab_size,
            hidden_size=model_config.get('hidden_size', 64),
            max_length=model_config.get('max_length', 50),
            learning_rate=training_config.get('initial_lr', 0.001)
        )
        
        # åˆ›å»ºç›®æ ‡ç½‘ç»œï¼ˆç”¨äºç¨³å®šè®­ç»ƒï¼‰
        self.target_model = ImprovedSimpleModel(
            vocab_size=self.tokenizer.vocab_size,
            hidden_size=model_config.get('hidden_size', 64),
            max_length=model_config.get('max_length', 50),
            learning_rate=training_config.get('initial_lr', 0.001)
        )
        self.target_model.copy_weights_from(self.model)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.lr_scheduler = AdaptiveLearningRateScheduler(
            initial_lr=training_config.get('initial_lr', 0.001),
            patience=precision_config.get('lr_patience', 3),
            factor=precision_config.get('lr_decay_factor', 0.7)
        )
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        replay_config = config.get('replay', {})
        self.replay_buffer = ExperienceReplayBuffer(
            capacity=replay_config.get('buffer_size', 15000)
        )
        
        # è®­ç»ƒçŠ¶æ€
        self.training_history = {
            'train_loss': [],
            'val_loss': [],
            'learning_rate': [],
            'memory_utilization': [],
            'target_updates': [],
            'gradient_health': []
        }
        
        # è®­ç»ƒå‚æ•°
        self.target_update_frequency = training_config.get('target_update_freq', 10)
        self.target_update_tau = training_config.get('target_update_tau', 0.01)
        self.gradient_clip_threshold = training_config.get('gradient_clip_threshold', 2.0)
        
        logger.info("âœ… çªç ´æ€§è®­ç»ƒç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        logger.info(f"æ¨¡å‹å‚æ•°: {self.model.get_model_info()}")
    
    def prepare_training_data(self, all_data: List[Dict], epoch: int) -> Tuple[List[Dict], List[Dict]]:
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œå®ç°è¯¾ç¨‹å­¦ä¹ ç­–ç•¥
        
        Args:
            all_data: æ‰€æœ‰è®­ç»ƒæ•°æ®
            epoch: å½“å‰epoch
            
        Returns:
            (new_data, replay_data): æ–°æ•°æ®å’Œå›æ”¾æ•°æ®
        """
        # è¯¾ç¨‹å­¦ä¹ ï¼šæ ¹æ®epoché€æ­¥å¢åŠ æ•°æ®å¤æ‚åº¦
        complexity_levels = ['simple', 'medium', 'complex']
        max_complexity_index = min(epoch // 10, len(complexity_levels) - 1)
        available_complexities = complexity_levels[:max_complexity_index + 1]
        
        # ç­›é€‰å½“å‰å¯ç”¨çš„æ•°æ®
        available_data = [
            sample for sample in all_data 
            if sample.get('complexity', 'simple') in available_complexities
        ]
        
        # æ–°æ•°æ®ï¼šä»å¯ç”¨æ•°æ®ä¸­éšæœºé‡‡æ ·
        new_data_size = min(len(available_data) // 4, 100)  # æ¯æ¬¡ä½¿ç”¨25%çš„å¯ç”¨æ•°æ®ï¼Œæœ€å¤š100ä¸ª
        new_data = random.sample(available_data, new_data_size) if available_data else []
        
        # å›æ”¾æ•°æ®ï¼šä»ç»éªŒç¼“å†²åŒºé‡‡æ ·
        replay_data_size = min(len(self.replay_buffer), new_data_size * 2)  # å›æ”¾æ•°æ®æ˜¯æ–°æ•°æ®çš„2å€
        replay_data = self.replay_buffer.sample(replay_data_size) if replay_data_size > 0 else []
        
        # å°†æ–°æ•°æ®æ·»åŠ åˆ°ç»éªŒç¼“å†²åŒº
        if new_data:
            self.replay_buffer.push_batch(new_data)
        
        logger.debug(f"Epoch {epoch}: æ–°æ•°æ® {len(new_data)}, å›æ”¾æ•°æ® {len(replay_data)}, ç¼“å†²åŒºåˆ©ç”¨ç‡ {self.replay_buffer.utilization():.2f}")
        
        return new_data, replay_data
    
    def train_step(self, sample: Dict) -> Tuple[float, bool]:
        """
        å•æ­¥è®­ç»ƒ
        
        Args:
            sample: è®­ç»ƒæ ·æœ¬
            
        Returns:
            (loss, gradient_clipped): æŸå¤±å€¼å’Œæ˜¯å¦è¿›è¡Œäº†æ¢¯åº¦è£å‰ª
        """
        try:
            # å‡†å¤‡è¾“å…¥å’Œç›®æ ‡
            input_tokens = self.tokenizer.encode(sample['noisy_prop'])
            target_tokens = self.tokenizer.encode(sample['target_contrapositive'])
            
            if not input_tokens or not target_tokens:
                logger.warning(f"ç©ºåºåˆ—è·³è¿‡: {sample}")
                return 0.0, False
            
            # æ‰§è¡Œè®­ç»ƒæ­¥éª¤
            loss = self.model.train_step_improved(input_tokens, target_tokens, self.tokenizer)
            
            # æ¢¯åº¦å¥åº·æ£€æŸ¥ï¼ˆåŸºäºæŸå¤±å€¼çš„ç®€å•æ£€æŸ¥ï¼‰
            gradient_clipped = False
            if loss > self.gradient_clip_threshold:
                gradient_clipped = True
                loss = min(loss, self.gradient_clip_threshold)  # æŸå¤±è£å‰ª
                logger.debug(f"æ¢¯åº¦å¼‚å¸¸æ£€æµ‹ï¼ŒæŸå¤±è¢«è£å‰ª: {loss}")
            
            return loss, gradient_clipped
            
        except Exception as e:
            logger.error(f"è®­ç»ƒæ­¥éª¤å¤±è´¥: {e}, æ ·æœ¬: {sample}")
            return float('inf'), False
    
    def evaluate_step(self, sample: Dict) -> float:
        """
        å•æ­¥è¯„ä¼°ï¼ˆä¸æ›´æ–°æƒé‡ï¼‰
        
        Args:
            sample: è¯„ä¼°æ ·æœ¬
            
        Returns:
            loss: æŸå¤±å€¼
        """
        try:
            # å‡†å¤‡è¾“å…¥å’Œç›®æ ‡
            input_tokens = self.tokenizer.encode(sample['noisy_prop'])
            target_tokens = self.tokenizer.encode(sample['target_contrapositive'])
            
            if not input_tokens or not target_tokens:
                return 0.0
            
            # ä½¿ç”¨è¯„ä¼°æ–¹æ³•ï¼ˆä¸æ›´æ–°æƒé‡ï¼‰
            loss = self.model.evaluate_step(input_tokens, target_tokens, self.tokenizer)
            return loss
            
        except Exception as e:
            logger.error(f"è¯„ä¼°æ­¥éª¤å¤±è´¥: {e}, æ ·æœ¬: {sample}")
            return float('inf')

    def train_epoch(self, train_data: List[Dict], val_data: List[Dict], epoch: int) -> Dict:
        """
        è®­ç»ƒä¸€ä¸ªepoch

        Args:
            train_data: è®­ç»ƒæ•°æ®
            val_data: éªŒè¯æ•°æ®
            epoch: å½“å‰epoch

        Returns:
            epoch_metrics: epochæŒ‡æ ‡å­—å…¸
        """
        epoch_start_time = time.time()

        # å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆè¯¾ç¨‹å­¦ä¹  + ç»éªŒå›æ”¾ï¼‰
        new_data, replay_data = self.prepare_training_data(train_data, epoch)
        combined_data = new_data + replay_data

        if not combined_data:
            logger.warning(f"Epoch {epoch}: æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒæ•°æ®")
            return self._create_empty_metrics(epoch)

        # æ‰“ä¹±æ•°æ®
        random.shuffle(combined_data)

        # è®­ç»ƒé˜¶æ®µ
        total_loss = 0.0
        gradient_clips = 0
        successful_steps = 0

        for sample in combined_data:
            loss, clipped = self.train_step(sample)

            if loss != float('inf'):
                total_loss += loss
                successful_steps += 1
                if clipped:
                    gradient_clips += 1

        # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
        avg_train_loss = total_loss / max(successful_steps, 1)
        gradient_health = 1.0 - (gradient_clips / max(successful_steps, 1))

        # éªŒè¯é˜¶æ®µ
        val_loss = self.evaluate_validation(val_data)

        # å­¦ä¹ ç‡è°ƒæ•´
        lr_adjusted = self.lr_scheduler.step(val_loss)
        if lr_adjusted:
            self.model.learning_rate = self.lr_scheduler.current_lr

        # ç›®æ ‡ç½‘ç»œæ›´æ–°
        target_updated = False
        if epoch % self.target_update_frequency == 0:
            self.target_model.soft_update_from(self.model, self.target_update_tau)
            target_updated = True
            logger.debug(f"ç›®æ ‡ç½‘ç»œè½¯æ›´æ–°å®Œæˆ (tau={self.target_update_tau})")

        # è®°å½•è®­ç»ƒå†å²
        epoch_metrics = {
            'epoch': epoch,
            'train_loss': avg_train_loss,
            'val_loss': val_loss,
            'learning_rate': self.lr_scheduler.current_lr,
            'memory_utilization': self.replay_buffer.utilization(),
            'gradient_health': gradient_health,
            'target_updated': target_updated,
            'new_samples': len(new_data),
            'replay_samples': len(replay_data),
            'successful_steps': successful_steps,
            'epoch_time': time.time() - epoch_start_time
        }

        # æ›´æ–°å†å²è®°å½•
        for key in ['train_loss', 'val_loss', 'learning_rate', 'memory_utilization', 'gradient_health']:
            if key in epoch_metrics:
                self.training_history[key].append(epoch_metrics[key])

        self.training_history['target_updates'].append(target_updated)

        return epoch_metrics

    def evaluate_validation(self, val_data: List[Dict]) -> float:
        """
        éªŒè¯é›†è¯„ä¼°ï¼ˆä¿®å¤åçš„ç‰ˆæœ¬ï¼Œä¸ä¼šè®­ç»ƒæ¨¡å‹ï¼‰

        Args:
            val_data: éªŒè¯æ•°æ®

        Returns:
            avg_val_loss: å¹³å‡éªŒè¯æŸå¤±
        """
        if not val_data:
            return 0.0

        total_loss = 0.0
        successful_evals = 0

        for sample in val_data:
            loss = self.evaluate_step(sample)  # ä½¿ç”¨è¯„ä¼°æ–¹æ³•ï¼Œä¸æ›´æ–°æƒé‡

            if loss != float('inf'):
                total_loss += loss
                successful_evals += 1

        avg_val_loss = total_loss / max(successful_evals, 1)

        logger.debug(f"éªŒè¯å®Œæˆ: {successful_evals}/{len(val_data)} æ ·æœ¬æˆåŠŸ, å¹³å‡æŸå¤±: {avg_val_loss:.4f}")

        return avg_val_loss

    def _create_empty_metrics(self, epoch: int) -> Dict:
        """åˆ›å»ºç©ºçš„epochæŒ‡æ ‡"""
        return {
            'epoch': epoch,
            'train_loss': 0.0,
            'val_loss': 0.0,
            'learning_rate': self.lr_scheduler.current_lr,
            'memory_utilization': self.replay_buffer.utilization(),
            'gradient_health': 1.0,
            'target_updated': False,
            'new_samples': 0,
            'replay_samples': 0,
            'successful_steps': 0,
            'epoch_time': 0.0
        }

    def run_training(self, train_data: List[Dict], val_data: List[Dict],
                    epochs: int = 50, save_frequency: int = 10,
                    output_dir: str = "outputs") -> Dict:
        """
        è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹

        Args:
            train_data: è®­ç»ƒæ•°æ®
            val_data: éªŒè¯æ•°æ®
            epochs: è®­ç»ƒè½®æ¬¡
            save_frequency: ä¿å­˜é¢‘ç‡
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            training_results: è®­ç»ƒç»“æœå­—å…¸
        """
        os.makedirs(output_dir, exist_ok=True)

        logger.info(f"ğŸš€ å¼€å§‹çªç ´æ€§è®­ç»ƒ: {epochs} epochs")
        logger.info(f"ğŸ“Š è®­ç»ƒæ•°æ®: {len(train_data)} æ ·æœ¬")
        logger.info(f"ğŸ“Š éªŒè¯æ•°æ®: {len(val_data)} æ ·æœ¬")

        # åˆå§‹åŒ–ç»éªŒå›æ”¾ç¼“å†²åŒº
        if train_data:
            # éšæœºé‡‡æ ·åˆå§‹æ•°æ®ï¼Œé¿å…åå·®
            initial_samples = random.sample(train_data, min(500, len(train_data)))
            self.replay_buffer.push_batch(initial_samples)
            logger.info(f"ç»éªŒç¼“å†²åŒºåˆå§‹åŒ–: {len(initial_samples)} æ ·æœ¬")

        training_start_time = time.time()
        best_val_loss = float('inf')

        try:
            for epoch in range(epochs):
                logger.info(f"\n{'='*60}")
                logger.info(f"ğŸ¯ Epoch {epoch + 1}/{epochs}")
                logger.info(f"{'='*60}")

                # è®­ç»ƒä¸€ä¸ªepoch
                epoch_metrics = self.train_epoch(train_data, val_data, epoch)

                # æ‰“å°epochç»“æœ
                self._log_epoch_results(epoch_metrics)

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if epoch_metrics['val_loss'] < best_val_loss:
                    best_val_loss = epoch_metrics['val_loss']
                    best_model_path = os.path.join(output_dir, "best_model.npz")
                    self.model.save_model(best_model_path)
                    logger.info(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {best_model_path}")

                # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
                if (epoch + 1) % save_frequency == 0:
                    checkpoint_path = os.path.join(output_dir, f"checkpoint_epoch_{epoch + 1}.npz")
                    self.model.save_model(checkpoint_path)
                    logger.info(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")

        except KeyboardInterrupt:
            logger.info("âš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")

        except Exception as e:
            logger.error(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            raise

        finally:
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œè®­ç»ƒå†å²
            final_model_path = os.path.join(output_dir, "final_model.npz")
            self.model.save_model(final_model_path)

            history_path = os.path.join(output_dir, "training_history.json")
            with open(history_path, 'w', encoding='utf-8') as f:
                json.dump(self.training_history, f, indent=2, ensure_ascii=False)

            total_time = time.time() - training_start_time
            logger.info(f"ğŸ‰ è®­ç»ƒå®Œæˆ! æ€»è€—æ—¶: {total_time:.2f}s")

            return {
                'best_val_loss': best_val_loss,
                'total_epochs': len(self.training_history['train_loss']),
                'training_history': self.training_history,
                'final_model_path': final_model_path,
                'history_path': history_path,
                'total_time': total_time
            }

    def _log_epoch_results(self, metrics: Dict):
        """è®°å½•epochç»“æœ"""
        logger.info(f"ğŸ“Š Epoch {metrics['epoch'] + 1} ç»“æœ:")
        logger.info(f"  è®­ç»ƒæŸå¤±: {metrics['train_loss']:.4f}")
        logger.info(f"  éªŒè¯æŸå¤±: {metrics['val_loss']:.4f}")
        logger.info(f"  å­¦ä¹ ç‡: {metrics['learning_rate']:.6f}")
        logger.info(f"  å†…å­˜åˆ©ç”¨ç‡: {metrics['memory_utilization']:.2f}")
        logger.info(f"  æ¢¯åº¦å¥åº·åº¦: {metrics['gradient_health']:.3f}")
        logger.info(f"  æ–°æ ·æœ¬/å›æ”¾æ ·æœ¬: {metrics['new_samples']}/{metrics['replay_samples']}")
        logger.info(f"  æˆåŠŸæ­¥éª¤: {metrics['successful_steps']}")
        logger.info(f"  è€—æ—¶: {metrics['epoch_time']:.2f}s")
        if metrics['target_updated']:
            logger.info(f"  ğŸ¯ ç›®æ ‡ç½‘ç»œå·²æ›´æ–°")


def create_breakthrough_config() -> Dict:
    """
    åˆ›å»ºçªç ´æ€§è®­ç»ƒé…ç½®
    ç»Ÿä¸€çš„åµŒå¥—é…ç½®ç»“æ„
    """
    return {
        'model': {
            'hidden_size': 128,
            'max_length': 100
        },
        'training': {
            'initial_lr': 0.001,
            'target_update_freq': 10,
            'target_update_tau': 0.01,
            'gradient_clip_threshold': 2.0
        },
        'precision': {
            'lr_patience': 3,
            'lr_decay_factor': 0.7,
            'min_lr': 1e-6
        },
        'replay': {
            'buffer_size': 15000,
            'initial_fill_ratio': 0.1
        },
        'curriculum': {
            'complexity_progression_epochs': 10,
            'new_data_ratio': 0.25,
            'replay_data_multiplier': 2
        }
    }


def load_training_data(data_dir: str = "data") -> Tuple[List[Dict], List[Dict]]:
    """
    åŠ è½½è®­ç»ƒæ•°æ®çš„æ”¹è¿›ç‰ˆæœ¬
    æ›´å®‰å…¨çš„æ–‡ä»¶åŠ è½½å’Œé”™è¯¯å¤„ç†
    """
    # å°è¯•åŠ è½½ä¸åŒçº§åˆ«çš„æ•°æ®æ–‡ä»¶
    data_files = [
        ("train_level_3_é²æ£’ç‰ˆ.json", "val_level_3_é²æ£’ç‰ˆ.json"),
        ("train_level_2_é²æ£’ç‰ˆ.json", "val_level_2_é²æ£’ç‰ˆ.json"),
        ("train_level_1_é²æ£’ç‰ˆ.json", "val_level_1_é²æ£’ç‰ˆ.json")
    ]

    for train_file, val_file in data_files:
        train_path = os.path.join(data_dir, train_file)
        val_path = os.path.join(data_dir, val_file)

        if os.path.exists(train_path) and os.path.exists(val_path):
            try:
                logger.info(f"ğŸ“Š åŠ è½½æ•°æ®æ–‡ä»¶: {train_file}, {val_file}")

                # å°è¯•åŠ è½½JSONLæ ¼å¼ï¼ˆæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼‰
                train_data = []
                with open(train_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line:
                            try:
                                train_data.append(json.loads(line))
                            except json.JSONDecodeError:
                                continue

                val_data = []
                with open(val_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line:
                            try:
                                val_data.append(json.loads(line))
                            except json.JSONDecodeError:
                                continue

                if train_data and val_data:
                    logger.info(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: è®­ç»ƒ {len(train_data)}, éªŒè¯ {len(val_data)}")
                    return train_data, val_data
                else:
                    logger.warning(f"âš ï¸ æ•°æ®æ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯: {train_file}, {val_file}")
                    continue

            except Exception as e:
                logger.error(f"âŒ åŠ è½½æ•°æ®æ–‡ä»¶å¤±è´¥ {train_file}: {e}")
                continue

    # å¦‚æœæ‰€æœ‰æ–‡ä»¶éƒ½åŠ è½½å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
    raise FileNotFoundError(f"åœ¨ {data_dir} ç›®å½•ä¸­æœªæ‰¾åˆ°å¯ç”¨çš„è®­ç»ƒæ•°æ®æ–‡ä»¶")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨é‡æ„åçš„çªç ´æ€§è®­ç»ƒç³»ç»Ÿ")
    print("=" * 60)

    try:
        # åˆ›å»ºé…ç½®
        config = create_breakthrough_config()
        logger.info("âœ… é…ç½®åˆ›å»ºå®Œæˆ")

        # åŠ è½½æ•°æ®
        train_data, val_data = load_training_data()

        # åˆ›å»ºè®­ç»ƒç³»ç»Ÿ
        trainer = BreakthroughTrainingSystem(config)

        # å¼€å§‹è®­ç»ƒ
        results = trainer.run_training(
            train_data=train_data,
            val_data=val_data,
            epochs=50,
            save_frequency=10,
            output_dir="outputs/breakthrough_refactored"
        )

        print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“Š æœ€ä½³éªŒè¯æŸå¤±: {results['best_val_loss']:.4f}")
        print(f"ğŸ“Š æ€»è®­ç»ƒè½®æ¬¡: {results['total_epochs']}")
        print(f"ğŸ“Š æ€»è€—æ—¶: {results['total_time']:.2f}s")
        print(f"ğŸ“ æœ€ç»ˆæ¨¡å‹: {results['final_model_path']}")

    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()
