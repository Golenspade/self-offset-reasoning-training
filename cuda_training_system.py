"""
æ–‡ä»¶å: cuda_training_system.py
CUDAåŠ é€Ÿè®­ç»ƒç³»ç»Ÿ
åŸºäºbreakthrough_training_system.pyçš„GPUä¼˜åŒ–ç‰ˆæœ¬
"""
import sys
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import GradScaler, autocast
import numpy as np
import time
import json
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from collections import deque
import random

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / 'src'))

from cuda_utils import CUDAManager
from logic_transformer.data_utils import Tokenizer
from model import create_cuda_model, get_model_memory_usage

logger = logging.getLogger(__name__)


class CUDABreakthroughTraining:
    """CUDAåŠ é€Ÿçš„çªç ´æ€§è®­ç»ƒç³»ç»Ÿ"""
    
    def __init__(self, config: Dict):
        """
        åˆå§‹åŒ–CUDAè®­ç»ƒç³»ç»Ÿ
        
        Args:
            config: è®­ç»ƒé…ç½®å­—å…¸
        """
        self.config = config
        self.tokenizer = Tokenizer()
        
        # CUDAç®¡ç†å™¨
        self.cuda_manager = CUDAManager(
            memory_fraction=config.get('gpu_memory_fraction', 0.8),
            auto_optimize=True
        )
        self.device = self.cuda_manager.device
        
        # åˆ›å»ºCUDAä¼˜åŒ–æ¨¡å‹
        self.model, _ = create_cuda_model(
            vocab_size=self.tokenizer.vocab_size,
            device=self.device,
            use_mixed_precision=config.get('use_mixed_precision', True),
            d_model=config.get('hidden_size', 128),
            nhead=config.get('num_heads', 8),
            num_encoder_layers=config.get('num_encoder_layers', 3),
            num_decoder_layers=config.get('num_decoder_layers', 3),
            dim_feedforward=config.get('dim_feedforward', 512),
            max_len=config.get('max_length', 100)
        )
        
        # ä¼˜åŒ–å™¨é…ç½®
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=config.get('learning_rate', 0.001),
            weight_decay=config.get('weight_decay', 1e-5),
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=config.get('lr_decay_factor', 0.5),
            patience=config.get('lr_patience', 3),
            verbose=True,
            min_lr=1e-7
        )
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        self.use_amp = (config.get('use_mixed_precision', True) and 
                       self.device.type == 'cuda' and 
                       self.cuda_manager.supports_mixed_precision())
        
        if self.use_amp:
            self.scaler = GradScaler()
            logger.info("âœ… å¯ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ")
        else:
            self.scaler = None
            logger.info("â„¹ï¸ ä½¿ç”¨æ ‡å‡†ç²¾åº¦è®­ç»ƒ")
        
        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss(
            ignore_index=self.tokenizer.PAD_TOKEN,
            label_smoothing=config.get('label_smoothing', 0.0)
        )
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.best_val_loss = float('inf')
        self.training_history = {
            'train_loss': [],
            'val_loss': [],
            'learning_rate': [],
            'gpu_memory': [],
            'epoch_time': []
        }
        
        # æ¢¯åº¦ç´¯ç§¯
        self.gradient_accumulation_steps = config.get('gradient_accumulation_steps', 1)
        
        # æ—©åœ
        self.early_stopping_patience = config.get('early_stopping_patience', 10)
        self.patience_counter = 0
        
        logger.info(f"ğŸš€ CUDAè®­ç»ƒç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“ è®¾å¤‡: {self.device}")
        logger.info(f"ğŸ”¥ æ··åˆç²¾åº¦: {self.use_amp}")
        logger.info(f"ğŸ“Š æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {self.gradient_accumulation_steps}")
    
    def prepare_batch_cuda(self, batch_data: List[Dict]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        å‡†å¤‡CUDAæ‰¹æ¬¡æ•°æ®
        
        Args:
            batch_data: æ‰¹æ¬¡æ•°æ®åˆ—è¡¨
            
        Returns:
            (src_batch, tgt_input, tgt_output): æºåºåˆ—ã€ç›®æ ‡è¾“å…¥ã€ç›®æ ‡è¾“å‡º
        """
        if not batch_data:
            return None, None, None
        
        batch_size = len(batch_data)
        
        # è®¡ç®—æœ€å¤§é•¿åº¦
        max_src_len = max(len(self.tokenizer.encode(item['noisy_prop'])) for item in batch_data)
        max_tgt_len = max(len(self.tokenizer.encode(item['target_contrapositive'])) for item in batch_data)
        
        # åˆ›å»ºå¼ é‡
        src_batch = torch.full(
            (max_src_len, batch_size), 
            self.tokenizer.PAD_TOKEN, 
            dtype=torch.long, 
            device=self.device
        )
        
        tgt_batch = torch.full(
            (max_tgt_len + 1, batch_size),  # +1 for START_TOKEN
            self.tokenizer.PAD_TOKEN,
            dtype=torch.long,
            device=self.device
        )
        
        # å¡«å……æ•°æ®
        for i, item in enumerate(batch_data):
            # ç¼–ç æºåºåˆ—
            src_tokens = self.tokenizer.encode(item['noisy_prop'])
            src_len = len(src_tokens)
            src_batch[:src_len, i] = torch.tensor(src_tokens, device=self.device)
            
            # ç¼–ç ç›®æ ‡åºåˆ—ï¼ˆæ·»åŠ START_TOKENï¼‰
            tgt_tokens = [self.tokenizer.START_TOKEN] + self.tokenizer.encode(item['target_contrapositive'])
            tgt_len = len(tgt_tokens)
            tgt_batch[:tgt_len, i] = torch.tensor(tgt_tokens, device=self.device)
        
        # åˆ†ç¦»è¾“å…¥å’Œè¾“å‡º
        tgt_input = tgt_batch[:-1]  # å»æ‰æœ€åä¸€ä¸ªtokenä½œä¸ºè¾“å…¥
        tgt_output = tgt_batch[1:]  # å»æ‰ç¬¬ä¸€ä¸ªtokenä½œä¸ºç›®æ ‡
        
        return src_batch, tgt_input, tgt_output
    
    def create_masks(self, src: torch.Tensor, tgt: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        åˆ›å»ºæ³¨æ„åŠ›æ©ç 
        
        Args:
            src: æºåºåˆ— [seq_len, batch_size]
            tgt: ç›®æ ‡åºåˆ— [seq_len, batch_size]
            
        Returns:
            (src_mask, tgt_mask): æºæ©ç å’Œç›®æ ‡æ©ç 
        """
        src_seq_len = src.size(0)
        tgt_seq_len = tgt.size(0)
        
        # æºåºåˆ—æ©ç ï¼ˆpaddingæ©ç ï¼‰
        src_mask = (src == self.tokenizer.PAD_TOKEN).transpose(0, 1)  # [batch_size, seq_len]
        
        # ç›®æ ‡åºåˆ—æ©ç ï¼ˆå› æœæ©ç  + paddingæ©ç ï¼‰
        tgt_mask = torch.triu(
            torch.ones(tgt_seq_len, tgt_seq_len, device=self.device), 
            diagonal=1
        ).bool()
        
        tgt_padding_mask = (tgt == self.tokenizer.PAD_TOKEN).transpose(0, 1)
        
        return src_mask, tgt_mask, tgt_padding_mask
    
    def train_step_cuda(self, batch_data: List[Dict], accumulate_gradients: bool = False) -> Dict:
        """
        CUDAåŠ é€Ÿçš„è®­ç»ƒæ­¥éª¤
        
        Args:
            batch_data: æ‰¹æ¬¡æ•°æ®
            accumulate_gradients: æ˜¯å¦ç´¯ç§¯æ¢¯åº¦
            
        Returns:
            è®­ç»ƒæŒ‡æ ‡å­—å…¸
        """
        if not accumulate_gradients:
            self.optimizer.zero_grad()
        
        # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
        src_batch, tgt_input, tgt_output = self.prepare_batch_cuda(batch_data)
        
        if src_batch is None:
            return {'loss': 0.0, 'grad_norm': 0.0}
        
        # åˆ›å»ºæ©ç 
        src_mask, tgt_mask, tgt_padding_mask = self.create_masks(src_batch, tgt_input)
        
        try:
            if self.use_amp:
                # æ··åˆç²¾åº¦è®­ç»ƒ
                with autocast():
                    output = self.model(
                        src_batch,
                        tgt_input,
                        src_key_padding_mask=src_mask,
                        tgt_mask=tgt_mask,
                        tgt_key_padding_mask=tgt_padding_mask
                    )
                    
                    # è®¡ç®—æŸå¤±
                    loss = self.criterion(
                        output.reshape(-1, output.size(-1)),
                        tgt_output.reshape(-1)
                    )
                    
                    # æ¢¯åº¦ç´¯ç§¯
                    loss = loss / self.gradient_accumulation_steps
                
                # åå‘ä¼ æ’­
                self.scaler.scale(loss).backward()
                
                if not accumulate_gradients:
                    # æ¢¯åº¦è£å‰ª
                    self.scaler.unscale_(self.optimizer)
                    grad_norm = torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), 
                        max_norm=self.config.get('max_grad_norm', 1.0)
                    )
                    
                    # ä¼˜åŒ–å™¨æ­¥éª¤
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    grad_norm = 0.0
                
            else:
                # æ ‡å‡†ç²¾åº¦è®­ç»ƒ
                output = self.model(
                    src_batch,
                    tgt_input,
                    src_key_padding_mask=src_mask,
                    tgt_mask=tgt_mask,
                    tgt_key_padding_mask=tgt_padding_mask
                )
                
                loss = self.criterion(
                    output.reshape(-1, output.size(-1)),
                    tgt_output.reshape(-1)
                )
                
                loss = loss / self.gradient_accumulation_steps
                loss.backward()
                
                if not accumulate_gradients:
                    grad_norm = torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        max_norm=self.config.get('max_grad_norm', 1.0)
                    )
                    self.optimizer.step()
                else:
                    grad_norm = 0.0
            
            return {
                'loss': loss.item() * self.gradient_accumulation_steps,
                'grad_norm': grad_norm.item() if isinstance(grad_norm, torch.Tensor) else grad_norm,
                'learning_rate': self.optimizer.param_groups[0]['lr']
            }
            
        except RuntimeError as e:
            if "out of memory" in str(e):
                logger.warning("âš ï¸ GPUå†…å­˜ä¸è¶³ï¼Œæ¸…ç†ç¼“å­˜...")
                torch.cuda.empty_cache()
                return {'loss': float('inf'), 'grad_norm': 0.0, 'learning_rate': 0.0}
            else:
                raise e
    
    def validate_cuda(self, val_data: List[Dict]) -> Dict:
        """
        CUDAéªŒè¯
        
        Args:
            val_data: éªŒè¯æ•°æ®
            
        Returns:
            éªŒè¯æŒ‡æ ‡å­—å…¸
        """
        self.model.eval()
        total_loss = 0.0
        num_batches = 0
        batch_size = self.config.get('batch_size', 16)
        
        with torch.no_grad():
            for i in range(0, len(val_data), batch_size):
                batch = val_data[i:i+batch_size]
                
                src_batch, tgt_input, tgt_output = self.prepare_batch_cuda(batch)
                
                if src_batch is None:
                    continue
                
                src_mask, tgt_mask, tgt_padding_mask = self.create_masks(src_batch, tgt_input)
                
                try:
                    if self.use_amp:
                        with autocast():
                            output = self.model(
                                src_batch,
                                tgt_input,
                                src_key_padding_mask=src_mask,
                                tgt_mask=tgt_mask,
                                tgt_key_padding_mask=tgt_padding_mask
                            )
                    else:
                        output = self.model(
                            src_batch,
                            tgt_input,
                            src_key_padding_mask=src_mask,
                            tgt_mask=tgt_mask,
                            tgt_key_padding_mask=tgt_padding_mask
                        )
                    
                    loss = self.criterion(
                        output.reshape(-1, output.size(-1)),
                        tgt_output.reshape(-1)
                    )
                    
                    total_loss += loss.item()
                    num_batches += 1
                    
                except RuntimeError as e:
                    if "out of memory" in str(e):
                        torch.cuda.empty_cache()
                        continue
                    else:
                        raise e
        
        avg_loss = total_loss / max(num_batches, 1)
        
        return {
            'val_loss': avg_loss,
            'num_batches': num_batches
        }
    
    def save_checkpoint(self, epoch: int, val_loss: float, filepath: str):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'val_loss': val_loss,
            'best_val_loss': self.best_val_loss,
            'training_history': self.training_history,
            'config': self.config,
            'tokenizer_vocab_size': self.tokenizer.vocab_size
        }
        
        if self.use_amp:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()
        
        torch.save(checkpoint, filepath)
        logger.info(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filepath}")
    
    def load_checkpoint(self, filepath: str) -> int:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        if self.use_amp and 'scaler_state_dict' in checkpoint:
            self.scaler.load_state_dict(checkpoint['scaler_state_dict'])
        
        self.best_val_loss = checkpoint.get('best_val_loss', float('inf'))
        self.training_history = checkpoint.get('training_history', {})
        
        epoch = checkpoint['epoch']
        logger.info(f"ğŸ“‚ æ£€æŸ¥ç‚¹å·²åŠ è½½: {filepath} (epoch {epoch})")

        return epoch

    def train_epoch_cuda(self, train_data: List[Dict], val_data: List[Dict], epoch: int) -> Dict:
        """
        CUDAåŠ é€Ÿçš„epochè®­ç»ƒ

        Args:
            train_data: è®­ç»ƒæ•°æ®
            val_data: éªŒè¯æ•°æ®
            epoch: å½“å‰epoch

        Returns:
            è®­ç»ƒæŒ‡æ ‡å­—å…¸
        """
        self.model.train()
        self.current_epoch = epoch

        batch_size = self.config.get('batch_size', 16)
        total_loss = 0.0
        num_batches = 0

        # éšæœºæ‰“ä¹±è®­ç»ƒæ•°æ®
        random.shuffle(train_data)

        epoch_start_time = time.time()

        # è®­ç»ƒå¾ªç¯
        for i in range(0, len(train_data), batch_size):
            batch = train_data[i:i+batch_size]

            # æ¢¯åº¦ç´¯ç§¯
            accumulate = (i + batch_size) % (batch_size * self.gradient_accumulation_steps) != 0

            with self.cuda_manager.memory_monitor(f"è®­ç»ƒæ‰¹æ¬¡ {i//batch_size + 1}"):
                metrics = self.train_step_cuda(batch, accumulate_gradients=accumulate)

            if metrics['loss'] != float('inf'):
                total_loss += metrics['loss']
                num_batches += 1

            # å®šæœŸæ‰“å°è¿›åº¦å’ŒGPUçŠ¶æ€
            if (i // batch_size + 1) % self.config.get('log_frequency', 50) == 0:
                memory_info = self.cuda_manager.get_memory_info()
                if memory_info and 'error' not in memory_info:
                    logger.info(
                        f"Batch {i//batch_size + 1}/{len(train_data)//batch_size}: "
                        f"Loss={metrics['loss']:.4f}, "
                        f"LR={metrics['learning_rate']:.6f}, "
                        f"GPU={memory_info['allocated_memory']:.1f}GB/"
                        f"{memory_info['total_memory']:.1f}GB"
                    )

        # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
        avg_train_loss = total_loss / max(num_batches, 1)

        # éªŒè¯é˜¶æ®µ
        logger.info("ğŸ” å¼€å§‹éªŒè¯...")
        val_metrics = self.validate_cuda(val_data)
        val_loss = val_metrics['val_loss']

        # æ›´æ–°å­¦ä¹ ç‡
        self.scheduler.step(val_loss)
        current_lr = self.optimizer.param_groups[0]['lr']

        # æ—©åœæ£€æŸ¥
        if val_loss < self.best_val_loss:
            self.best_val_loss = val_loss
            self.patience_counter = 0
        else:
            self.patience_counter += 1

        # è®°å½•è®­ç»ƒå†å²
        epoch_time = time.time() - epoch_start_time

        self.training_history['train_loss'].append(avg_train_loss)
        self.training_history['val_loss'].append(val_loss)
        self.training_history['learning_rate'].append(current_lr)
        self.training_history['epoch_time'].append(epoch_time)

        # è®°å½•GPUå†…å­˜ä½¿ç”¨
        if self.device.type == 'cuda':
            memory_info = self.cuda_manager.get_memory_info()
            if memory_info and 'error' not in memory_info:
                self.training_history['gpu_memory'].append(memory_info['allocated_memory'])

        return {
            'epoch': epoch,
            'train_loss': avg_train_loss,
            'val_loss': val_loss,
            'learning_rate': current_lr,
            'epoch_time': epoch_time,
            'best_val_loss': self.best_val_loss,
            'patience_counter': self.patience_counter,
            'early_stop': self.patience_counter >= self.early_stopping_patience
        }

    def run_cuda_training(self, train_data: List[Dict], val_data: List[Dict],
                         output_dir: str = "outputs") -> Dict:
        """
        è¿è¡Œå®Œæ•´çš„CUDAè®­ç»ƒæµç¨‹

        Args:
            train_data: è®­ç»ƒæ•°æ®
            val_data: éªŒè¯æ•°æ®
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        os.makedirs(output_dir, exist_ok=True)

        epochs = self.config.get('epochs', 50)
        save_frequency = self.config.get('save_frequency', 10)

        logger.info(f"ğŸš€ å¼€å§‹CUDAè®­ç»ƒ: {epochs} epochs")
        logger.info(f"ğŸ“Š è®­ç»ƒæ•°æ®: {len(train_data)} æ ·æœ¬")
        logger.info(f"ğŸ“Š éªŒè¯æ•°æ®: {len(val_data)} æ ·æœ¬")

        training_start_time = time.time()

        try:
            for epoch in range(epochs):
                logger.info(f"\n{'='*60}")
                logger.info(f"ğŸ¯ Epoch {epoch + 1}/{epochs}")
                logger.info(f"{'='*60}")

                # è®­ç»ƒä¸€ä¸ªepoch
                epoch_metrics = self.train_epoch_cuda(train_data, val_data, epoch)

                # æ‰“å°epochç»“æœ
                logger.info(f"ğŸ“Š Epoch {epoch + 1} ç»“æœ:")
                logger.info(f"  è®­ç»ƒæŸå¤±: {epoch_metrics['train_loss']:.4f}")
                logger.info(f"  éªŒè¯æŸå¤±: {epoch_metrics['val_loss']:.4f}")
                logger.info(f"  æœ€ä½³éªŒè¯æŸå¤±: {epoch_metrics['best_val_loss']:.4f}")
                logger.info(f"  å­¦ä¹ ç‡: {epoch_metrics['learning_rate']:.6f}")
                logger.info(f"  è€—æ—¶: {epoch_metrics['epoch_time']:.2f}s")
                logger.info(f"  æ—©åœè®¡æ•°: {epoch_metrics['patience_counter']}/{self.early_stopping_patience}")

                # ä¿å­˜æ£€æŸ¥ç‚¹
                if (epoch + 1) % save_frequency == 0 or epoch_metrics['val_loss'] == self.best_val_loss:
                    checkpoint_path = os.path.join(
                        output_dir,
                        f"cuda_checkpoint_epoch_{epoch + 1}.pth"
                    )
                    self.save_checkpoint(epoch, epoch_metrics['val_loss'], checkpoint_path)

                # æ—©åœæ£€æŸ¥
                if epoch_metrics['early_stop']:
                    logger.info(f"ğŸ›‘ æ—©åœè§¦å‘ (patience: {self.early_stopping_patience})")
                    break

        except KeyboardInterrupt:
            logger.info("âš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")

        except Exception as e:
            logger.error(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            raise

        finally:
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            final_model_path = os.path.join(output_dir, "final_cuda_model.pth")
            self.save_checkpoint(self.current_epoch, self.best_val_loss, final_model_path)

            # ä¿å­˜è®­ç»ƒå†å²
            history_path = os.path.join(output_dir, "cuda_training_history.json")
            with open(history_path, 'w', encoding='utf-8') as f:
                json.dump(self.training_history, f, indent=2, ensure_ascii=False)

            total_time = time.time() - training_start_time
            logger.info(f"ğŸ‰ è®­ç»ƒå®Œæˆ! æ€»è€—æ—¶: {total_time:.2f}s")

            # æ¸…ç†GPUç¼“å­˜
            self.cuda_manager.clear_cache()

        return {
            'best_val_loss': self.best_val_loss,
            'total_epochs': self.current_epoch + 1,
            'training_history': self.training_history,
            'final_model_path': final_model_path,
            'history_path': history_path
        }
