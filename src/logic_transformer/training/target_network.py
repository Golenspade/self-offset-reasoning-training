"""
æ–‡ä»¶å: src/logic_transformer/training/target_network.py
ç¬¬ä¸‰é˜¶æ®µï¼šæ¶æ„é©å‘½ - ç›®æ ‡ç½‘ç»œç³»ç»Ÿ
å®ç°ç¨³å®šçš„"åŒ—ææ˜Ÿ"ï¼Œå½»åº•è§£å†³è¿½é€ç§»åŠ¨ç›®æ ‡çš„é—®é¢˜
"""

import torch
import torch.nn as nn
import copy
import numpy as np
from typing import Dict, List, Tuple, Optional
import logging

logger = logging.getLogger(__name__)


class TargetNetworkSystem:
    """ç›®æ ‡ç½‘ç»œç³»ç»Ÿ - ç¨³å®šçš„å­¦ä¹ æŒ‡å¯¼è€…"""

    def __init__(self, learning_model, config: Dict):
        self.learning_model = learning_model
        self.config = config

        # åˆ›å»ºç›®æ ‡ç½‘ç»œ - å­¦ä¹ ç½‘ç»œçš„å®Œå…¨å‰¯æœ¬
        self.target_model = copy.deepcopy(learning_model)

        # å†»ç»“ç›®æ ‡ç½‘ç»œçš„å‚æ•°ï¼Œå®ƒä¸é€šè¿‡æ¢¯åº¦ä¸‹é™æ›´æ–°
        for param in self.target_model.parameters():
            param.requires_grad = False

        # è½¯æ›´æ–°å‚æ•°
        self.tau = config.get("tau", 1e-3)  # è½¯æ›´æ–°ç³»æ•°
        self.update_frequency = config.get("update_frequency", 1)  # æ›´æ–°é¢‘ç‡
        self.update_counter = 0

        # ç¨³å®šæ€§ç›‘æ§
        self.stability_metrics = {
            "parameter_divergence": [],
            "output_consistency": [],
            "learning_stability": [],
        }

        logger.info("ğŸŒŸ ç›®æ ‡ç½‘ç»œç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  è½¯æ›´æ–°ç³»æ•° Ï„: {self.tau}")
        logger.info(f"  æ›´æ–°é¢‘ç‡: {self.update_frequency}")

    def soft_update(self):
        """è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ - ç¼“æ…¢å¸æ”¶å­¦ä¹ ç½‘ç»œçš„çŸ¥è¯†"""
        param_divergence = 0.0
        param_count = 0

        for target_param, learning_param in zip(
            self.target_model.parameters(), self.learning_model.parameters()
        ):
            # è®¡ç®—å‚æ•°å·®å¼‚ï¼ˆç”¨äºç›‘æ§ç¨³å®šæ€§ï¼‰
            param_diff = torch.norm(target_param.data - learning_param.data).item()
            param_divergence += param_diff
            param_count += 1

            # è½¯æ›´æ–°ï¼štarget = Ï„ * learning + (1-Ï„) * target
            target_param.data.copy_(
                self.tau * learning_param.data + (1.0 - self.tau) * target_param.data
            )

        # è®°å½•å‚æ•°å·®å¼‚
        avg_divergence = param_divergence / param_count if param_count > 0 else 0.0
        self.stability_metrics["parameter_divergence"].append(avg_divergence)

        self.update_counter += 1

        if self.update_counter % 100 == 0:
            logger.debug(
                f"ç›®æ ‡ç½‘ç»œè½¯æ›´æ–° #{self.update_counter}, å¹³å‡å‚æ•°å·®å¼‚: {avg_divergence:.6f}"
            )

    def hard_update(self):
        """ç¡¬æ›´æ–°ç›®æ ‡ç½‘ç»œ - å®Œå…¨å¤åˆ¶å­¦ä¹ ç½‘ç»œ"""
        self.target_model.load_state_dict(self.learning_model.state_dict())
        logger.info("æ‰§è¡Œäº†ç›®æ ‡ç½‘ç»œç¡¬æ›´æ–°")

    def should_update(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        return self.update_counter % self.update_frequency == 0

    def compute_stable_targets(self, inputs: List, tokenizer) -> List:
        """ä½¿ç”¨ç›®æ ‡ç½‘ç»œè®¡ç®—ç¨³å®šçš„ç›®æ ‡å€¼"""
        self.target_model.eval()

        stable_targets = []

        with torch.no_grad():
            for input_seq in inputs:
                try:
                    # ä½¿ç”¨ç›®æ ‡ç½‘ç»œç”Ÿæˆç¨³å®šçš„é¢„æµ‹
                    target_prediction = self.target_model.predict(input_seq, tokenizer)
                    stable_targets.append(target_prediction)
                except Exception as e:
                    logger.warning(f"ç›®æ ‡ç½‘ç»œé¢„æµ‹å¤±è´¥: {e}")
                    stable_targets.append([])

        return stable_targets

    def evaluate_consistency(self, test_inputs: List, tokenizer) -> float:
        """è¯„ä¼°å­¦ä¹ ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œçš„ä¸€è‡´æ€§"""
        if not test_inputs:
            return 1.0

        consistency_scores = []

        for input_seq in test_inputs[:10]:  # åªæµ‹è¯•å‰10ä¸ªæ ·æœ¬
            try:
                # å­¦ä¹ ç½‘ç»œé¢„æµ‹
                learning_pred = self.learning_model.predict(input_seq, tokenizer)

                # ç›®æ ‡ç½‘ç»œé¢„æµ‹
                with torch.no_grad():
                    target_pred = self.target_model.predict(input_seq, tokenizer)

                # è®¡ç®—ä¸€è‡´æ€§ï¼ˆç®€å•çš„åºåˆ—ç›¸ä¼¼åº¦ï¼‰
                if len(learning_pred) == len(target_pred):
                    matches = sum(
                        1 for a, b in zip(learning_pred, target_pred) if a == b
                    )
                    consistency = matches / len(learning_pred) if learning_pred else 0.0
                else:
                    consistency = 0.0

                consistency_scores.append(consistency)

            except Exception as e:
                logger.warning(f"ä¸€è‡´æ€§è¯„ä¼°å¤±è´¥: {e}")
                continue

        avg_consistency = np.mean(consistency_scores) if consistency_scores else 0.0
        self.stability_metrics["output_consistency"].append(avg_consistency)

        return avg_consistency

    def get_stability_report(self) -> Dict:
        """è·å–ç¨³å®šæ€§æŠ¥å‘Š"""
        if not self.stability_metrics["parameter_divergence"]:
            return {"status": "no_data"}

        param_div = self.stability_metrics["parameter_divergence"]
        output_cons = self.stability_metrics["output_consistency"]

        return {
            "update_count": self.update_counter,
            "parameter_divergence": {
                "mean": np.mean(param_div),
                "std": np.std(param_div),
                "trend": (
                    "stable"
                    if np.std(param_div[-10:]) < np.std(param_div)
                    else "unstable"
                ),
            },
            "output_consistency": {
                "mean": np.mean(output_cons) if output_cons else 0.0,
                "latest": output_cons[-1] if output_cons else 0.0,
            },
            "stability_score": self._compute_stability_score(),
        }

    def _compute_stability_score(self) -> float:
        """è®¡ç®—ç»¼åˆç¨³å®šæ€§åˆ†æ•°"""
        if not self.stability_metrics["parameter_divergence"]:
            return 0.0

        # å‚æ•°ç¨³å®šæ€§ï¼šå·®å¼‚è¶Šå°è¶Šç¨³å®š
        param_stability = 1.0 / (
            1.0 + np.mean(self.stability_metrics["parameter_divergence"][-10:])
        )

        # è¾“å‡ºä¸€è‡´æ€§ï¼šä¸€è‡´æ€§è¶Šé«˜è¶Šç¨³å®š
        output_stability = (
            np.mean(self.stability_metrics["output_consistency"][-5:])
            if self.stability_metrics["output_consistency"]
            else 0.0
        )

        # ç»¼åˆåˆ†æ•°
        stability_score = 0.6 * param_stability + 0.4 * output_stability

        return min(stability_score, 1.0)

    def save_target_network(self, filepath: str):
        """ä¿å­˜ç›®æ ‡ç½‘ç»œ"""
        torch.save(
            {
                "target_model_state_dict": self.target_model.state_dict(),
                "learning_model_state_dict": self.learning_model.state_dict(),
                "config": self.config,
                "update_counter": self.update_counter,
                "stability_metrics": self.stability_metrics,
            },
            filepath,
        )
        logger.info(f"ç›®æ ‡ç½‘ç»œå·²ä¿å­˜: {filepath}")

    def load_target_network(self, filepath: str):
        """åŠ è½½ç›®æ ‡ç½‘ç»œ"""
        checkpoint = torch.load(filepath)
        self.target_model.load_state_dict(checkpoint["target_model_state_dict"])
        self.learning_model.load_state_dict(checkpoint["learning_model_state_dict"])
        self.update_counter = checkpoint["update_counter"]
        self.stability_metrics = checkpoint["stability_metrics"]
        logger.info(f"ç›®æ ‡ç½‘ç»œå·²åŠ è½½: {filepath}")


class StabilizedTrainingLoop:
    """ç¨³å®šåŒ–è®­ç»ƒå¾ªç¯ - æ•´åˆç›®æ ‡ç½‘ç»œçš„è®­ç»ƒç³»ç»Ÿ"""

    def __init__(self, target_system: TargetNetworkSystem, config: Dict):
        self.target_system = target_system
        self.config = config

        # è®­ç»ƒå‚æ•°
        self.stability_check_frequency = config.get("stability_check_frequency", 10)
        self.min_stability_threshold = config.get("min_stability_threshold", 0.7)

        logger.info("ğŸ¯ ç¨³å®šåŒ–è®­ç»ƒå¾ªç¯åˆå§‹åŒ–å®Œæˆ")

    def train_step_with_stability(self, batch_data: List[Dict], tokenizer) -> Dict:
        """æ‰§è¡Œç¨³å®šåŒ–è®­ç»ƒæ­¥éª¤"""

        # 1. æ­£å¸¸çš„å­¦ä¹ ç½‘ç»œè®­ç»ƒ
        learning_results = self._train_learning_network(batch_data, tokenizer)

        # 2. æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ç›®æ ‡ç½‘ç»œ
        if self.target_system.should_update():
            self.target_system.soft_update()

        # 3. å®šæœŸè¿›è¡Œç¨³å®šæ€§æ£€æŸ¥
        stability_info = {}
        if self.target_system.update_counter % self.stability_check_frequency == 0:
            test_inputs = [sample["input"] for sample in batch_data[:5]]
            consistency = self.target_system.evaluate_consistency(
                test_inputs, tokenizer
            )
            stability_report = self.target_system.get_stability_report()

            stability_info = {
                "consistency": consistency,
                "stability_score": stability_report.get("stability_score", 0.0),
            }

            # å¦‚æœç¨³å®šæ€§è¿‡ä½ï¼Œè€ƒè™‘ç¡¬æ›´æ–°
            if (
                stability_report.get("stability_score", 0.0)
                < self.min_stability_threshold
            ):
                logger.warning(
                    f"ç¨³å®šæ€§è¿‡ä½ ({stability_report.get('stability_score', 0.0):.3f})ï¼Œè€ƒè™‘è°ƒæ•´è®­ç»ƒç­–ç•¥"
                )

        return {
            **learning_results,
            **stability_info,
            "target_updates": self.target_system.update_counter,
        }

    def _train_learning_network(self, batch_data: List[Dict], tokenizer) -> Dict:
        """è®­ç»ƒå­¦ä¹ ç½‘ç»œ"""
        # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„è®­ç»ƒé€»è¾‘
        # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿç»“æœ
        return {"learning_loss": 0.5, "learning_accuracy": 0.8}


def test_target_network():
    """æµ‹è¯•ç›®æ ‡ç½‘ç»œç³»ç»Ÿ"""
    print("ğŸ§ª æµ‹è¯•ç›®æ ‡ç½‘ç»œç³»ç»Ÿ")
    print("=" * 50)

    # æ¨¡æ‹Ÿä¸€ä¸ªç®€å•çš„æ¨¡å‹
    class MockModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.linear = nn.Linear(10, 5)

        def forward(self, x):
            return self.linear(x)

        def predict(self, input_seq, tokenizer):
            return [1, 2, 3]  # æ¨¡æ‹Ÿé¢„æµ‹

    # åˆ›å»ºé…ç½®
    config = {"tau": 1e-3, "update_frequency": 1, "stability_check_frequency": 5}

    # åˆ›å»ºç›®æ ‡ç½‘ç»œç³»ç»Ÿ
    learning_model = MockModel()
    target_system = TargetNetworkSystem(learning_model, config)

    print("âœ… ç›®æ ‡ç½‘ç»œç³»ç»Ÿåˆ›å»ºæˆåŠŸ")

    # æµ‹è¯•è½¯æ›´æ–°
    for i in range(10):
        target_system.soft_update()

    print("âœ… è½¯æ›´æ–°æµ‹è¯•å®Œæˆ")

    # æµ‹è¯•ç¨³å®šæ€§æŠ¥å‘Š
    stability_report = target_system.get_stability_report()
    print(f"ğŸ“Š ç¨³å®šæ€§æŠ¥å‘Š: {stability_report}")

    print("\nğŸ¯ ç›®æ ‡ç½‘ç»œçš„æ ¸å¿ƒä¼˜åŠ¿:")
    print("  ğŸŒŸ ç¨³å®šæŒ‡å¯¼: æä¾›ç¨³å®šçš„å­¦ä¹ ç›®æ ‡")
    print("  ğŸ”„ è½¯æ›´æ–°: ç¼“æ…¢å¸æ”¶æ–°çŸ¥è¯†")
    print("  ğŸ“ˆ æŒç»­ç¨³å®š: é¿å…è®­ç»ƒéœ‡è¡")


if __name__ == "__main__":
    test_target_network()
