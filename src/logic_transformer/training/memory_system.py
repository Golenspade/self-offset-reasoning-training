"""
æ–‡ä»¶å: src/logic_transformer/training/memory_system.py
ç¬¬äºŒé˜¶æ®µï¼šç´¯ç§¯å­¦ä¹  - ç»éªŒå›æ”¾ç¼“å†²åŒº
å®ç°è®°å¿†å®«æ®¿ï¼Œè§£å†³ç¾éš¾æ€§é—å¿˜é—®é¢˜
"""

import random
import numpy as np
from collections import deque
from typing import Dict, List, Tuple, Optional, Any
import pickle
import os
import logging

logger = logging.getLogger(__name__)


class ReplayBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº - æ¨¡å‹çš„è®°å¿†å®«æ®¿"""

    def __init__(self, capacity: int, save_path: Optional[str] = None):
        self.capacity = capacity
        self.buffer = deque(maxlen=capacity)
        self.save_path = save_path

        # ç»Ÿè®¡ä¿¡æ¯
        self.total_added = 0
        self.total_sampled = 0

        logger.info(f"ğŸ›ï¸ è®°å¿†å®«æ®¿åˆå§‹åŒ–å®Œæˆï¼Œå®¹é‡: {capacity}")

    def push(self, experience: Dict):
        """å°†å•æ¡ç»éªŒå­˜å…¥ç¼“å†²åŒº"""
        # ä¸ºç»éªŒæ·»åŠ æ—¶é—´æˆ³å’ŒID
        enhanced_experience = {
            **experience,
            "timestamp": self.total_added,
            "experience_id": f"exp_{self.total_added}",
        }

        self.buffer.append(enhanced_experience)
        self.total_added += 1

        if self.total_added % 1000 == 0:
            logger.debug(f"è®°å¿†å®«æ®¿å·²å­˜å‚¨ {self.total_added} æ¡ç»éªŒ")

    def push_batch(self, experiences: List[Dict]):
        """æ‰¹é‡å­˜å…¥ç»éªŒ"""
        for exp in experiences:
            self.push(exp)

    def sample(self, batch_size: int, strategy: str = "random") -> List[Dict]:
        """ä»ç¼“å†²åŒºä¸­æŠ½æ ·ç»éªŒ"""
        if len(self.buffer) < batch_size:
            return list(self.buffer)

        if strategy == "random":
            sampled = random.sample(self.buffer, batch_size)
        elif strategy == "recent_bias":
            # åå‘äºé‡‡æ ·æ›´æ–°çš„ç»éªŒ
            sampled = self._sample_with_recent_bias(batch_size)
        elif strategy == "diverse":
            # å°½é‡é‡‡æ ·å¤šæ ·åŒ–çš„ç»éªŒ
            sampled = self._sample_diverse(batch_size)
        else:
            sampled = random.sample(self.buffer, batch_size)

        self.total_sampled += len(sampled)
        return sampled

    def _sample_with_recent_bias(self, batch_size: int) -> List[Dict]:
        """å¸¦æœ‰æ–°è¿‘åå‘çš„é‡‡æ ·"""
        buffer_list = list(self.buffer)

        # ä¸ºæ¯ä¸ªç»éªŒåˆ†é…æƒé‡ï¼Œè¶Šæ–°çš„æƒé‡è¶Šé«˜
        weights = []
        for i, exp in enumerate(buffer_list):
            # ä½¿ç”¨æŒ‡æ•°è¡°å‡æƒé‡
            age = len(buffer_list) - i
            weight = np.exp(-age / (len(buffer_list) * 0.3))
            weights.append(weight)

        # å½’ä¸€åŒ–æƒé‡
        weights = np.array(weights)
        weights = weights / weights.sum()

        # æ ¹æ®æƒé‡é‡‡æ ·
        indices = np.random.choice(
            len(buffer_list), size=batch_size, replace=False, p=weights
        )

        return [buffer_list[i] for i in indices]

    def _sample_diverse(self, batch_size: int) -> List[Dict]:
        """å¤šæ ·åŒ–é‡‡æ · - å°½é‡é€‰æ‹©ä¸åŒç±»å‹çš„ç»éªŒ"""
        buffer_list = list(self.buffer)

        # ç®€å•çš„å¤šæ ·åŒ–ç­–ç•¥ï¼šæŒ‰æ—¶é—´æˆ³åˆ†æ®µé‡‡æ ·
        segments = 5
        segment_size = len(buffer_list) // segments
        samples_per_segment = batch_size // segments

        sampled = []
        for i in range(segments):
            start_idx = i * segment_size
            end_idx = min((i + 1) * segment_size, len(buffer_list))
            segment_data = buffer_list[start_idx:end_idx]

            if segment_data:
                segment_samples = random.sample(
                    segment_data, min(samples_per_segment, len(segment_data))
                )
                sampled.extend(segment_samples)

        # å¦‚æœè¿˜éœ€è¦æ›´å¤šæ ·æœ¬ï¼Œéšæœºè¡¥å……
        remaining = batch_size - len(sampled)
        if remaining > 0:
            remaining_pool = [exp for exp in buffer_list if exp not in sampled]
            if remaining_pool:
                additional = random.sample(
                    remaining_pool, min(remaining, len(remaining_pool))
                )
                sampled.extend(additional)

        return sampled[:batch_size]

    def get_stats(self) -> Dict:
        """è·å–ç¼“å†²åŒºç»Ÿè®¡ä¿¡æ¯"""
        if not self.buffer:
            return {"size": 0, "capacity": self.capacity}

        # åˆ†æç»éªŒçš„å¤šæ ·æ€§
        complexity_levels = {}
        for exp in self.buffer:
            level = exp.get("complexity_level", "unknown")
            complexity_levels[level] = complexity_levels.get(level, 0) + 1

        return {
            "size": len(self.buffer),
            "capacity": self.capacity,
            "utilization": len(self.buffer) / self.capacity,
            "total_added": self.total_added,
            "total_sampled": self.total_sampled,
            "complexity_distribution": complexity_levels,
            "oldest_timestamp": (
                min(exp["timestamp"] for exp in self.buffer) if self.buffer else None
            ),
            "newest_timestamp": (
                max(exp["timestamp"] for exp in self.buffer) if self.buffer else None
            ),
        }

    def save_to_disk(self, filepath: Optional[str] = None):
        """ä¿å­˜ç¼“å†²åŒºåˆ°ç£ç›˜"""
        save_path = filepath or self.save_path
        if not save_path:
            logger.warning("æ²¡æœ‰æŒ‡å®šä¿å­˜è·¯å¾„ï¼Œè·³è¿‡ä¿å­˜")
            return

        os.makedirs(os.path.dirname(save_path), exist_ok=True)

        data = {
            "buffer": list(self.buffer),
            "capacity": self.capacity,
            "total_added": self.total_added,
            "total_sampled": self.total_sampled,
        }

        with open(save_path, "wb") as f:
            pickle.dump(data, f)

        logger.info(f"è®°å¿†å®«æ®¿å·²ä¿å­˜åˆ°: {save_path}")

    def load_from_disk(self, filepath: Optional[str] = None):
        """ä»ç£ç›˜åŠ è½½ç¼“å†²åŒº"""
        load_path = filepath or self.save_path
        if not load_path or not os.path.exists(load_path):
            logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•åŠ è½½: {load_path}")
            return

        with open(load_path, "rb") as f:
            data = pickle.load(f)

        self.buffer = deque(data["buffer"], maxlen=self.capacity)
        self.total_added = data["total_added"]
        self.total_sampled = data["total_sampled"]

        logger.info(f"è®°å¿†å®«æ®¿å·²ä»ç£ç›˜åŠ è½½: {load_path}")
        logger.info(f"åŠ è½½äº† {len(self.buffer)} æ¡ç»éªŒ")

    def clear(self):
        """æ¸…ç©ºç¼“å†²åŒº"""
        self.buffer.clear()
        self.total_added = 0
        self.total_sampled = 0
        logger.info("è®°å¿†å®«æ®¿å·²æ¸…ç©º")

    def __len__(self):
        return len(self.buffer)


class CumulativeLearningSystem:
    """ç´¯ç§¯å­¦ä¹ ç³»ç»Ÿ - æ•´åˆè®°å¿†å®«æ®¿çš„è®­ç»ƒç³»ç»Ÿ"""

    def __init__(self, replay_buffer: ReplayBuffer, config: Dict):
        self.replay_buffer = replay_buffer
        self.config = config

        # å­¦ä¹ å‚æ•°
        self.new_data_ratio = config.get(
            "new_data_ratio", 0.3
        )  # æ–°æ•°æ®åœ¨è®­ç»ƒæ‰¹æ¬¡ä¸­çš„æ¯”ä¾‹
        self.training_iterations_per_loop = config.get(
            "training_iterations_per_loop", 5
        )
        self.min_buffer_size = config.get("min_buffer_size", 100)

        logger.info("ğŸ§  ç´¯ç§¯å­¦ä¹ ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  æ–°æ•°æ®æ¯”ä¾‹: {self.new_data_ratio}")
        logger.info(f"  æ¯è½®è®­ç»ƒè¿­ä»£: {self.training_iterations_per_loop}")

    def prepare_training_batch(
        self, new_samples: List[Dict], batch_size: int
    ) -> List[Dict]:
        """å‡†å¤‡è®­ç»ƒæ‰¹æ¬¡ - æ–°æ—§æ•°æ®æ··åˆ"""
        if len(self.replay_buffer) < self.min_buffer_size:
            # ç¼“å†²åŒºæ•°æ®ä¸è¶³ï¼Œä¸»è¦ä½¿ç”¨æ–°æ•°æ®
            logger.debug("ç¼“å†²åŒºæ•°æ®ä¸è¶³ï¼Œä¸»è¦ä½¿ç”¨æ–°æ•°æ®")
            return new_samples[:batch_size]

        # è®¡ç®—æ–°æ—§æ•°æ®çš„æ•°é‡
        new_data_count = int(batch_size * self.new_data_ratio)
        old_data_count = batch_size - new_data_count

        # è·å–æ–°æ•°æ®
        new_batch = new_samples[:new_data_count] if new_samples else []

        # ä»è®°å¿†å®«æ®¿é‡‡æ ·æ—§æ•°æ®
        old_batch = self.replay_buffer.sample(old_data_count, strategy="diverse")

        # æ··åˆå¹¶æ‰“ä¹±
        mixed_batch = new_batch + old_batch
        random.shuffle(mixed_batch)

        logger.debug(f"è®­ç»ƒæ‰¹æ¬¡ç»„æˆ: {len(new_batch)} æ–°æ•°æ® + {len(old_batch)} æ—§æ•°æ®")

        return mixed_batch

    def update_memory(self, new_experiences: List[Dict]):
        """æ›´æ–°è®°å¿†å®«æ®¿"""
        self.replay_buffer.push_batch(new_experiences)

        # å®šæœŸä¿å­˜
        if self.replay_buffer.total_added % 1000 == 0:
            self.replay_buffer.save_to_disk()


def test_memory_system():
    """æµ‹è¯•è®°å¿†ç³»ç»Ÿ"""
    print("ğŸ§ª æµ‹è¯•ç´¯ç§¯å­¦ä¹ è®°å¿†ç³»ç»Ÿ")
    print("=" * 50)

    # åˆ›å»ºè®°å¿†å®«æ®¿
    buffer = ReplayBuffer(capacity=1000, save_path="test_memory.pkl")

    # æ·»åŠ ä¸€äº›æµ‹è¯•ç»éªŒ
    test_experiences = [
        {
            "input_text": f"test_{i}",
            "target_text": f"target_{i}",
            "complexity_level": "simple",
        }
        for i in range(50)
    ]

    buffer.push_batch(test_experiences)
    print(f"âœ… æ·»åŠ äº† {len(test_experiences)} æ¡ç»éªŒ")

    # æµ‹è¯•é‡‡æ ·
    samples = buffer.sample(10, strategy="random")
    print(f"âœ… éšæœºé‡‡æ ·äº† {len(samples)} æ¡ç»éªŒ")

    # æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯
    stats = buffer.get_stats()
    print(f"ğŸ“Š ç¼“å†²åŒºç»Ÿè®¡: {stats}")

    # æµ‹è¯•ç´¯ç§¯å­¦ä¹ ç³»ç»Ÿ
    config = {
        "new_data_ratio": 0.3,
        "training_iterations_per_loop": 5,
        "min_buffer_size": 10,
    }

    learning_system = CumulativeLearningSystem(buffer, config)

    # æµ‹è¯•æ‰¹æ¬¡å‡†å¤‡
    new_samples = [{"input_text": "new_1", "target_text": "new_target_1"}]
    batch = learning_system.prepare_training_batch(new_samples, batch_size=20)
    print(f"âœ… å‡†å¤‡äº†å¤§å°ä¸º {len(batch)} çš„è®­ç»ƒæ‰¹æ¬¡")

    print("\nğŸ¯ ç´¯ç§¯å­¦ä¹ çš„æ ¸å¿ƒä¼˜åŠ¿:")
    print("  ğŸ›ï¸ è®°å¿†å®«æ®¿: é˜²æ­¢ç¾éš¾æ€§é—å¿˜")
    print("  ğŸ”„ æ–°æ—§æ··åˆ: æ¸©æ•…è€ŒçŸ¥æ–°")
    print("  ğŸ“ˆ æŒç»­ç§¯ç´¯: çŸ¥è¯†ä¸æ–­å¢é•¿")


if __name__ == "__main__":
    test_memory_system()
