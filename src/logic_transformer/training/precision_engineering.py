"""
æ–‡ä»¶å: src/logic_transformer/training/precision_engineering.py
ç¬¬ä¸€é˜¶æ®µï¼šç²¾å‡†å·¥ç¨‹ - æ™ºæ…§è°ƒé€Ÿå™¨ä¸å®‰å…¨åˆ¹è½¦
å®ç°è‡ªé€‚åº”å­¦ä¹ ç‡å’Œæ¢¯åº¦è£å‰ªï¼Œå¥ å®šç¨³å®šæ ¹åŸº
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from typing import Dict, List, Tuple, Optional
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class PrecisionTrainer:
    """ç²¾å‡†å·¥ç¨‹è®­ç»ƒå™¨ - å®ç°æ™ºæ…§è°ƒé€Ÿå™¨å’Œå®‰å…¨åˆ¹è½¦"""
    
    def __init__(self, model, tokenizer, config: Dict):
        self.model = model
        self.tokenizer = tokenizer
        self.config = config
        
        # æ™ºæ…§è°ƒé€Ÿå™¨ï¼šè‡ªé€‚åº”å­¦ä¹ ç‡
        self.optimizer = optim.AdamW(
            model.parameters(), 
            lr=config.get('initial_lr', 0.001),
            weight_decay=config.get('weight_decay', 1e-5)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨ - å½“éªŒè¯æŸå¤±ä¸å†æ”¹å–„æ—¶è‡ªåŠ¨é™ä½å­¦ä¹ ç‡
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',           # ç›‘æ§çš„æŒ‡æ ‡è¶Šå°è¶Šå¥½
            factor=config.get('lr_decay_factor', 0.5),     # å­¦ä¹ ç‡è¡°å‡å› å­
            patience=config.get('lr_patience', 2),         # å®¹å¿å¤šå°‘ä¸ªepochæ²¡æœ‰æ”¹å–„
            min_lr=config.get('min_lr', 1e-6),            # æœ€å°å­¦ä¹ ç‡
            verbose=True          # è°ƒæ•´æ—¶è¾“å‡ºæ—¥å¿—
        )
        
        # å®‰å…¨åˆ¹è½¦ï¼šæ¢¯åº¦è£å‰ªå‚æ•°
        self.max_grad_norm = config.get('max_grad_norm', 1.0)
        
        # è®­ç»ƒå†å²è®°å½•
        self.training_history = {
            'epochs': [],
            'train_loss': [],
            'val_loss': [],
            'learning_rates': [],
            'grad_norms': [],
            'clipped_steps': 0
        }
        
        logger.info("ğŸš€ ç²¾å‡†å·¥ç¨‹è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  åˆå§‹å­¦ä¹ ç‡: {config.get('initial_lr', 0.001)}")
        logger.info(f"  æ¢¯åº¦è£å‰ªé˜ˆå€¼: {self.max_grad_norm}")
        logger.info(f"  å­¦ä¹ ç‡è¡°å‡å› å­: {config.get('lr_decay_factor', 0.5)}")
    
    def train_step_with_precision(self, batch_data: List[Dict]) -> Dict:
        """æ‰§è¡Œä¸€ä¸ªç²¾å‡†è®­ç»ƒæ­¥éª¤"""
        self.model.train()
        
        total_loss = 0.0
        batch_size = len(batch_data)
        
        # æ¸…é›¶æ¢¯åº¦
        self.optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­
        for sample in batch_data:
            try:
                # è®¡ç®—æŸå¤±
                loss = self.model.train_step_improved(
                    sample['input'], 
                    sample['target'], 
                    self.tokenizer
                )
                total_loss += loss
            except Exception as e:
                logger.warning(f"è®­ç»ƒæ ·æœ¬å‡ºé”™: {e}")
                continue
        
        # å¹³å‡æŸå¤±
        avg_loss = total_loss / batch_size if batch_size > 0 else 0.0
        
        # åå‘ä¼ æ’­
        if avg_loss > 0:
            # è¿™é‡Œéœ€è¦ç¡®ä¿lossæ˜¯tensorå¹¶ä¸”requires_grad=True
            loss_tensor = torch.tensor(avg_loss, requires_grad=True)
            loss_tensor.backward()
            
            # ğŸ›¡ï¸ å®‰å…¨åˆ¹è½¦ï¼šæ¢¯åº¦è£å‰ª
            grad_norm = torch.nn.utils.clip_grad_norm_(
                self.model.parameters(), 
                max_norm=self.max_grad_norm
            )
            
            # è®°å½•æ¢¯åº¦ä¿¡æ¯
            self.training_history['grad_norms'].append(float(grad_norm))
            if grad_norm > self.max_grad_norm:
                self.training_history['clipped_steps'] += 1
                logger.debug(f"æ¢¯åº¦è¢«è£å‰ª: {grad_norm:.4f} -> {self.max_grad_norm}")
            
            # æ›´æ–°å‚æ•°
            self.optimizer.step()
        
        return {
            'loss': avg_loss,
            'grad_norm': float(grad_norm) if 'grad_norm' in locals() else 0.0,
            'learning_rate': self.optimizer.param_groups[0]['lr']
        }
    
    def validate_and_adjust(self, val_data: List[Dict]) -> Dict:
        """éªŒè¯å¹¶è°ƒæ•´å­¦ä¹ ç‡"""
        self.model.eval()
        
        total_val_loss = 0.0
        num_samples = 0
        
        with torch.no_grad():
            for sample in val_data:
                try:
                    # è¿™é‡Œéœ€è¦å®ç°éªŒè¯æŸå¤±è®¡ç®—
                    # æš‚æ—¶ä½¿ç”¨è®­ç»ƒæŸå¤±ä½œä¸ºä»£ç†
                    val_loss = self.model.train_step_improved(
                        sample['input'], 
                        sample['target'], 
                        self.tokenizer
                    )
                    total_val_loss += val_loss
                    num_samples += 1
                except Exception as e:
                    continue
        
        avg_val_loss = total_val_loss / num_samples if num_samples > 0 else float('inf')
        
        # ğŸ§  æ™ºæ…§è°ƒé€Ÿå™¨ï¼šæ ¹æ®éªŒè¯æŸå¤±è°ƒæ•´å­¦ä¹ ç‡
        old_lr = self.optimizer.param_groups[0]['lr']
        self.scheduler.step(avg_val_loss)
        new_lr = self.optimizer.param_groups[0]['lr']
        
        if new_lr != old_lr:
            logger.info(f"ğŸ¯ å­¦ä¹ ç‡è‡ªåŠ¨è°ƒæ•´: {old_lr:.6f} -> {new_lr:.6f}")
        
        return {
            'val_loss': avg_val_loss,
            'learning_rate': new_lr,
            'lr_adjusted': new_lr != old_lr
        }
    
    def get_training_stats(self) -> Dict:
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        if not self.training_history['grad_norms']:
            return {}
        
        grad_norms = self.training_history['grad_norms']
        
        return {
            'avg_grad_norm': np.mean(grad_norms),
            'max_grad_norm': np.max(grad_norms),
            'clipped_ratio': self.training_history['clipped_steps'] / len(grad_norms),
            'current_lr': self.optimizer.param_groups[0]['lr'],
            'total_steps': len(grad_norms)
        }
    
    def save_training_state(self, filepath: str):
        """ä¿å­˜è®­ç»ƒçŠ¶æ€"""
        state = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'training_history': self.training_history,
            'config': self.config
        }
        torch.save(state, filepath)
        logger.info(f"è®­ç»ƒçŠ¶æ€å·²ä¿å­˜: {filepath}")
    
    def load_training_state(self, filepath: str):
        """åŠ è½½è®­ç»ƒçŠ¶æ€"""
        state = torch.load(filepath)
        self.model.load_state_dict(state['model_state_dict'])
        self.optimizer.load_state_dict(state['optimizer_state_dict'])
        self.scheduler.load_state_dict(state['scheduler_state_dict'])
        self.training_history = state['training_history']
        logger.info(f"è®­ç»ƒçŠ¶æ€å·²åŠ è½½: {filepath}")


def create_precision_config() -> Dict:
    """åˆ›å»ºç²¾å‡†å·¥ç¨‹é…ç½®"""
    return {
        'initial_lr': 0.001,        # åˆå§‹å­¦ä¹ ç‡
        'lr_decay_factor': 0.5,     # å­¦ä¹ ç‡è¡°å‡å› å­
        'lr_patience': 2,           # å­¦ä¹ ç‡è°ƒæ•´çš„è€å¿ƒå€¼
        'min_lr': 1e-6,            # æœ€å°å­¦ä¹ ç‡
        'max_grad_norm': 1.0,      # æ¢¯åº¦è£å‰ªé˜ˆå€¼
        'weight_decay': 1e-5,      # æƒé‡è¡°å‡
    }


def test_precision_engineering():
    """æµ‹è¯•ç²¾å‡†å·¥ç¨‹æ¨¡å—"""
    print("ğŸ§ª æµ‹è¯•ç²¾å‡†å·¥ç¨‹æ¨¡å—")
    print("=" * 50)
    
    # åˆ›å»ºé…ç½®
    config = create_precision_config()
    print("âœ… é…ç½®åˆ›å»ºæˆåŠŸ")
    
    # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
    mock_batch = [
        {'input': [1, 2, 3], 'target': [4, 5, 6]},
        {'input': [7, 8, 9], 'target': [10, 11, 12]}
    ]
    
    print("âœ… æ¨¡æ‹Ÿæ•°æ®åˆ›å»ºæˆåŠŸ")
    print(f"ğŸ“Š é…ç½®å‚æ•°:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    
    print("\nğŸ¯ ç²¾å‡†å·¥ç¨‹çš„æ ¸å¿ƒç‰¹æ€§:")
    print("  ğŸ§  æ™ºæ…§è°ƒé€Ÿå™¨: è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒæ•´")
    print("  ğŸ›¡ï¸ å®‰å…¨åˆ¹è½¦: æ¢¯åº¦è£å‰ªé˜²æ­¢è®­ç»ƒå¤±æ§")
    print("  ğŸ“ˆ ç¨³å®šæ€§ä¿è¯: å¹³æ»‘çš„å­¦ä¹ æ›²çº¿")


if __name__ == "__main__":
    test_precision_engineering()
