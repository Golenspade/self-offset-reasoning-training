"""
æ–‡ä»¶å: formal_training_50_epochs.py
æ­£å¼çš„50è½®è®­ç»ƒ
ä½¿ç”¨æ··åˆç³»ç»Ÿè¿›è¡Œå®Œæ•´çš„è‡ªåç§»æ¨ç†è®­ç»ƒ
"""

import numpy as np
import matplotlib.pyplot as plt
import json
import time
import os
import sys
from pathlib import Path
from typing import List, Dict, Tuple

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / 'src'))

from logic_transformer.data_utils import Tokenizer, load_dataset
from logic_transformer.models.base_model import ImprovedSimpleModel
from hybrid_logic_system import HybridLogicSystem


class FormalTrainingSystem:
    """æ­£å¼è®­ç»ƒç³»ç»Ÿ"""
    
    def __init__(self):
        self.tokenizer = Tokenizer()
        self.model = None
        self.hybrid_system = None
        
        # è®­ç»ƒå‚æ•°
        self.total_epochs = 50
        self.batch_size = 8
        self.learning_rate = 0.002  # ç¨å¾®é™ä½å­¦ä¹ ç‡
        
        # è®°å½•è®­ç»ƒå†å²
        self.training_history = {
            'epochs': [],
            'train_loss': [],
            'val_exact_acc': [],
            'val_logical_acc': [],
            'hybrid_acc': [],
            'training_time': []
        }
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs('outputs/formal_training', exist_ok=True)
        os.makedirs('outputs/formal_training/models', exist_ok=True)
        os.makedirs('outputs/formal_training/figures', exist_ok=True)
        
    def initialize_model(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        print("ğŸš€ åˆå§‹åŒ–æ¨¡å‹...")
        
        self.model = ImprovedSimpleModel(
            vocab_size=self.tokenizer.vocab_size,
            hidden_size=128,
            max_length=50,
            learning_rate=self.learning_rate
        )
        
        # å°è¯•åŠ è½½å·²æœ‰çš„æœ€ä½³æ¨¡å‹ä½œä¸ºèµ·ç‚¹
        existing_model_path = 'outputs/trained_models/robust_model_Level_1_é²æ£’ç‰ˆ.npz'
        if os.path.exists(existing_model_path):
            if self.model.load_model(existing_model_path):
                print(f"âœ… åŠ è½½å·²æœ‰æ¨¡å‹: {existing_model_path}")
            else:
                print(f"âš ï¸ æ— æ³•åŠ è½½å·²æœ‰æ¨¡å‹ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
        else:
            print(f"ğŸ“ ä»å¤´å¼€å§‹è®­ç»ƒæ–°æ¨¡å‹")
        
        # åˆ›å»ºæ··åˆç³»ç»Ÿ
        self.hybrid_system = HybridLogicSystem(self.model, self.tokenizer)
        
    def load_training_data(self):
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        print("ğŸ“š åŠ è½½è®­ç»ƒæ•°æ®...")
        
        # åŠ è½½é²æ£’æ•°æ®é›†
        train_files = [
            'data/train_level_1_é²æ£’ç‰ˆ.json',
            'data/train_level_2_é²æ£’ç‰ˆ.json',
            'data/train_level_3_é²æ£’ç‰ˆ.json'
        ]
        
        val_files = [
            'data/val_level_1_é²æ£’ç‰ˆ.json',
            'data/val_level_2_é²æ£’ç‰ˆ.json',
            'data/val_level_3_é²æ£’ç‰ˆ.json'
        ]
        
        # åˆå¹¶æ‰€æœ‰è®­ç»ƒæ•°æ®
        all_train_data = []
        all_val_data = []
        
        for train_file in train_files:
            if os.path.exists(train_file):
                data = load_dataset(train_file, self.tokenizer, 1000)  # æ¯ä¸ªçº§åˆ«1000æ ·æœ¬
                if data:
                    all_train_data.extend(data)
                    print(f"  âœ… åŠ è½½è®­ç»ƒæ•°æ®: {train_file} ({len(data)} æ ·æœ¬)")
        
        for val_file in val_files:
            if os.path.exists(val_file):
                data = load_dataset(val_file, self.tokenizer, 100)  # æ¯ä¸ªçº§åˆ«100æ ·æœ¬
                if data:
                    all_val_data.extend(data)
                    print(f"  âœ… åŠ è½½éªŒè¯æ•°æ®: {val_file} ({len(data)} æ ·æœ¬)")
        
        print(f"ğŸ“Š æ€»è®­ç»ƒæ ·æœ¬: {len(all_train_data)}")
        print(f"ğŸ“Š æ€»éªŒè¯æ ·æœ¬: {len(all_val_data)}")
        
        return all_train_data, all_val_data
    
    def evaluate_model(self, val_data: List[Dict], epoch: int) -> Tuple[float, float, float]:
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        if not val_data:
            return 0.0, 0.0, 0.0
        
        correct_exact = 0
        correct_logical = 0
        correct_hybrid = 0
        total = min(len(val_data), 50)  # è¯„ä¼°50ä¸ªæ ·æœ¬
        
        for i, sample in enumerate(val_data[:total]):
            try:
                # 1. åŸå§‹æ¨¡å‹é¢„æµ‹
                predicted_tokens = self.model.predict(sample['input'], self.tokenizer)
                predicted_text = self.tokenizer.decode(predicted_tokens).strip()
                target_text = sample['target_text'].strip()
                
                # ç²¾ç¡®åŒ¹é…
                if predicted_text == target_text:
                    correct_exact += 1
                    correct_logical += 1
                    correct_hybrid += 1
                else:
                    # é€»è¾‘åŒ¹é…ï¼ˆåŸºæœ¬ç»“æ„æ­£ç¡®ï¼‰
                    if (len(predicted_text) > 5 and 
                        '->' in predicted_text and 
                        '~' in predicted_text and
                        not predicted_text.startswith('-> -> ->')):
                        correct_logical += 1
                
                # 2. æ··åˆç³»ç»Ÿé¢„æµ‹
                try:
                    neural_output, hybrid_output, intent = self.hybrid_system.generate_hybrid_solution(sample['input_text'])
                    if hybrid_output.strip() == target_text:
                        correct_hybrid += 1
                    elif hybrid_output.replace(' ', '') == target_text.replace(' ', ''):
                        correct_hybrid += 1  # å…è®¸ç©ºæ ¼å·®å¼‚
                except:
                    pass
                
            except Exception as e:
                continue
        
        exact_acc = correct_exact / total if total > 0 else 0
        logical_acc = correct_logical / total if total > 0 else 0
        hybrid_acc = correct_hybrid / total if total > 0 else 0
        
        return exact_acc, logical_acc, hybrid_acc
    
    def train_epoch(self, train_data: List[Dict], epoch: int) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        total_loss = 0
        num_batches = 0
        
        # æ‰“ä¹±æ•°æ®
        np.random.shuffle(train_data)
        
        # æ‰¹æ¬¡è®­ç»ƒ
        for i in range(0, len(train_data), self.batch_size):
            batch = train_data[i:i+self.batch_size]
            batch_loss = 0
            
            for sample in batch:
                loss = self.model.train_step_improved(sample['input'], sample['target'], self.tokenizer)
                batch_loss += loss
            
            total_loss += batch_loss / len(batch)
            num_batches += 1
        
        avg_loss = total_loss / num_batches if num_batches > 0 else 0
        return avg_loss
    
    def save_checkpoint(self, epoch: int, is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        # ä¿å­˜æ¨¡å‹
        if is_best:
            model_path = f'outputs/formal_training/models/best_model_epoch_{epoch}.npz'
        else:
            model_path = f'outputs/formal_training/models/model_epoch_{epoch}.npz'
        
        self.model.save_model(model_path)
        
        # ä¿å­˜è®­ç»ƒå†å²
        history_path = 'outputs/formal_training/training_history.json'
        with open(history_path, 'w') as f:
            json.dump(self.training_history, f, indent=2)
    
    def plot_training_progress(self):
        """ç»˜åˆ¶è®­ç»ƒè¿›åº¦"""
        if not self.training_history['epochs']:
            return
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('æ­£å¼è®­ç»ƒ50è½® - è‡ªåç§»æ¨ç†è®­ç»ƒè¿›åº¦', fontsize=16, fontweight='bold')
        
        epochs = self.training_history['epochs']
        
        # 1. è®­ç»ƒæŸå¤±
        ax1.plot(epochs, self.training_history['train_loss'], 'b-', linewidth=2, marker='o', markersize=3)
        ax1.set_title('è®­ç»ƒæŸå¤±', fontsize=14, fontweight='bold')
        ax1.set_xlabel('è®­ç»ƒè½®æ¬¡')
        ax1.set_ylabel('æŸå¤±å€¼')
        ax1.grid(True, alpha=0.3)
        
        # 2. å‡†ç¡®ç‡å¯¹æ¯”
        ax2.plot(epochs, [acc * 100 for acc in self.training_history['val_exact_acc']], 
                'r-', linewidth=2, marker='s', markersize=3, label='ç²¾ç¡®åŒ¹é…')
        ax2.plot(epochs, [acc * 100 for acc in self.training_history['val_logical_acc']], 
                'g-', linewidth=2, marker='^', markersize=3, label='é€»è¾‘æ­£ç¡®')
        ax2.plot(epochs, [acc * 100 for acc in self.training_history['hybrid_acc']], 
                'purple', linewidth=2, marker='D', markersize=3, label='æ··åˆç³»ç»Ÿ')
        ax2.set_title('éªŒè¯å‡†ç¡®ç‡å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax2.set_xlabel('è®­ç»ƒè½®æ¬¡')
        ax2.set_ylabel('å‡†ç¡®ç‡ (%)')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. è®­ç»ƒæ—¶é—´
        if self.training_history['training_time']:
            ax3.plot(epochs, self.training_history['training_time'], 'orange', linewidth=2, marker='o', markersize=3)
            ax3.set_title('æ¯è½®è®­ç»ƒæ—¶é—´', fontsize=14, fontweight='bold')
            ax3.set_xlabel('è®­ç»ƒè½®æ¬¡')
            ax3.set_ylabel('æ—¶é—´ (ç§’)')
            ax3.grid(True, alpha=0.3)
        
        # 4. å­¦ä¹ æ•ˆç‡ï¼ˆå‡†ç¡®ç‡æå‡é€Ÿåº¦ï¼‰
        if len(self.training_history['hybrid_acc']) > 1:
            acc_diff = np.diff(self.training_history['hybrid_acc'])
            ax4.plot(epochs[1:], acc_diff, 'teal', linewidth=2, marker='v', markersize=3)
            ax4.axhline(y=0, color='black', linestyle='--', alpha=0.5)
            ax4.set_title('æ··åˆç³»ç»Ÿå‡†ç¡®ç‡å˜åŒ–ç‡', fontsize=14, fontweight='bold')
            ax4.set_xlabel('è®­ç»ƒè½®æ¬¡')
            ax4.set_ylabel('å‡†ç¡®ç‡å˜åŒ–')
            ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾åƒ
        plt.savefig('outputs/formal_training/figures/training_progress.png', dpi=300, bbox_inches='tight')
        plt.savefig('outputs/formal_training/figures/training_progress.pdf', bbox_inches='tight')
        plt.show()
    
    def run_formal_training(self):
        """è¿è¡Œæ­£å¼è®­ç»ƒ"""
        print("ğŸ¯ å¼€å§‹æ­£å¼çš„50è½®è®­ç»ƒ")
        print("=" * 80)
        
        # åˆå§‹åŒ–
        self.initialize_model()
        train_data, val_data = self.load_training_data()
        
        if not train_data:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒæ•°æ®")
            return
        
        best_hybrid_acc = 0.0
        
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒå¾ªç¯...")
        print(f"æ€»è½®æ¬¡: {self.total_epochs}")
        print(f"è®­ç»ƒæ ·æœ¬: {len(train_data)}")
        print(f"éªŒè¯æ ·æœ¬: {len(val_data)}")
        print("=" * 80)
        
        for epoch in range(1, self.total_epochs + 1):
            start_time = time.time()
            
            # è®­ç»ƒä¸€ä¸ªepoch
            train_loss = self.train_epoch(train_data, epoch)
            
            # è¯„ä¼°æ¨¡å‹
            exact_acc, logical_acc, hybrid_acc = self.evaluate_model(val_data, epoch)
            
            # è®°å½•æ—¶é—´
            epoch_time = time.time() - start_time
            
            # æ›´æ–°å†å²è®°å½•
            self.training_history['epochs'].append(epoch)
            self.training_history['train_loss'].append(train_loss)
            self.training_history['val_exact_acc'].append(exact_acc)
            self.training_history['val_logical_acc'].append(logical_acc)
            self.training_history['hybrid_acc'].append(hybrid_acc)
            self.training_history['training_time'].append(epoch_time)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
            is_best = hybrid_acc > best_hybrid_acc
            if is_best:
                best_hybrid_acc = hybrid_acc
            
            # æ‰“å°è¿›åº¦
            print(f"Epoch {epoch:2d}/{self.total_epochs}: "
                  f"Loss={train_loss:.4f}, "
                  f"ç²¾ç¡®={exact_acc:.1%}, "
                  f"é€»è¾‘={logical_acc:.1%}, "
                  f"æ··åˆ={hybrid_acc:.1%}, "
                  f"æ—¶é—´={epoch_time:.1f}s"
                  f"{' ğŸ†' if is_best else ''}")
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if epoch % 10 == 0 or is_best:
                self.save_checkpoint(epoch, is_best)
            
            # å®šæœŸç»˜åˆ¶è¿›åº¦å›¾
            if epoch % 10 == 0:
                self.plot_training_progress()
        
        # æœ€ç»ˆä¿å­˜å’Œæ€»ç»“
        self.save_checkpoint(self.total_epochs, False)
        self.plot_training_progress()
        
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print(f"æœ€ä½³æ··åˆç³»ç»Ÿå‡†ç¡®ç‡: {best_hybrid_acc:.2%}")
        print(f"æœ€ç»ˆæ··åˆç³»ç»Ÿå‡†ç¡®ç‡: {hybrid_acc:.2%}")
        print(f"è®­ç»ƒå†å²å·²ä¿å­˜åˆ°: outputs/formal_training/")


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®éšæœºç§å­
    np.random.seed(42)
    
    # åˆ›å»ºè®­ç»ƒç³»ç»Ÿ
    training_system = FormalTrainingSystem()
    
    # è¿è¡Œæ­£å¼è®­ç»ƒ
    training_system.run_formal_training()


if __name__ == "__main__":
    main()
