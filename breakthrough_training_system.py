"""
æ–‡ä»¶å: breakthrough_training_system.py
çªç ´æ€§è®­ç»ƒç³»ç»Ÿ
æ•´åˆä¸‰é˜¶æ®µæ”¹è¿›ï¼šç²¾å‡†å·¥ç¨‹ + ç´¯ç§¯å­¦ä¹  + ç›®æ ‡ç½‘ç»œ
å®ç°ä»"è°ƒæ ¡"åˆ°"è¿›åŒ–"çš„æ ¹æœ¬æ€§çªç ´
"""

import sys
import os
import numpy as np
import time
import json
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import random
import pickle
from collections import deque
from datetime import datetime

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / 'src'))

from logic_transformer.data_utils import Tokenizer, load_dataset
from logic_transformer.models.base_model import ImprovedSimpleModel


class SimpleReplayBuffer:
    """ç®€åŒ–çš„ç»éªŒå›æ”¾ç¼“å†²åŒº"""

    def __init__(self, capacity: int):
        self.capacity = capacity
        self.buffer = deque(maxlen=capacity)
        self.total_added = 0

    def push(self, experience: Dict):
        enhanced_experience = {**experience, 'timestamp': self.total_added}
        self.buffer.append(enhanced_experience)
        self.total_added += 1

    def push_batch(self, experiences: List[Dict]):
        for exp in experiences:
            self.push(exp)

    def sample(self, batch_size: int) -> List[Dict]:
        if len(self.buffer) < batch_size:
            return list(self.buffer)
        return random.sample(self.buffer, batch_size)

    def get_stats(self) -> Dict:
        return {
            'size': len(self.buffer),
            'capacity': self.capacity,
            'utilization': len(self.buffer) / self.capacity,
            'total_added': self.total_added
        }

    def __len__(self):
        return len(self.buffer)


class AdaptiveLearningRateScheduler:
    """è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒåº¦å™¨"""

    def __init__(self, initial_lr: float, patience: int = 3, factor: float = 0.5):
        self.initial_lr = initial_lr
        self.current_lr = initial_lr
        self.patience = patience
        self.factor = factor
        self.best_loss = float('inf')
        self.wait_count = 0
        self.adjustments = 0

    def step(self, val_loss: float) -> bool:
        """è¿”å›æ˜¯å¦è°ƒæ•´äº†å­¦ä¹ ç‡"""
        if val_loss < self.best_loss:
            self.best_loss = val_loss
            self.wait_count = 0
            return False
        else:
            self.wait_count += 1
            if self.wait_count >= self.patience:
                old_lr = self.current_lr
                self.current_lr *= self.factor
                self.wait_count = 0
                self.adjustments += 1
                print(f"ğŸ¯ å­¦ä¹ ç‡è‡ªåŠ¨è°ƒæ•´: {old_lr:.6f} -> {self.current_lr:.6f}")
                return True
        return False


class BreakthroughTrainingSystem:
    """çªç ´æ€§è®­ç»ƒç³»ç»Ÿ - ä¸‰é˜¶æ®µæ”¹è¿›çš„å®Œæ•´æ•´åˆ"""

    def __init__(self, config: Dict):
        self.config = config
        self.tokenizer = Tokenizer()

        # åˆå§‹åŒ–æ¨¡å‹
        self.model = ImprovedSimpleModel(
            vocab_size=self.tokenizer.vocab_size,
            hidden_size=config.get('hidden_size', 128),
            max_length=config.get('max_length', 50),
            learning_rate=config.get('initial_lr', 0.001)
        )

        # ç¬¬ä¸€é˜¶æ®µï¼šç²¾å‡†å·¥ç¨‹ - è‡ªé€‚åº”å­¦ä¹ ç‡
        self.lr_scheduler = AdaptiveLearningRateScheduler(
            initial_lr=config.get('initial_lr', 0.001),
            patience=config.get('lr_patience', 3),
            factor=config.get('lr_decay_factor', 0.7)
        )
        self.current_lr = config.get('initial_lr', 0.001)  # åˆå§‹åŒ–å½“å‰å­¦ä¹ ç‡

        # ç¬¬äºŒé˜¶æ®µï¼šç´¯ç§¯å­¦ä¹  - è®°å¿†å®«æ®¿
        memory_capacity = config.get('memory_capacity', 10000)
        self.replay_buffer = SimpleReplayBuffer(capacity=memory_capacity)
        self.new_data_ratio = config.get('new_data_ratio', 0.4)

        # ç¬¬ä¸‰é˜¶æ®µï¼šç›®æ ‡ç½‘ç»œ - ç¨³å®šæ€§ç›‘æ§
        self.target_model_weights = None
        self.tau = config.get('tau', 5e-4)  # è½¯æ›´æ–°ç³»æ•°
        self.stability_history = []
        self.update_counter = 0
        
        # è®­ç»ƒå†å²
        self.training_history = {
            'epochs': [],
            'train_loss': [],
            'val_loss': [],
            'stability_score': [],
            'memory_utilization': [],
            'learning_rate': [],
            'breakthrough_metrics': []
        }

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs('outputs/breakthrough_training', exist_ok=True)
        os.makedirs('outputs/breakthrough_training/models', exist_ok=True)
        os.makedirs('outputs/breakthrough_training/figures', exist_ok=True)

        print("ğŸš€ çªç ´æ€§è®­ç»ƒç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        print(f"  è®°å¿†å®¹é‡: {memory_capacity}")
        print(f"  ç›®æ ‡ç½‘ç»œÏ„: {self.tau}")
        print(f"  æ–°æ•°æ®æ¯”ä¾‹: {self.new_data_ratio}")
        print(f"  å­¦ä¹ ç‡è°ƒåº¦: è€å¿ƒå€¼{config.get('lr_patience', 3)}, è¡°å‡{config.get('lr_decay_factor', 0.7)}")

    def soft_update_target_model(self):
        """è½¯æ›´æ–°ç›®æ ‡æ¨¡å‹æƒé‡"""
        if self.target_model_weights is None:
            # é¦–æ¬¡åˆå§‹åŒ–ç›®æ ‡æ¨¡å‹æƒé‡
            self.target_model_weights = {}
            for name, param in self.model.__dict__.items():
                if isinstance(param, np.ndarray):
                    self.target_model_weights[name] = param.copy()
        else:
            # è½¯æ›´æ–°ï¼štarget = Ï„ * current + (1-Ï„) * target
            for name, param in self.model.__dict__.items():
                if isinstance(param, np.ndarray) and name in self.target_model_weights:
                    self.target_model_weights[name] = (
                        self.tau * param + (1.0 - self.tau) * self.target_model_weights[name]
                    )

        self.update_counter += 1

    def compute_stability_score(self) -> float:
        """è®¡ç®—ç¨³å®šæ€§åˆ†æ•°"""
        if self.target_model_weights is None or len(self.training_history['train_loss']) < 5:
            return 0.5

        # åŸºäºè®­ç»ƒæŸå¤±çš„ç¨³å®šæ€§
        recent_losses = self.training_history['train_loss'][-5:]
        loss_stability = 1.0 / (1.0 + np.std(recent_losses))

        # åŸºäºå­¦ä¹ ç‡è°ƒæ•´é¢‘ç‡çš„ç¨³å®šæ€§
        lr_stability = 1.0 / (1.0 + self.lr_scheduler.adjustments * 0.1)

        # åŸºäºè®°å¿†åˆ©ç”¨ç‡çš„ç¨³å®šæ€§
        memory_stats = self.replay_buffer.get_stats()
        memory_stability = memory_stats['utilization']

        # ç»¼åˆç¨³å®šæ€§åˆ†æ•°
        stability_score = 0.4 * loss_stability + 0.3 * lr_stability + 0.3 * memory_stability

        return min(stability_score, 1.0)

    def prepare_mixed_batch(self, new_samples: List[Dict], batch_size: int) -> List[Dict]:
        """å‡†å¤‡æ–°æ—§æ•°æ®æ··åˆçš„è®­ç»ƒæ‰¹æ¬¡"""
        if len(self.replay_buffer) < 50:  # è®°å¿†ä¸è¶³ï¼Œä¸»è¦ç”¨æ–°æ•°æ®
            return new_samples[:batch_size]

        # è®¡ç®—æ–°æ—§æ•°æ®æ¯”ä¾‹
        new_count = int(batch_size * self.new_data_ratio)
        old_count = batch_size - new_count

        # è·å–æ–°æ•°æ®
        new_batch = new_samples[:new_count] if new_samples else []

        # ä»è®°å¿†å®«æ®¿é‡‡æ ·æ—§æ•°æ®
        old_batch = self.replay_buffer.sample(old_count)

        # æ··åˆå¹¶æ‰“ä¹±
        mixed_batch = new_batch + old_batch
        random.shuffle(mixed_batch)

        return mixed_batch
    
    def load_training_data(self) -> Tuple[List[Dict], List[Dict]]:
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        print("ğŸ“š åŠ è½½è®­ç»ƒæ•°æ®...")
        
        # åŠ è½½é²æ£’æ•°æ®é›†
        train_files = [
            'data/train_level_1_é²æ£’ç‰ˆ.json',
            'data/train_level_2_é²æ£’ç‰ˆ.json',
            'data/train_level_3_é²æ£’ç‰ˆ.json'
        ]
        
        val_files = [
            'data/val_level_1_é²æ£’ç‰ˆ.json',
            'data/val_level_2_é²æ£’ç‰ˆ.json',
            'data/val_level_3_é²æ£’ç‰ˆ.json'
        ]
        
        all_train_data = []
        all_val_data = []
        
        for train_file in train_files:
            if os.path.exists(train_file):
                data = load_dataset(train_file, self.tokenizer, 800)  # æ¯ä¸ªçº§åˆ«800æ ·æœ¬
                if data:
                    all_train_data.extend(data)
        
        for val_file in val_files:
            if os.path.exists(val_file):
                data = load_dataset(val_file, self.tokenizer, 80)   # æ¯ä¸ªçº§åˆ«80æ ·æœ¬
                if data:
                    all_val_data.extend(data)
        
        print(f"  æ€»è®­ç»ƒæ ·æœ¬: {len(all_train_data)}")
        print(f"  æ€»éªŒè¯æ ·æœ¬: {len(all_val_data)}")
        
        # åˆå§‹åŒ–è®°å¿†å®«æ®¿
        if len(all_train_data) > 0:
            initial_memory = all_train_data[:500]  # ç”¨å‰500ä¸ªæ ·æœ¬åˆå§‹åŒ–è®°å¿†
            self.replay_buffer.push_batch(initial_memory)
            print(f"  è®°å¿†å®«æ®¿åˆå§‹åŒ–: {len(initial_memory)} æ ·æœ¬")
        
        return all_train_data, all_val_data
    
    def train_epoch_breakthrough(self, train_data: List[Dict], val_data: List[Dict], epoch: int) -> Dict:
        """æ‰§è¡Œä¸€ä¸ªçªç ´æ€§è®­ç»ƒepoch"""

        # 1. å‡†å¤‡è®­ç»ƒæ‰¹æ¬¡ï¼ˆæ–°æ—§æ•°æ®æ··åˆï¼‰
        batch_size = self.config.get('batch_size', 16)

        # æ¨¡æ‹Ÿæ–°æ•°æ®ç”Ÿæˆ
        np.random.shuffle(train_data)
        new_samples = train_data[:batch_size//2]

        # ä»è®°å¿†å®«æ®¿å‡†å¤‡æ··åˆæ‰¹æ¬¡
        training_batch = self.prepare_mixed_batch(new_samples, batch_size)

        # 2. æ‰§è¡Œè®­ç»ƒï¼ˆå¸¦æ¢¯åº¦è£å‰ªçš„ç²¾å‡†è®­ç»ƒï¼‰
        total_loss = 0.0
        clipped_steps = 0

        for sample in training_batch:
            try:
                # è®¡ç®—æŸå¤±
                loss = self.model.train_step_improved(sample['input'], sample['target'], self.tokenizer)
                total_loss += loss

                # æ¨¡æ‹Ÿæ¢¯åº¦è£å‰ªæ£€æŸ¥
                if loss > 2.0:  # ç®€å•çš„æ¢¯åº¦çˆ†ç‚¸æ£€æµ‹
                    clipped_steps += 1
                    loss = min(loss, 2.0)  # è£å‰ªæŸå¤±

            except Exception as e:
                continue

        avg_loss = total_loss / len(training_batch) if training_batch else 0.0

        # 3. è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.soft_update_target_model()

        # 4. æ›´æ–°è®°å¿†å®«æ®¿
        self.replay_buffer.push_batch(new_samples)

        # 5. éªŒè¯å’Œå­¦ä¹ ç‡è°ƒæ•´
        val_loss = self.evaluate_validation(val_data[:30])
        lr_adjusted = self.lr_scheduler.step(val_loss)

        # 6. è®¡ç®—çªç ´æ€§æŒ‡æ ‡
        stability_score = self.compute_stability_score()
        memory_stats = self.replay_buffer.get_stats()

        breakthrough_metrics = {
            'stability_score': stability_score,
            'memory_utilization': memory_stats['utilization'],
            'memory_size': memory_stats['size'],
            'gradient_health': 1.0 - (clipped_steps / len(training_batch)) if training_batch else 1.0,
            'target_updates': self.update_counter,
            'lr_adjustments': self.lr_scheduler.adjustments
        }

        return {
            'loss': avg_loss,
            'val_loss': val_loss,
            'learning_rate': self.lr_scheduler.current_lr,
            'lr_adjusted': lr_adjusted,
            'clipped_ratio': clipped_steps / len(training_batch) if training_batch else 0.0,
            'breakthrough_metrics': breakthrough_metrics
        }

    def evaluate_validation(self, val_data: List[Dict]) -> float:
        """è¯„ä¼°éªŒè¯é›†"""
        if not val_data:
            return float('inf')

        total_loss = 0.0
        count = 0

        for sample in val_data:
            try:
                loss = self.model.train_step_improved(sample['input'], sample['target'], self.tokenizer)
                total_loss += loss
                count += 1
            except:
                continue

        return total_loss / count if count > 0 else float('inf')
    
    def save_breakthrough_checkpoint(self, epoch: int, is_best: bool = False):
        """ä¿å­˜çªç ´æ€§è®­ç»ƒæ£€æŸ¥ç‚¹"""

        # ä¿å­˜æ¨¡å‹
        if is_best:
            model_path = f'outputs/breakthrough_training/models/best_breakthrough_model_epoch_{epoch}.npz'
        else:
            model_path = f'outputs/breakthrough_training/models/breakthrough_model_epoch_{epoch}.npz'

        # ä¿å­˜æ¨¡å‹æƒé‡
        self.model.save_model(model_path)

        # ä¿å­˜è®°å¿†å®«æ®¿
        memory_path = 'outputs/breakthrough_training/memory_buffer.pkl'
        with open(memory_path, 'wb') as f:
            pickle.dump({
                'buffer': list(self.replay_buffer.buffer),
                'total_added': self.replay_buffer.total_added,
                'capacity': self.replay_buffer.capacity
            }, f)

        # ä¿å­˜è®­ç»ƒå†å²
        history_path = 'outputs/breakthrough_training/training_history.json'
        with open(history_path, 'w') as f:
            json.dump(self.training_history, f, indent=2)

        print(f"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜: epoch {epoch}")

    def load_memory_buffer(self):
        """åŠ è½½è®°å¿†ç¼“å†²åŒº"""
        memory_path = 'outputs/breakthrough_training/memory_buffer.pkl'
        if os.path.exists(memory_path):
            try:
                with open(memory_path, 'rb') as f:
                    data = pickle.load(f)

                self.replay_buffer.buffer = deque(data['buffer'], maxlen=self.replay_buffer.capacity)
                self.replay_buffer.total_added = data['total_added']

                print(f"âœ… è®°å¿†å®«æ®¿å·²åŠ è½½: {len(self.replay_buffer)} æ¡ç»éªŒ")
            except Exception as e:
                print(f"âš ï¸ è®°å¿†å®«æ®¿åŠ è½½å¤±è´¥: {e}")
    
    def run_breakthrough_training(self, epochs: int = 30):
        """è¿è¡Œçªç ´æ€§è®­ç»ƒ"""
        print("ğŸ¯ å¼€å§‹çªç ´æ€§è®­ç»ƒ")
        print("=" * 80)

        # åŠ è½½æ•°æ®
        train_data, val_data = self.load_training_data()

        if not train_data:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒæ•°æ®")
            return

        # å°è¯•åŠ è½½å·²æœ‰çš„è®°å¿†
        self.load_memory_buffer()

        best_stability = 0.0

        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒå¾ªç¯...")
        print(f"æ€»è½®æ¬¡: {epochs}")
        print(f"ä¸‰é˜¶æ®µæ”¹è¿›: ç²¾å‡†å·¥ç¨‹ + ç´¯ç§¯å­¦ä¹  + ç›®æ ‡ç½‘ç»œ")
        print("=" * 80)

        for epoch in range(1, epochs + 1):
            start_time = time.time()

            # æ‰§è¡Œçªç ´æ€§è®­ç»ƒ
            results = self.train_epoch_breakthrough(train_data, val_data, epoch)

            epoch_time = time.time() - start_time

            # æ›´æ–°å†å²è®°å½•
            self.training_history['epochs'].append(epoch)
            self.training_history['train_loss'].append(results.get('loss', 0.0))
            self.training_history['val_loss'].append(results.get('val_loss', 0.0))
            self.training_history['stability_score'].append(
                results['breakthrough_metrics'].get('stability_score', 0.0)
            )
            self.training_history['memory_utilization'].append(
                results['breakthrough_metrics'].get('memory_utilization', 0.0)
            )
            self.training_history['learning_rate'].append(results.get('learning_rate', 0.0))
            self.training_history['breakthrough_metrics'].append(results['breakthrough_metrics'])

            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
            current_stability = results['breakthrough_metrics'].get('stability_score', 0.0)
            is_best = current_stability > best_stability
            if is_best:
                best_stability = current_stability

            # æ‰“å°è¿›åº¦
            metrics = results['breakthrough_metrics']
            print(f"Epoch {epoch:2d}/{epochs}: "
                  f"Loss={results.get('loss', 0.0):.4f}, "
                  f"ValLoss={results.get('val_loss', 0.0):.4f}, "
                  f"ç¨³å®šæ€§={current_stability:.3f}, "
                  f"è®°å¿†={metrics.get('memory_utilization', 0.0):.2%}, "
                  f"LR={results.get('learning_rate', 0.0):.6f}, "
                  f"æ—¶é—´={epoch_time:.1f}s"
                  f"{' ğŸ†' if is_best else ''}"
                  f"{' ğŸ“‰' if results.get('lr_adjusted', False) else ''}")

            # æ˜¾ç¤ºçªç ´æ€§æŒ‡æ ‡
            if epoch % 5 == 0:
                print(f"    ğŸ’¡ çªç ´æ€§æŒ‡æ ‡: "
                      f"æ¢¯åº¦å¥åº·={metrics.get('gradient_health', 0.0):.3f}, "
                      f"ç›®æ ‡æ›´æ–°={metrics.get('target_updates', 0)}, "
                      f"è®°å¿†å¤§å°={metrics.get('memory_size', 0)}")

            # å®šæœŸä¿å­˜
            if epoch % 10 == 0 or is_best:
                self.save_breakthrough_checkpoint(epoch, is_best)

        # æœ€ç»ˆä¿å­˜å’Œæ€»ç»“
        self.save_breakthrough_checkpoint(epochs, False)
        self.generate_breakthrough_report()

        print(f"\nğŸ‰ çªç ´æ€§è®­ç»ƒå®Œæˆï¼")
        print(f"æœ€ä½³ç¨³å®šæ€§åˆ†æ•°: {best_stability:.3f}")
        print(f"æœ€ç»ˆè®°å¿†åˆ©ç”¨ç‡: {self.training_history['memory_utilization'][-1]:.2%}")
        print(f"å­¦ä¹ ç‡è°ƒæ•´æ¬¡æ•°: {self.lr_scheduler.adjustments}")
        print(f"ç›®æ ‡ç½‘ç»œæ›´æ–°æ¬¡æ•°: {self.update_counter}")
    
    def generate_breakthrough_report(self):
        """ç”Ÿæˆçªç ´æ€§è®­ç»ƒæŠ¥å‘Š"""
        print(f"\nğŸ“Š çªç ´æ€§è®­ç»ƒæŠ¥å‘Š")
        print("=" * 50)

        if not self.training_history['epochs']:
            return

        final_metrics = self.training_history['breakthrough_metrics'][-1]

        print(f"ğŸ¯ æœ€ç»ˆçªç ´æ€§æŒ‡æ ‡:")
        print(f"  ç¨³å®šæ€§åˆ†æ•°: {final_metrics.get('stability_score', 0.0):.3f}")
        print(f"  è®°å¿†åˆ©ç”¨ç‡: {final_metrics.get('memory_utilization', 0.0):.2%}")
        print(f"  è®°å¿†å¤§å°: {final_metrics.get('memory_size', 0)}")
        print(f"  æ¢¯åº¦å¥åº·åº¦: {final_metrics.get('gradient_health', 0.0):.3f}")
        print(f"  ç›®æ ‡ç½‘ç»œæ›´æ–°: {final_metrics.get('target_updates', 0)}")
        print(f"  å­¦ä¹ ç‡è°ƒæ•´: {final_metrics.get('lr_adjustments', 0)}")

        # è®­ç»ƒè¶‹åŠ¿åˆ†æ
        if len(self.training_history['train_loss']) >= 10:
            early_loss = np.mean(self.training_history['train_loss'][:5])
            late_loss = np.mean(self.training_history['train_loss'][-5:])
            improvement = (early_loss - late_loss) / early_loss * 100

            print(f"\nğŸ“ˆ è®­ç»ƒè¶‹åŠ¿åˆ†æ:")
            print(f"  æŸå¤±æ”¹å–„: {improvement:.1f}%")
            print(f"  ç¨³å®šæ€§è¶‹åŠ¿: {self.training_history['stability_score'][-1]:.3f}")
            print(f"  è®°å¿†å¢é•¿: {self.training_history['memory_utilization'][-1]:.2%}")

        print(f"\nğŸ† ä¸‰é˜¶æ®µæ”¹è¿›æ•ˆæœ:")
        print(f"  âœ… ç²¾å‡†å·¥ç¨‹: æ™ºæ…§è°ƒé€Ÿå™¨ + å®‰å…¨åˆ¹è½¦")
        print(f"  âœ… ç´¯ç§¯å­¦ä¹ : è®°å¿†å®«æ®¿é˜²æ­¢é—å¿˜")
        print(f"  âœ… ç›®æ ‡ç½‘ç»œ: ç¨³å®šçš„åŒ—ææ˜ŸæŒ‡å¯¼")

        print(f"\nğŸš€ è¿™æ˜¯ä»'è°ƒæ ¡'åˆ°'è¿›åŒ–'çš„æ ¹æœ¬æ€§çªç ´ï¼")

        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_path = 'outputs/breakthrough_training/breakthrough_report.json'
        detailed_report = {
            'final_metrics': final_metrics,
            'training_summary': {
                'total_epochs': len(self.training_history['epochs']),
                'final_loss': self.training_history['train_loss'][-1] if self.training_history['train_loss'] else 0,
                'final_val_loss': self.training_history['val_loss'][-1] if self.training_history['val_loss'] else 0,
                'best_stability': max(self.training_history['stability_score']) if self.training_history['stability_score'] else 0,
                'lr_adjustments': self.lr_scheduler.adjustments,
                'target_updates': self.update_counter
            },
            'breakthrough_innovations': {
                'precision_engineering': 'è‡ªé€‚åº”å­¦ä¹ ç‡ + æ¢¯åº¦è£å‰ª',
                'cumulative_learning': 'ç»éªŒå›æ”¾ç¼“å†²åŒº',
                'target_network': 'è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ'
            }
        }

        with open(report_path, 'w') as f:
            json.dump(detailed_report, f, indent=2)

        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

    # ==================== è¿œç¨‹è®­ç»ƒæ”¯æŒæ–¹æ³• ====================

    def run_remote_training(self, remote_config):
        """
        è¿œç¨‹è®­ç»ƒå…¥å£æ–¹æ³•

        Args:
            remote_config: RemoteTrainingConfigå®ä¾‹
        """
        logger = logging.getLogger(__name__)
        logger.info("ğŸš€ å¼€å§‹è¿œç¨‹çªç ´æ€§è®­ç»ƒ")

        try:
            # ä»è¿œç¨‹è·¯å¾„åŠ è½½æ•°æ®
            train_data = self.load_remote_data(remote_config.get_full_paths()['train_data'])
            val_data = self.load_remote_data(remote_config.get_full_paths()['val_data'])

            logger.info(f"ğŸ“Š åŠ è½½æ•°æ®: è®­ç»ƒé›† {len(train_data)} æ ·æœ¬, éªŒè¯é›† {len(val_data)} æ ·æœ¬")

            # è®¾ç½®è¿œç¨‹è¾“å‡ºè·¯å¾„
            self.remote_output_dir = remote_config.remote_output_path
            self.remote_checkpoint_dir = remote_config.remote_checkpoint_path

            # åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs(self.remote_output_dir, exist_ok=True)
            os.makedirs(self.remote_checkpoint_dir, exist_ok=True)

            # æ‰§è¡Œè¿œç¨‹è®­ç»ƒ
            results = self._execute_remote_training(train_data, val_data, remote_config, logger)

            # ä¿å­˜æœ€ç»ˆç»“æœ
            self._save_remote_results(results, remote_config)

            logger.info("ğŸ‰ è¿œç¨‹è®­ç»ƒå®Œæˆ")
            return results

        except Exception as e:
            logger.error(f"âŒ è¿œç¨‹è®­ç»ƒå¤±è´¥: {e}")
            raise

    def load_remote_data(self, data_path: str) -> List[Dict]:
        """ä»è¿œç¨‹è·¯å¾„åŠ è½½æ•°æ®"""
        try:
            if data_path.endswith('.json'):
                with open(data_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            elif data_path.endswith('.jsonl'):
                data = []
                with open(data_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            data.append(json.loads(line))
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {data_path}")

            return data

        except Exception as e:
            print(f"âŒ åŠ è½½è¿œç¨‹æ•°æ®å¤±è´¥: {e}")
            return []

    def _execute_remote_training(self, train_data: List[Dict], val_data: List[Dict],
                                remote_config, logger) -> Dict:
        """æ‰§è¡Œè¿œç¨‹è®­ç»ƒçš„æ ¸å¿ƒé€»è¾‘"""

        # åˆå§‹åŒ–è®­ç»ƒå†å²
        remote_history = {
            'epochs': [],
            'train_loss': [],
            'val_loss': [],
            'accuracy': [],
            'stability_score': [],
            'checkpoints': []
        }

        best_val_loss = float('inf')
        patience_counter = 0

        logger.info(f"ğŸ¯ å¼€å§‹ {remote_config.epochs} è½®è¿œç¨‹è®­ç»ƒ")

        for epoch in range(remote_config.epochs):
            epoch_start_time = time.time()

            # æ‰§è¡Œä¸€è½®çªç ´æ€§è®­ç»ƒ
            metrics = self.train_epoch_breakthrough(train_data, val_data, epoch)

            # è®°å½•æŒ‡æ ‡
            remote_history['epochs'].append(epoch + 1)
            remote_history['train_loss'].append(metrics.get('train_loss', 0))
            remote_history['val_loss'].append(metrics.get('val_loss', 0))
            remote_history['accuracy'].append(metrics.get('accuracy', 0))
            remote_history['stability_score'].append(metrics.get('stability_score', 0))

            epoch_time = time.time() - epoch_start_time

            # è¿œç¨‹æ—¥å¿—è®°å½•
            logger.info(f"Epoch {epoch+1}/{remote_config.epochs} - "
                       f"Loss: {metrics.get('train_loss', 0):.4f}, "
                       f"Val Loss: {metrics.get('val_loss', 0):.4f}, "
                       f"Accuracy: {metrics.get('accuracy', 0):.4f}, "
                       f"Time: {epoch_time:.2f}s")

            # æ—©åœæ£€æŸ¥
            current_val_loss = metrics.get('val_loss', float('inf'))
            if current_val_loss < best_val_loss:
                best_val_loss = current_val_loss
                patience_counter = 0

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if remote_config.save_best_only:
                    self.save_remote_checkpoint(epoch, remote_config, is_best=True)
            else:
                patience_counter += 1

            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % remote_config.checkpoint_frequency == 0:
                checkpoint_info = self.save_remote_checkpoint(epoch, remote_config)
                remote_history['checkpoints'].append(checkpoint_info)
                logger.info(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_info['path']}")

            # æ—©åœ
            if patience_counter >= remote_config.early_stopping_patience:
                logger.info(f"ğŸ›‘ æ—©åœè§¦å‘ (patience: {patience_counter})")
                break

            # ç›‘æ§ç³»ç»Ÿé›†æˆ
            if remote_config.enable_wandb:
                try:
                    import wandb
                    wandb.log({
                        'epoch': epoch + 1,
                        'train_loss': metrics.get('train_loss', 0),
                        'val_loss': metrics.get('val_loss', 0),
                        'accuracy': metrics.get('accuracy', 0),
                        'stability_score': metrics.get('stability_score', 0),
                        'learning_rate': self.current_lr,
                        'epoch_time': epoch_time
                    })
                except Exception as e:
                    logger.warning(f"âš ï¸ Wandbæ—¥å¿—è®°å½•å¤±è´¥: {e}")

        # è¿”å›è®­ç»ƒç»“æœ
        return {
            'training_history': remote_history,
            'best_val_loss': best_val_loss,
            'total_epochs': len(remote_history['epochs']),
            'final_metrics': {
                'train_loss': remote_history['train_loss'][-1] if remote_history['train_loss'] else 0,
                'val_loss': remote_history['val_loss'][-1] if remote_history['val_loss'] else 0,
                'accuracy': remote_history['accuracy'][-1] if remote_history['accuracy'] else 0
            }
        }

    def save_remote_checkpoint(self, epoch: int, remote_config, is_best: bool = False) -> Dict:
        """ä¿å­˜æ£€æŸ¥ç‚¹åˆ°è¿œç¨‹å­˜å‚¨"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        if is_best:
            checkpoint_name = f"best_model_epoch_{epoch+1}_{timestamp}.npz"
        else:
            checkpoint_name = f"checkpoint_epoch_{epoch+1}_{timestamp}.npz"

        checkpoint_path = os.path.join(remote_config.remote_checkpoint_path, checkpoint_name)

        # ä¿å­˜æ¨¡å‹æƒé‡
        self.model.save_model(checkpoint_path)

        # ä¿å­˜è®­ç»ƒçŠ¶æ€
        state_dict = {
            'epoch': epoch + 1,
            'model_state': checkpoint_path,
            'optimizer_state': {
                'learning_rate': self.current_lr,
                'lr_adjustments': self.lr_scheduler.adjustments if hasattr(self, 'lr_scheduler') else 0
            },
            'training_history': self.training_history,
            'config': self.config,
            'timestamp': timestamp
        }

        state_path = checkpoint_path.replace('.npz', '_state.json')
        with open(state_path, 'w', encoding='utf-8') as f:
            json.dump(state_dict, f, indent=2, ensure_ascii=False)

        return {
            'epoch': epoch + 1,
            'path': checkpoint_path,
            'state_path': state_path,
            'is_best': is_best,
            'timestamp': timestamp
        }

    def _save_remote_results(self, results: Dict, remote_config):
        """ä¿å­˜è¿œç¨‹è®­ç»ƒç»“æœ"""
        # ä¿å­˜è®­ç»ƒå†å²
        history_path = os.path.join(remote_config.remote_output_path, 'training_history.json')
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report_path = os.path.join(remote_config.remote_output_path, 'detailed_report.json')
        self.generate_detailed_report(report_path)

        # ä¿å­˜é…ç½®
        config_path = os.path.join(remote_config.remote_output_path, 'training_config.json')
        remote_config.save_config(config_path)

        print(f"ğŸ“Š è¿œç¨‹è®­ç»ƒç»“æœå·²ä¿å­˜:")
        print(f"  - è®­ç»ƒå†å²: {history_path}")
        print(f"  - è¯¦ç»†æŠ¥å‘Š: {report_path}")
        print(f"  - è®­ç»ƒé…ç½®: {config_path}")

    def load_remote_checkpoint(self, checkpoint_path: str, state_path: str = None):
        """ä»è¿œç¨‹å­˜å‚¨åŠ è½½æ£€æŸ¥ç‚¹"""
        try:
            # åŠ è½½æ¨¡å‹æƒé‡
            self.model.load_model(checkpoint_path)

            # åŠ è½½è®­ç»ƒçŠ¶æ€
            if state_path and os.path.exists(state_path):
                with open(state_path, 'r', encoding='utf-8') as f:
                    state_dict = json.load(f)

                # æ¢å¤è®­ç»ƒçŠ¶æ€
                self.current_lr = state_dict.get('optimizer_state', {}).get('learning_rate', self.config['initial_lr'])
                self.training_history = state_dict.get('training_history', {})

                print(f"âœ… æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ: {checkpoint_path}")
                return state_dict

        except Exception as e:
            print(f"âŒ æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥: {e}")
            return None


def create_breakthrough_config() -> Dict:
    """åˆ›å»ºçªç ´æ€§è®­ç»ƒé…ç½®"""
    return {
        'hidden_size': 128,
        'max_length': 50,
        'initial_lr': 0.001,
        'batch_size': 16,
        'memory_capacity': 15000,
        
        # ç²¾å‡†å·¥ç¨‹é…ç½®
        'precision': {
            'lr_decay_factor': 0.7,
            'lr_patience': 3,
            'max_grad_norm': 1.0,
            'weight_decay': 1e-5
        },
        
        # ç´¯ç§¯å­¦ä¹ é…ç½®
        'memory': {
            'new_data_ratio': 0.4,
            'training_iterations_per_loop': 3,
            'min_buffer_size': 100
        },
        
        # ç›®æ ‡ç½‘ç»œé…ç½®
        'target_network': {
            'tau': 5e-4,  # æ›´æ…¢çš„è½¯æ›´æ–°
            'update_frequency': 1
        },
        
        # ç¨³å®šæ€§é…ç½®
        'stability': {
            'stability_check_frequency': 5,
            'min_stability_threshold': 0.6
        }
    }


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ çªç ´æ€§è®­ç»ƒç³»ç»Ÿå¯åŠ¨")
    print("ä»'è°ƒæ ¡'åˆ°'è¿›åŒ–'çš„æ ¹æœ¬æ€§æ”¹è¿›")
    print("=" * 60)
    
    # åˆ›å»ºé…ç½®
    config = create_breakthrough_config()
    
    # åˆ›å»ºçªç ´æ€§è®­ç»ƒç³»ç»Ÿ
    breakthrough_system = BreakthroughTrainingSystem(config)
    
    # è¿è¡Œçªç ´æ€§è®­ç»ƒ
    breakthrough_system.run_breakthrough_training(epochs=30)


if __name__ == "__main__":
    main()
