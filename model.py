"""
æ–‡ä»¶å: model.py
Transformer Seq2Seqæ¨¡å‹å®šä¹‰
ç”¨äºå­¦ä¹ ä»å™ªå£°å‘½é¢˜åˆ°é€†å¦å‘½é¢˜çš„è½¬æ¢
æ”¯æŒCUDAåŠ é€Ÿå’Œæ··åˆç²¾åº¦è®­ç»ƒ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import logging
from typing import Optional, Tuple

logger = logging.getLogger(__name__)


class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç æ¨¡å—"""
    
    def __init__(self, d_model, max_len=100):
        super(PositionalEncoding, self).__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        return x + self.pe[:x.size(0), :]


class LogicTransformer(nn.Module):
    """
    ç”¨äºé€»è¾‘æ¨ç†çš„Transformer Seq2Seqæ¨¡å‹
    """
    
    def __init__(self, vocab_size, d_model=128, nhead=8, num_encoder_layers=3, 
                 num_decoder_layers=3, dim_feedforward=512, max_len=100):
        super(LogicTransformer, self).__init__()
        
        self.d_model = d_model
        self.vocab_size = vocab_size
        
        # è¯åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model, max_len)
        
        # Transformeræ ¸å¿ƒ
        self.transformer = nn.Transformer(
            d_model=d_model,
            nhead=nhead,
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            dim_feedforward=dim_feedforward,
            batch_first=False  # ä½¿ç”¨ (seq_len, batch, features) æ ¼å¼
        )
        
        # è¾“å‡ºæŠ•å½±å±‚
        self.output_projection = nn.Linear(d_model, vocab_size)
        
        # åˆå§‹åŒ–æƒé‡
        self.init_weights()
    
    def init_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        initrange = 0.1
        self.embedding.weight.data.uniform_(-initrange, initrange)
        self.output_projection.bias.data.zero_()
        self.output_projection.weight.data.uniform_(-initrange, initrange)
    
    def forward(self, src, tgt, src_mask=None, tgt_mask=None, 
                src_key_padding_mask=None, tgt_key_padding_mask=None):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            src: æºåºåˆ— (seq_len, batch_size)
            tgt: ç›®æ ‡åºåˆ— (seq_len, batch_size)
            src_mask: æºåºåˆ—mask
            tgt_mask: ç›®æ ‡åºåˆ—mask
            src_key_padding_mask: æºåºåˆ—padding mask
            tgt_key_padding_mask: ç›®æ ‡åºåˆ—padding mask
        """
        
        # è¯åµŒå…¥å’Œä½ç½®ç¼–ç 
        src_emb = self.embedding(src) * math.sqrt(self.d_model)
        tgt_emb = self.embedding(tgt) * math.sqrt(self.d_model)
        
        src_emb = self.pos_encoding(src_emb)
        tgt_emb = self.pos_encoding(tgt_emb)
        
        # Transformerå‰å‘ä¼ æ’­
        output = self.transformer(
            src_emb, tgt_emb,
            src_mask=src_mask,
            tgt_mask=tgt_mask,
            src_key_padding_mask=src_key_padding_mask,
            tgt_key_padding_mask=tgt_key_padding_mask
        )
        
        # è¾“å‡ºæŠ•å½±
        output = self.output_projection(output)
        
        return output
    
    def generate_square_subsequent_mask(self, sz):
        """ç”Ÿæˆå› æœmaskï¼Œé˜²æ­¢æ¨¡å‹çœ‹åˆ°æœªæ¥çš„token"""
        mask = torch.triu(torch.ones(sz, sz), diagonal=1)
        mask = mask.masked_fill(mask == 1, float('-inf'))
        return mask
    
    def encode(self, src, src_mask=None, src_key_padding_mask=None):
        """ç¼–ç å™¨å‰å‘ä¼ æ’­"""
        src_emb = self.embedding(src) * math.sqrt(self.d_model)
        src_emb = self.pos_encoding(src_emb)
        
        memory = self.transformer.encoder(
            src_emb, 
            mask=src_mask, 
            src_key_padding_mask=src_key_padding_mask
        )
        return memory
    
    def decode(self, tgt, memory, tgt_mask=None, tgt_key_padding_mask=None):
        """è§£ç å™¨å‰å‘ä¼ æ’­"""
        tgt_emb = self.embedding(tgt) * math.sqrt(self.d_model)
        tgt_emb = self.pos_encoding(tgt_emb)
        
        output = self.transformer.decoder(
            tgt_emb, memory,
            tgt_mask=tgt_mask,
            tgt_key_padding_mask=tgt_key_padding_mask
        )
        
        output = self.output_projection(output)
        return output


def create_padding_mask(seq, pad_token):
    """åˆ›å»ºpadding mask"""
    return (seq == pad_token)


def create_model(vocab_size, device='cpu'):
    """åˆ›å»ºå¹¶åˆå§‹åŒ–æ¨¡å‹"""
    model = LogicTransformer(
        vocab_size=vocab_size,
        d_model=128,
        nhead=8,
        num_encoder_layers=3,
        num_decoder_layers=3,
        dim_feedforward=512,
        max_len=100
    )
    
    model = model.to(device)
    
    # è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"æ¨¡å‹åˆ›å»ºæˆåŠŸ!")
    print(f"æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")
    
    return model


def inference(model, src_tokens, tokenizer, device='cpu', max_length=50):
    """
    æ¨ç†å‡½æ•°ï¼šç»™å®šè¾“å…¥åºåˆ—ï¼Œç”Ÿæˆè¾“å‡ºåºåˆ—
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        src_tokens: è¾“å…¥tokenåºåˆ—
        tokenizer: tokenizerå¯¹è±¡
        device: è®¾å¤‡
        max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
    
    Returns:
        ç”Ÿæˆçš„tokenåºåˆ—
    """
    model.eval()
    
    with torch.no_grad():
        # å‡†å¤‡è¾“å…¥
        src = torch.tensor(src_tokens).unsqueeze(1).to(device)  # (seq_len, 1)
        
        # ç¼–ç 
        memory = model.encode(src)
        
        # åˆå§‹åŒ–è§£ç åºåˆ—
        tgt_tokens = [tokenizer.START_TOKEN]
        
        for _ in range(max_length):
            tgt = torch.tensor(tgt_tokens).unsqueeze(1).to(device)  # (seq_len, 1)
            
            # åˆ›å»ºå› æœmask
            tgt_mask = model.generate_square_subsequent_mask(len(tgt_tokens)).to(device)
            
            # è§£ç 
            output = model.decode(tgt, memory, tgt_mask=tgt_mask)
            
            # è·å–ä¸‹ä¸€ä¸ªtoken
            next_token_logits = output[-1, 0, :]  # æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
            next_token = torch.argmax(next_token_logits).item()
            
            # å¦‚æœç”Ÿæˆäº†ç»“æŸtokenï¼Œåœæ­¢ç”Ÿæˆ
            if next_token == tokenizer.END_TOKEN:
                break
            
            tgt_tokens.append(next_token)
        
        return tgt_tokens[1:]  # å»æ‰START_TOKEN


def create_cuda_model(vocab_size: int, device: str = 'auto',
                     use_mixed_precision: bool = True, **kwargs) -> Tuple[LogicTransformer, torch.device]:
    """
    åˆ›å»ºCUDAä¼˜åŒ–çš„æ¨¡å‹

    Args:
        vocab_size: è¯æ±‡è¡¨å¤§å°
        device: è®¾å¤‡é€‰æ‹© ('auto', 'cpu', 'cuda', 'cuda:0'ç­‰)
        use_mixed_precision: æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦
        **kwargs: æ¨¡å‹å‚æ•°

    Returns:
        (model, device): æ¨¡å‹å’Œè®¾å¤‡
    """
    try:
        from cuda_utils import CUDAManager

        # è‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¾å¤‡
        if device == 'auto':
            cuda_manager = CUDAManager()
            device = cuda_manager.device
            cuda_manager.optimize_cuda_settings()
        else:
            device = torch.device(device)

        # åˆ›å»ºæ¨¡å‹
        model = LogicTransformer(
            vocab_size=vocab_size,
            d_model=kwargs.get('d_model', 128),
            nhead=kwargs.get('nhead', 8),
            num_encoder_layers=kwargs.get('num_encoder_layers', 3),
            num_decoder_layers=kwargs.get('num_decoder_layers', 3),
            dim_feedforward=kwargs.get('dim_feedforward', 512),
            max_len=kwargs.get('max_len', 100)
        )

        # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        model = model.to(device)

        # æ··åˆç²¾åº¦ä¼˜åŒ–
        if use_mixed_precision and device.type == 'cuda':
            # æ£€æŸ¥æ˜¯å¦æ”¯æŒæ··åˆç²¾åº¦
            props = torch.cuda.get_device_properties(device)
            if props.major >= 7:  # Voltaæ¶æ„åŠä»¥ä¸Š
                # å°†æ¨¡å‹è½¬æ¢ä¸ºåŠç²¾åº¦ï¼ˆåœ¨éœ€è¦æ—¶ï¼‰
                # æ³¨æ„ï¼šå®é™…çš„æ··åˆç²¾åº¦è®­ç»ƒé€šè¿‡GradScalerå®ç°
                logger.info("âœ… æ¨¡å‹æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒ")
            else:
                logger.warning(f"âš ï¸ GPUè®¡ç®—èƒ½åŠ›{props.major}.{props.minor}ä¸æ”¯æŒé«˜æ•ˆæ··åˆç²¾åº¦")
                use_mixed_precision = False

        # ç¼–è¯‘æ¨¡å‹ï¼ˆPyTorch 2.0+ï¼‰
        if hasattr(torch, 'compile') and device.type == 'cuda':
            try:
                model = torch.compile(model, mode='default')
                logger.info("ğŸš€ æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–å·²å¯ç”¨")
            except Exception as e:
                logger.warning(f"æ¨¡å‹ç¼–è¯‘å¤±è´¥: {e}")

        # ç»Ÿè®¡å‚æ•°
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

        logger.info(f"ğŸš€ CUDAæ¨¡å‹åˆ›å»ºæˆåŠŸ!")
        logger.info(f"ğŸ“ è®¾å¤‡: {device}")
        logger.info(f"ğŸ“Š æ€»å‚æ•°: {total_params:,}")
        logger.info(f"ğŸ¯ å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        logger.info(f"ğŸ”¥ æ··åˆç²¾åº¦: {'å¯ç”¨' if use_mixed_precision else 'ç¦ç”¨'}")

        # ä¼°ç®—æ¨¡å‹å†…å­˜ä½¿ç”¨
        model_size_mb = total_params * 4 / (1024 * 1024)  # å‡è®¾float32
        logger.info(f"ğŸ’¾ ä¼°ç®—æ¨¡å‹å¤§å°: {model_size_mb:.1f}MB")

        return model, device

    except ImportError:
        logger.warning("CUDAå·¥å…·ä¸å¯ç”¨ï¼Œå›é€€åˆ°CPUæ¨¡å¼")
        device = torch.device('cpu')
        model = LogicTransformer(vocab_size=vocab_size, **kwargs)
        return model, device


def optimize_model_for_inference(model: LogicTransformer, device: torch.device) -> LogicTransformer:
    """
    ä¸ºæ¨ç†ä¼˜åŒ–æ¨¡å‹

    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        device: ç›®æ ‡è®¾å¤‡

    Returns:
        ä¼˜åŒ–åçš„æ¨¡å‹
    """
    model.eval()

    # å¦‚æœæ˜¯CUDAè®¾å¤‡ï¼Œè¿›è¡Œé¢å¤–ä¼˜åŒ–
    if device.type == 'cuda':
        # å¯ç”¨cudnnåŸºå‡†æ¨¡å¼
        torch.backends.cudnn.benchmark = True

        # å°è¯•ä½¿ç”¨TorchScriptä¼˜åŒ–
        try:
            # åˆ›å»ºç¤ºä¾‹è¾“å…¥
            vocab_size = model.vocab_size
            sample_src = torch.randint(0, vocab_size, (10, 1), device=device)
            sample_tgt = torch.randint(0, vocab_size, (10, 1), device=device)

            # è½¬æ¢ä¸ºTorchScript
            traced_model = torch.jit.trace(model, (sample_src, sample_tgt))
            traced_model = torch.jit.optimize_for_inference(traced_model)

            logger.info("âœ… TorchScriptä¼˜åŒ–å®Œæˆ")
            return traced_model

        except Exception as e:
            logger.warning(f"TorchScriptä¼˜åŒ–å¤±è´¥: {e}")

    return model


def get_model_memory_usage(model: LogicTransformer, device: torch.device) -> Dict[str, float]:
    """
    è·å–æ¨¡å‹å†…å­˜ä½¿ç”¨æƒ…å†µ

    Args:
        model: æ¨¡å‹
        device: è®¾å¤‡

    Returns:
        å†…å­˜ä½¿ç”¨ä¿¡æ¯å­—å…¸
    """
    if device.type != 'cuda':
        return {'error': 'Only available for CUDA devices'}

    # è®¡ç®—æ¨¡å‹å‚æ•°å†…å­˜
    param_memory = sum(p.numel() * p.element_size() for p in model.parameters())

    # è®¡ç®—ç¼“å†²åŒºå†…å­˜
    buffer_memory = sum(b.numel() * b.element_size() for b in model.buffers())

    # è·å–GPUå†…å­˜ä¿¡æ¯
    allocated = torch.cuda.memory_allocated(device)
    reserved = torch.cuda.memory_reserved(device)

    return {
        'param_memory_mb': param_memory / (1024 * 1024),
        'buffer_memory_mb': buffer_memory / (1024 * 1024),
        'total_model_memory_mb': (param_memory + buffer_memory) / (1024 * 1024),
        'gpu_allocated_mb': allocated / (1024 * 1024),
        'gpu_reserved_mb': reserved / (1024 * 1024)
    }


# ä¿æŒå‘åå…¼å®¹
def create_model(vocab_size: int, **kwargs) -> LogicTransformer:
    """åˆ›å»ºæ ‡å‡†æ¨¡å‹ï¼ˆå‘åå…¼å®¹ï¼‰"""
    return LogicTransformer(vocab_size=vocab_size, **kwargs)


if __name__ == "__main__":
    # æµ‹è¯•CUDAæ¨¡å‹åˆ›å»º
    from logic_utils import Tokenizer

    print("ğŸ§ª æµ‹è¯•CUDAæ¨¡å‹åˆ›å»º")
    print("=" * 50)

    tokenizer = Tokenizer()

    # æµ‹è¯•CUDAæ¨¡å‹
    try:
        model, device = create_cuda_model(
            vocab_size=tokenizer.vocab_size,
            device='auto',
            d_model=128,
            nhead=8
        )

        print(f"\nğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        print(f"è®¾å¤‡: {device}")
        print(f"è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")

        # è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ
        if device.type == 'cuda':
            memory_info = get_model_memory_usage(model, device)
            print(f"æ¨¡å‹å†…å­˜: {memory_info['total_model_memory_mb']:.1f}MB")
            print(f"GPUå·²åˆ†é…: {memory_info['gpu_allocated_mb']:.1f}MB")

        # æµ‹è¯•å‰å‘ä¼ æ’­
        print("\nğŸ” æµ‹è¯•å‰å‘ä¼ æ’­...")
        batch_size = 2
        seq_len = 10

        src = torch.randint(0, tokenizer.vocab_size, (seq_len, batch_size), device=device)
        tgt = torch.randint(0, tokenizer.vocab_size, (seq_len, batch_size), device=device)

        with torch.no_grad():
            output = model(src, tgt[:-1])
            print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
            print("âœ… å‰å‘ä¼ æ’­æµ‹è¯•æˆåŠŸ")

    except Exception as e:
        print(f"âŒ CUDAæ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")

        # å›é€€åˆ°CPUæ¨¡å‹
        print("ğŸ”„ å›é€€åˆ°CPUæ¨¡å‹...")
        model = create_model(tokenizer.vocab_size)
        print("âœ… CPUæ¨¡å‹åˆ›å»ºæˆåŠŸ")
