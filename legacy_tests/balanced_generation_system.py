"""
å¹³è¡¡çš„ç”Ÿæˆç³»ç»Ÿ
ç»“åˆè½¯çº¦æŸå’Œæ··åˆæ¨¡å‹æ€è·¯ï¼Œå¹³è¡¡é€»è¾‘å­¦ä¹ å’Œè¯­æ³•è§„èŒƒ
"""

import numpy as np
from typing import List, Tuple, Dict
import sys
import os
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / 'src'))

from logic_transformer.data_utils import Tokenizer


class BalancedSequenceGenerator:
    """å¹³è¡¡çš„åºåˆ—ç”Ÿæˆå™¨ï¼Œå®ç°è½¯çº¦æŸå’Œé€»è¾‘ä¼˜å…ˆç­–ç•¥"""
    
    def __init__(self, model, tokenizer: Tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        
        # è½¯çº¦æŸå‚æ•°
        self.logic_priority_weight = 0.7    # é€»è¾‘ä¼˜å…ˆæƒé‡
        self.grammar_guidance_weight = 0.3  # è¯­æ³•æŒ‡å¯¼æƒé‡
        
        # æ¸©å’Œçš„æƒ©ç½šå‚æ•°
        self.soft_repetition_penalty = 1.1  # é™ä½é‡å¤æƒ©ç½š
        self.soft_cycle_penalty = 0.5       # æ¸©å’Œçš„å¾ªç¯æƒ©ç½š
        self.completion_encouragement = 1.5  # é¼“åŠ±å®Œæˆ
        
        # é€»è¾‘å®Œæ•´æ€§æ£€æŸ¥
        self.min_logical_length = 5        # æœ€å°é€»è¾‘é•¿åº¦
        self.max_logical_length = 15       # æœ€å¤§é€»è¾‘é•¿åº¦
        
    def calculate_logic_reward(self, generated_tokens: List[int], input_sequence: List[int]) -> float:
        """è®¡ç®—é€»è¾‘å¥–åŠ±åˆ†æ•°"""
        if not generated_tokens:
            return 0.0
        
        current_text = self.tokenizer.decode(generated_tokens)
        input_text = self.tokenizer.decode(input_sequence)
        
        reward = 0.0
        
        # åŸºç¡€ç»“æ„å¥–åŠ±
        if '~' in current_text:
            reward += 0.2  # æœ‰å¦å®šç¬¦
        if '->' in current_text:
            reward += 0.3  # æœ‰è•´å«ç¬¦
        
        # å®Œæ•´æ€§å¥–åŠ±
        if '->' in current_text:
            arrow_pos = current_text.find('->')
            after_arrow = current_text[arrow_pos + 2:].strip()
            if after_arrow:  # ç®­å¤´åæœ‰å†…å®¹
                reward += 0.3
                if len(after_arrow) > 1:  # ç®­å¤´åæœ‰å®è´¨å†…å®¹
                    reward += 0.2
        
        # å˜é‡ä¸€è‡´æ€§å¥–åŠ±
        input_vars = set(c for c in input_text if c in 'pqrst')
        output_vars = set(c for c in current_text if c in 'pqrst')
        if input_vars and output_vars:
            overlap = len(input_vars & output_vars) / len(input_vars)
            reward += overlap * 0.3
        
        return min(reward, 1.0)  # é™åˆ¶åœ¨[0,1]èŒƒå›´
    
    def apply_soft_constraints(self, logits: np.ndarray, generated_tokens: List[int]) -> np.ndarray:
        """åº”ç”¨è½¯çº¦æŸï¼ˆæ¸©å’Œçš„è¯­æ³•æŒ‡å¯¼ï¼‰"""
        
        # 1. æ¸©å’Œçš„é‡å¤æƒ©ç½š
        if generated_tokens:
            token_counts = {}
            for token in generated_tokens:
                token_counts[token] = token_counts.get(token, 0) + 1
            
            for token, count in token_counts.items():
                if token < len(logits) and count > 1:
                    penalty = self.soft_repetition_penalty ** (count - 1)
                    logits[token] /= penalty
        
        # 2. æ¸©å’Œçš„å¾ªç¯æ£€æµ‹
        if len(generated_tokens) >= 6:
            last_3 = generated_tokens[-3:]
            prev_3 = generated_tokens[-6:-3]
            if last_3 == prev_3:
                for token in set(last_3):
                    if token < len(logits):
                        logits[token] *= self.soft_cycle_penalty
        
        # 3. åŸºæœ¬çš„ç»“æ„æŒ‡å¯¼ï¼ˆä½†ä¸å¼ºåˆ¶ï¼‰
        if generated_tokens:
            last_token = generated_tokens[-1]
            
            if last_token == self.tokenizer.char_to_int.get('-', -1):
                # ç ´æŠ˜å·åé¼“åŠ±å¤§äºå·
                gt_token = self.tokenizer.char_to_int.get('>', -1)
                if gt_token >= 0 and gt_token < len(logits):
                    logits[gt_token] *= 1.5  # æ¸©å’Œçš„é¼“åŠ±
            
            elif last_token == self.tokenizer.char_to_int.get('>', -1):
                # å¤§äºå·åé¼“åŠ±ç©ºæ ¼æˆ–å˜é‡
                space_token = self.tokenizer.char_to_int.get(' ', -1)
                if space_token >= 0 and space_token < len(logits):
                    logits[space_token] *= 1.3
                
                # é¼“åŠ±å˜é‡
                for var_token in [0, 1, 2, 3, 4]:  # p,q,r,s,t
                    if var_token < len(logits):
                        logits[var_token] *= 1.2
        
        return logits
    
    def apply_completion_encouragement(self, logits: np.ndarray, generated_tokens: List[int]) -> np.ndarray:
        """é¼“åŠ±å®Œæˆå®Œæ•´çš„é€»è¾‘è¡¨è¾¾å¼"""
        
        current_text = self.tokenizer.decode(generated_tokens)
        
        # å¦‚æœå·²ç»æœ‰ç®­å¤´ä½†åé¢å†…å®¹ä¸å®Œæ•´ï¼Œé¼“åŠ±ç»§ç»­ç”Ÿæˆ
        if '->' in current_text:
            arrow_pos = current_text.find('->')
            after_arrow = current_text[arrow_pos + 2:].strip()
            
            if not after_arrow:
                # ç®­å¤´åæ²¡æœ‰å†…å®¹ï¼Œé¼“åŠ±ç”Ÿæˆç©ºæ ¼
                space_token = self.tokenizer.char_to_int.get(' ', -1)
                if space_token >= 0 and space_token < len(logits):
                    logits[space_token] *= 2.0
            
            elif after_arrow == ' ':
                # åªæœ‰ç©ºæ ¼ï¼Œé¼“åŠ±ç”Ÿæˆå¦å®šç¬¦æˆ–å˜é‡
                neg_token = self.tokenizer.char_to_int.get('~', -1)
                if neg_token >= 0 and neg_token < len(logits):
                    logits[neg_token] *= 1.8
                
                for var_token in [0, 1, 2, 3, 4]:  # p,q,r,s,t
                    if var_token < len(logits):
                        logits[var_token] *= 1.5
            
            elif len(after_arrow.strip()) == 1:
                # åªæœ‰ä¸€ä¸ªå­—ç¬¦ï¼Œå¯èƒ½éœ€è¦æ›´å¤šå†…å®¹
                # é€‚åº¦æŠ‘åˆ¶END_TOKEN
                logits[self.tokenizer.END_TOKEN] *= 0.7
        
        # å¦‚æœé•¿åº¦å¤ªçŸ­ï¼ŒæŠ‘åˆ¶END_TOKEN
        if len(generated_tokens) < self.min_logical_length:
            logits[self.tokenizer.END_TOKEN] *= 0.3
        
        # å¦‚æœé•¿åº¦åˆé€‚ä¸”æœ‰å®Œæ•´ç»“æ„ï¼Œé¼“åŠ±END_TOKEN
        elif (len(generated_tokens) >= self.min_logical_length and 
              '~' in current_text and '->' in current_text):
            arrow_pos = current_text.find('->')
            if arrow_pos >= 0 and len(current_text) > arrow_pos + 3:
                logits[self.tokenizer.END_TOKEN] *= self.completion_encouragement
        
        return logits
    
    def generate_balanced_sequence(self, input_sequence: List[int], max_steps: int = 20) -> Tuple[List[int], str, Dict]:
        """ç”Ÿæˆå¹³è¡¡çš„åºåˆ—ï¼Œè¿”å›tokensã€æ–‡æœ¬å’Œè°ƒè¯•ä¿¡æ¯"""
        
        # ç¼–ç è¾“å…¥
        encoded = self.model.encode(input_sequence)
        
        # åˆå§‹åŒ–
        generated_tokens = []
        current_token = self.tokenizer.START_TOKEN
        debug_info = {
            'logic_rewards': [],
            'step_decisions': [],
            'final_logic_score': 0.0
        }
        
        for step in range(max_steps):
            # è§£ç æ­¥éª¤
            hidden_state, raw_logits = self.model.decode_step(encoded, current_token)
            
            # è®¡ç®—å½“å‰çš„é€»è¾‘å¥–åŠ±
            logic_reward = self.calculate_logic_reward(generated_tokens, input_sequence)
            debug_info['logic_rewards'].append(logic_reward)
            
            # åº”ç”¨å¹³è¡¡ç­–ç•¥
            logits = raw_logits.copy()
            
            # è½¯çº¦æŸï¼ˆè¯­æ³•æŒ‡å¯¼ï¼‰
            logits = self.apply_soft_constraints(logits, generated_tokens)
            
            # å®Œæˆé¼“åŠ±ï¼ˆé€»è¾‘å®Œæ•´æ€§ï¼‰
            logits = self.apply_completion_encouragement(logits, generated_tokens)
            
            # é‡æ–°è®¡ç®—æ¦‚ç‡
            exp_logits = np.exp(logits - np.max(logits))
            probabilities = exp_logits / np.sum(exp_logits)
            
            # é€‰æ‹©ä¸‹ä¸€ä¸ªtoken
            next_token = int(np.argmax(probabilities))
            next_char = self.tokenizer.int_to_char.get(next_token, 'UNK')
            
            debug_info['step_decisions'].append({
                'step': step,
                'token': next_token,
                'char': next_char,
                'logic_reward': logic_reward,
                'top_3_probs': [(i, probabilities[i]) for i in np.argsort(probabilities)[-3:][::-1]]
            })
            
            # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
            if next_token == self.tokenizer.END_TOKEN:
                break
            
            # æ£€æŸ¥tokenæœ‰æ•ˆæ€§
            if next_token >= self.tokenizer.vocab_size or next_token < 0:
                break
            
            # æ·»åŠ åˆ°åºåˆ—
            generated_tokens.append(next_token)
            current_token = next_token
            
            # é•¿åº¦é™åˆ¶
            if len(generated_tokens) >= self.max_logical_length:
                break
        
        # è®¡ç®—æœ€ç»ˆé€»è¾‘åˆ†æ•°
        debug_info['final_logic_score'] = self.calculate_logic_reward(generated_tokens, input_sequence)
        
        # è§£ç ç»“æœ
        decoded_text = self.tokenizer.decode(generated_tokens)
        
        return generated_tokens, decoded_text, debug_info


def test_balanced_generation():
    """æµ‹è¯•å¹³è¡¡ç”Ÿæˆç³»ç»Ÿ"""
    print("âš–ï¸ æµ‹è¯•å¹³è¡¡ç”Ÿæˆç³»ç»Ÿ")
    print("=" * 60)
    
    # åŠ è½½æ¨¡å‹å’Œtokenizer
    tokenizer = Tokenizer()
    
    from logic_transformer.models.base_model import ImprovedSimpleModel
    
    model = ImprovedSimpleModel(
        vocab_size=tokenizer.vocab_size,
        hidden_size=128,
        max_length=50,
        learning_rate=0.003
    )
    
    model_path = 'outputs/trained_models/robust_model_Level_1_é²æ£’ç‰ˆ.npz'
    if not model.load_model(model_path):
        print(f"âŒ æ— æ³•åŠ è½½æ¨¡å‹: {model_path}")
        return
    
    # åˆ›å»ºå¹³è¡¡ç”Ÿæˆå™¨
    balanced_generator = BalancedSequenceGenerator(model, tokenizer)
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        ("p -> q", "~q -> ~p"),
        ("~p -> r", "~r -> p"),
        ("q -> s", "~s -> ~q")
    ]
    
    for test_input, expected in test_cases:
        print(f"\nğŸ§ª æµ‹è¯•è¾“å…¥: '{test_input}'")
        print(f"æœŸæœ›è¾“å‡º: '{expected}'")
        print("-" * 50)
        
        input_sequence = tokenizer.encode(test_input)
        
        # ç”Ÿæˆå¤šä¸ªæ ·æœ¬
        for i in range(3):
            tokens, text, debug = balanced_generator.generate_balanced_sequence(input_sequence)
            
            print(f"  æ ·æœ¬ {i+1}: '{text}' (é€»è¾‘åˆ†æ•°: {debug['final_logic_score']:.2f})")
            
            # åˆ†æè´¨é‡
            if text.count('->') == 1 and '~' in text:
                if len(text.split('->')[1].strip()) > 0:
                    print(f"    âœ… ç»“æ„å®Œæ•´")
                else:
                    print(f"    ğŸ”„ ç»“æ„ä¸å®Œæ•´")
            else:
                print(f"    âŒ ç»“æ„æœ‰é—®é¢˜")
    
    print(f"\nğŸ“Š å¹³è¡¡ç”Ÿæˆç³»ç»Ÿåˆ†æ:")
    print(f"  ä¼˜åŠ¿: è½¯çº¦æŸä¿æŒäº†é€»è¾‘æ¢ç´¢ç©ºé—´")
    print(f"  æ”¹è¿›: é¼“åŠ±å®Œæˆæœºåˆ¶æé«˜äº†è¾“å‡ºå®Œæ•´æ€§")
    print(f"  å¹³è¡¡: åœ¨è¯­æ³•è§„èŒƒå’Œé€»è¾‘è‡ªç”±ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ç‚¹")


if __name__ == "__main__":
    test_balanced_generation()
