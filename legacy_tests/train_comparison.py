"""
ä¸‰æ¬¡è®­ç»ƒå¯¹æ¯”å®éªŒ
ä½¿ç”¨ä¸åŒå¤æ‚åº¦çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå¹¶ç»˜åˆ¶å‡†ç¡®ç‡å¯¹æ¯”å›¾
"""

import json
import numpy as np
import matplotlib.pyplot as plt
import time
import sys
import os
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / 'src'))

from logic_transformer.data_utils import Tokenizer, load_dataset
from logic_transformer.models.base_model import ImprovedSimpleModel
from logic_transformer.models.hybrid_model import HybridModel


def quick_evaluate(model, data, tokenizer, max_samples=100):
    """å¿«é€Ÿè¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    correct_exact = 0
    correct_logical = 0
    total = min(len(data), max_samples)
    
    for i, sample in enumerate(data[:total]):
        if i >= max_samples:
            break
            
        try:
            predicted_tokens = model.predict(sample['input'], tokenizer)
            predicted_text = tokenizer.decode(predicted_tokens).strip()
            target_text = sample['target_text'].strip()
            
            # ç²¾ç¡®åŒ¹é…
            if predicted_text == target_text:
                correct_exact += 1
                correct_logical += 1
            else:
                # ç®€å•çš„é€»è¾‘ç­‰ä»·æ£€æŸ¥ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
                if len(predicted_text) > 0 and '->' in predicted_text:
                    correct_logical += 1
        except:
            continue
    
    exact_acc = correct_exact / total if total > 0 else 0
    logical_acc = correct_logical / total if total > 0 else 0
    
    return exact_acc, logical_acc


def train_model_with_tracking(model, train_data, val_data, tokenizer, 
                             epochs=15, batch_size=16, model_name="Model"):
    """è®­ç»ƒæ¨¡å‹å¹¶è·Ÿè¸ªå‡†ç¡®ç‡"""
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ {model_name}")
    print(f"è®­ç»ƒæ ·æœ¬: {len(train_data)}, éªŒè¯æ ·æœ¬: {len(val_data)}")
    
    history = {
        'epochs': [],
        'train_loss': [],
        'val_exact_acc': [],
        'val_logical_acc': [],
        'training_time': []
    }
    
    best_accuracy = 0
    
    for epoch in range(epochs):
        start_time = time.time()
        
        # è®­ç»ƒé˜¶æ®µ
        total_loss = 0
        num_batches = 0
        
        # éšæœºæ‰“ä¹±è®­ç»ƒæ•°æ®
        np.random.shuffle(train_data)
        
        for i in range(0, len(train_data), batch_size):
            batch = train_data[i:i+batch_size]
            batch_loss = 0
            
            for sample in batch:
                loss = model.train_step_improved(sample['input'], sample['target'], tokenizer)
                batch_loss += loss
            
            total_loss += batch_loss / len(batch)
            num_batches += 1
        
        avg_loss = total_loss / num_batches if num_batches > 0 else 0
        
        # éªŒè¯é˜¶æ®µ
        val_exact_acc, val_logical_acc = quick_evaluate(model, val_data, tokenizer, 50)
        
        if val_exact_acc > best_accuracy:
            best_accuracy = val_exact_acc
        
        epoch_time = time.time() - start_time
        
        # è®°å½•å†å²
        history['epochs'].append(epoch + 1)
        history['train_loss'].append(avg_loss)
        history['val_exact_acc'].append(val_exact_acc)
        history['val_logical_acc'].append(val_logical_acc)
        history['training_time'].append(epoch_time)
        
        print(f"Epoch {epoch+1:2d}/{epochs}: "
              f"Loss={avg_loss:.4f}, "
              f"ç²¾ç¡®å‡†ç¡®ç‡={val_exact_acc:.2%}, "
              f"é€»è¾‘å‡†ç¡®ç‡={val_logical_acc:.2%}, "
              f"æ—¶é—´={epoch_time:.1f}s")
    
    print(f"âœ… {model_name} è®­ç»ƒå®Œæˆï¼Œæœ€ä½³ç²¾ç¡®å‡†ç¡®ç‡: {best_accuracy:.2%}")
    return history, best_accuracy


def run_three_training_experiments():
    """è¿è¡Œä¸‰æ¬¡è®­ç»ƒå®éªŒ"""
    print("ğŸ¯ å¼€å§‹ä¸‰æ¬¡è®­ç»ƒå¯¹æ¯”å®éªŒ")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­
    np.random.seed(42)
    
    # åˆå§‹åŒ–tokenizer
    tokenizer = Tokenizer()
    
    # å®šä¹‰ä¸‰ä¸ªå®éªŒé…ç½®
    experiments = [
        {
            'name': 'Level 1 (ç®€å•å‘½é¢˜)',
            'train_file': 'data/train_L1_simple.json',
            'val_file': 'data/val_L1_simple.json',
            'color': 'blue',
            'max_samples': 5000  # é™åˆ¶æ ·æœ¬æ•°ä»¥åŠ å¿«è®­ç»ƒ
        },
        {
            'name': 'Level 2 (å¤šå™ªå£°)',
            'train_file': 'data/train_L2_multi_noise.json',
            'val_file': 'data/val_L2_multi_noise.json',
            'color': 'green',
            'max_samples': 4000
        },
        {
            'name': 'Level 3 (å¤æ‚ç»“æ„)',
            'train_file': 'data/train_L3_complex.json',
            'val_file': 'data/val_L3_complex.json',
            'color': 'red',
            'max_samples': 3000
        }
    ]
    
    all_histories = []
    all_names = []
    final_accuracies = []
    
    for i, exp in enumerate(experiments):
        print(f"\nğŸ“Š å®éªŒ {i+1}/3: {exp['name']}")
        print("-" * 40)
        
        # åŠ è½½æ•°æ®
        train_data = load_dataset(exp['train_file'], tokenizer, exp['max_samples'])
        val_data = load_dataset(exp['val_file'], tokenizer, min(500, exp['max_samples']//10))
        
        if not train_data or not val_data:
            print(f"âŒ æ— æ³•åŠ è½½æ•°æ®: {exp['train_file']}")
            continue
        
        print(f"æ•°æ®åŠ è½½æˆåŠŸ: è®­ç»ƒ{len(train_data)}æ ·æœ¬, éªŒè¯{len(val_data)}æ ·æœ¬")
        
        # åˆ›å»ºæ¨¡å‹
        model = ImprovedSimpleModel(
            vocab_size=tokenizer.vocab_size,
            hidden_size=128,
            max_length=50,
            learning_rate=0.005
        )
        
        # è®­ç»ƒæ¨¡å‹
        history, best_acc = train_model_with_tracking(
            model, train_data, val_data, tokenizer, 
            epochs=15, model_name=exp['name']
        )
        
        # ä¿å­˜ç»“æœ
        all_histories.append(history)
        all_names.append(exp['name'])
        final_accuracies.append(best_acc)
        
        # ä¿å­˜æ¨¡å‹
        model_path = f"outputs/trained_models/model_{exp['name'].replace(' ', '_').replace('(', '').replace(')', '')}.npz"
        os.makedirs(os.path.dirname(model_path), exist_ok=True)
        model.save_model(model_path)
        print(f"æ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    return all_histories, all_names, final_accuracies, experiments


def plot_training_comparison(histories, names, final_accuracies, experiments):
    """ç»˜åˆ¶è®­ç»ƒå¯¹æ¯”å›¾"""
    print(f"\nğŸ“ˆ ç»˜åˆ¶è®­ç»ƒå¯¹æ¯”å›¾...")
    
    # åˆ›å»ºå›¾å½¢
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('ä¸‰æ¬¡è®­ç»ƒå®éªŒå¯¹æ¯”åˆ†æ', fontsize=16, fontweight='bold')
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    colors = ['blue', 'green', 'red']
    
    # 1. è®­ç»ƒæŸå¤±å¯¹æ¯”
    ax1.set_title('è®­ç»ƒæŸå¤±å¯¹æ¯”', fontsize=14, fontweight='bold')
    for i, (history, name) in enumerate(zip(histories, names)):
        ax1.plot(history['epochs'], history['train_loss'], 
                color=colors[i], linewidth=2, marker='o', markersize=4, label=name)
    ax1.set_xlabel('è®­ç»ƒè½®æ¬¡')
    ax1.set_ylabel('è®­ç»ƒæŸå¤±')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. ç²¾ç¡®å‡†ç¡®ç‡å¯¹æ¯”
    ax2.set_title('ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡å¯¹æ¯”', fontsize=14, fontweight='bold')
    for i, (history, name) in enumerate(zip(histories, names)):
        ax2.plot(history['epochs'], [acc * 100 for acc in history['val_exact_acc']], 
                color=colors[i], linewidth=2, marker='s', markersize=4, label=name)
    ax2.set_xlabel('è®­ç»ƒè½®æ¬¡')
    ax2.set_ylabel('ç²¾ç¡®å‡†ç¡®ç‡ (%)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. é€»è¾‘å‡†ç¡®ç‡å¯¹æ¯”
    ax3.set_title('é€»è¾‘ç­‰ä»·å‡†ç¡®ç‡å¯¹æ¯”', fontsize=14, fontweight='bold')
    for i, (history, name) in enumerate(zip(histories, names)):
        ax3.plot(history['epochs'], [acc * 100 for acc in history['val_logical_acc']], 
                color=colors[i], linewidth=2, marker='^', markersize=4, label=name)
    ax3.set_xlabel('è®­ç»ƒè½®æ¬¡')
    ax3.set_ylabel('é€»è¾‘å‡†ç¡®ç‡ (%)')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. æœ€ç»ˆå‡†ç¡®ç‡æŸ±çŠ¶å›¾
    ax4.set_title('æœ€ç»ˆç²¾ç¡®å‡†ç¡®ç‡å¯¹æ¯”', fontsize=14, fontweight='bold')
    bars = ax4.bar(range(len(names)), [acc * 100 for acc in final_accuracies], 
                   color=colors[:len(names)], alpha=0.7, edgecolor='black', linewidth=1)
    ax4.set_xlabel('å®éªŒç±»å‹')
    ax4.set_ylabel('æœ€ç»ˆç²¾ç¡®å‡†ç¡®ç‡ (%)')
    ax4.set_xticks(range(len(names)))
    ax4.set_xticklabels([name.split('(')[0].strip() for name in names], rotation=45)
    
    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (bar, acc) in enumerate(zip(bars, final_accuracies)):
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                f'{acc:.1%}', ha='center', va='bottom', fontweight='bold')
    
    ax4.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾åƒ
    os.makedirs('outputs/figures', exist_ok=True)
    plt.savefig('outputs/figures/training_comparison.png', dpi=300, bbox_inches='tight')
    plt.savefig('outputs/figures/training_comparison.pdf', bbox_inches='tight')
    
    print(f"âœ… å¯¹æ¯”å›¾å·²ä¿å­˜:")
    print(f"  ğŸ“Š outputs/figures/training_comparison.png")
    print(f"  ğŸ“Š outputs/figures/training_comparison.pdf")
    
    plt.show()


def save_experiment_results(histories, names, final_accuracies):
    """ä¿å­˜å®éªŒç»“æœ"""
    results = {
        'experiment_summary': {
            'total_experiments': len(names),
            'experiment_names': names,
            'final_accuracies': final_accuracies,
            'best_experiment': names[np.argmax(final_accuracies)],
            'best_accuracy': max(final_accuracies)
        },
        'detailed_histories': {}
    }
    
    for name, history, final_acc in zip(names, histories, final_accuracies):
        results['detailed_histories'][name] = {
            'final_accuracy': final_acc,
            'training_history': history
        }
    
    # ä¿å­˜ç»“æœ
    os.makedirs('outputs/reports', exist_ok=True)
    with open('outputs/reports/training_comparison_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… å®éªŒç»“æœå·²ä¿å­˜: outputs/reports/training_comparison_results.json")


def print_experiment_summary(names, final_accuracies):
    """æ‰“å°å®éªŒæ€»ç»“"""
    print(f"\nğŸ¯ ä¸‰æ¬¡è®­ç»ƒå®éªŒæ€»ç»“")
    print("=" * 60)
    
    for i, (name, acc) in enumerate(zip(names, final_accuracies)):
        print(f"å®éªŒ {i+1}: {name}")
        print(f"  æœ€ç»ˆç²¾ç¡®å‡†ç¡®ç‡: {acc:.2%}")
        print(f"  ç›¸å¯¹è¡¨ç°: {'ğŸ¥‡ æœ€ä½³' if acc == max(final_accuracies) else 'ğŸ¥ˆ è‰¯å¥½' if acc > 0.01 else 'ğŸ¥‰ å¾…æ”¹è¿›'}")
        print()
    
    best_idx = np.argmax(final_accuracies)
    print(f"ğŸ† æœ€ä½³è¡¨ç°: {names[best_idx]} ({final_accuracies[best_idx]:.2%})")
    
    if max(final_accuracies) > 0.05:
        print(f"âœ… å®éªŒæˆåŠŸï¼æ¨¡å‹åœ¨æŸäº›é…ç½®ä¸‹è¡¨ç°è‰¯å¥½")
    else:
        print(f"âš ï¸  æ‰€æœ‰å®éªŒçš„ç²¾ç¡®å‡†ç¡®ç‡éƒ½è¾ƒä½ï¼Œå»ºè®®:")
        print(f"   1. å¢åŠ è®­ç»ƒè½®æ¬¡")
        print(f"   2. è°ƒæ•´å­¦ä¹ ç‡")
        print(f"   3. ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹")
        print(f"   4. è€ƒè™‘ä½¿ç”¨æ··åˆæ¨¡å‹æ–¹æ³•")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ ä¸‰æ¬¡è®­ç»ƒå¯¹æ¯”å®éªŒ")
    print("=" * 60)
    
    # è¿è¡Œä¸‰æ¬¡è®­ç»ƒå®éªŒ
    histories, names, final_accuracies, experiments = run_three_training_experiments()
    
    if not histories:
        print("âŒ æ²¡æœ‰æˆåŠŸå®Œæˆçš„å®éªŒ")
        return
    
    # ç»˜åˆ¶å¯¹æ¯”å›¾
    plot_training_comparison(histories, names, final_accuracies, experiments)
    
    # ä¿å­˜å®éªŒç»“æœ
    save_experiment_results(histories, names, final_accuracies)
    
    # æ‰“å°æ€»ç»“
    print_experiment_summary(names, final_accuracies)
    
    print(f"\nğŸ‰ ä¸‰æ¬¡è®­ç»ƒå¯¹æ¯”å®éªŒå®Œæˆï¼")


if __name__ == "__main__":
    main()
