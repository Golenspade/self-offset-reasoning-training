"""
è¯¾ç¨‹å­¦ä¹ çš„æƒ©ç½šæœºåˆ¶
å®ç°æ¸è¿›å¼çº¦æŸå¼ºåº¦ï¼Œå¹³è¡¡é€»è¾‘å­¦ä¹ å’Œè¯­æ³•è§„èŒƒ
"""

import numpy as np
from typing import List, Tuple, Dict
import sys
import os
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / 'src'))

from logic_transformer.data_utils import Tokenizer


class CurriculumPenaltyGenerator:
    """è¯¾ç¨‹å­¦ä¹ çš„åºåˆ—ç”Ÿæˆå™¨ï¼Œå®ç°æ¸è¿›å¼æƒ©ç½šå¼ºåº¦"""
    
    def __init__(self, model, tokenizer: Tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        
        # è¯¾ç¨‹å­¦ä¹ å‚æ•°
        self.current_epoch = 0
        self.total_epochs = 60  # æ€»è®­ç»ƒè½®æ¬¡
        
        # ä¸‰é˜¶æ®µè¯¾ç¨‹è®¾è®¡
        self.stage1_epochs = 20  # è‡ªç”±æ¢ç´¢é˜¶æ®µ
        self.stage2_epochs = 25  # æ¸è¿›çº¦æŸé˜¶æ®µ  
        self.stage3_epochs = 15  # ç²¾ç»†è°ƒä¼˜é˜¶æ®µ
        
        # åŸºç¡€æƒ©ç½šå‚æ•°ï¼ˆä¼šæ ¹æ®é˜¶æ®µè°ƒæ•´ï¼‰
        self.base_repetition_penalty = 1.2
        self.base_cycle_penalty = 0.1
        self.base_end_token_boost = 2.0
        self.base_structural_guidance = 2.0
        
        # å½“å‰é˜¶æ®µçš„å®é™…æƒ©ç½šå¼ºåº¦
        self.current_penalties = self.calculate_current_penalties()
        
    def calculate_current_penalties(self) -> Dict[str, float]:
        """æ ¹æ®å½“å‰è®­ç»ƒé˜¶æ®µè®¡ç®—æƒ©ç½šå¼ºåº¦"""
        
        if self.current_epoch <= self.stage1_epochs:
            # é˜¶æ®µ1ï¼šè‡ªç”±æ¢ç´¢ - æœ€å°çº¦æŸ
            stage = "free_exploration"
            progress = self.current_epoch / self.stage1_epochs
            
            penalties = {
                'repetition_penalty': 1.05,  # æå¼±çš„é‡å¤æƒ©ç½š
                'cycle_penalty': 0.8,        # å…è®¸ä¸€äº›å¾ªç¯
                'end_token_boost': 1.2,      # è½»å¾®çš„ç»“æŸæå‡
                'structural_guidance': 1.1,  # æœ€å°çš„ç»“æ„æŒ‡å¯¼
                'stage': stage,
                'progress': progress
            }
            
        elif self.current_epoch <= self.stage1_epochs + self.stage2_epochs:
            # é˜¶æ®µ2ï¼šæ¸è¿›çº¦æŸ - é€æ­¥å¢å¼º
            stage = "progressive_constraint"
            stage_epoch = self.current_epoch - self.stage1_epochs
            progress = stage_epoch / self.stage2_epochs
            
            # çº¿æ€§æ’å€¼å¢å¼ºæƒ©ç½šå¼ºåº¦
            penalties = {
                'repetition_penalty': 1.05 + (self.base_repetition_penalty - 1.05) * progress,
                'cycle_penalty': 0.8 - (0.8 - self.base_cycle_penalty) * progress,
                'end_token_boost': 1.2 + (self.base_end_token_boost - 1.2) * progress,
                'structural_guidance': 1.1 + (self.base_structural_guidance - 1.1) * progress,
                'stage': stage,
                'progress': progress
            }
            
        else:
            # é˜¶æ®µ3ï¼šç²¾ç»†è°ƒä¼˜ - æœ€å¼ºçº¦æŸ
            stage = "fine_tuning"
            stage_epoch = self.current_epoch - self.stage1_epochs - self.stage2_epochs
            progress = stage_epoch / self.stage3_epochs
            
            penalties = {
                'repetition_penalty': self.base_repetition_penalty,
                'cycle_penalty': self.base_cycle_penalty,
                'end_token_boost': self.base_end_token_boost,
                'structural_guidance': self.base_structural_guidance,
                'stage': stage,
                'progress': progress
            }
        
        return penalties
    
    def update_epoch(self, epoch: int):
        """æ›´æ–°å½“å‰è®­ç»ƒè½®æ¬¡å¹¶é‡æ–°è®¡ç®—æƒ©ç½šå¼ºåº¦"""
        self.current_epoch = epoch
        self.current_penalties = self.calculate_current_penalties()
        
        print(f"\nğŸ“š è¯¾ç¨‹å­¦ä¹ çŠ¶æ€æ›´æ–°:")
        print(f"  å½“å‰è½®æ¬¡: {epoch}/{self.total_epochs}")
        print(f"  å½“å‰é˜¶æ®µ: {self.current_penalties['stage']}")
        print(f"  é˜¶æ®µè¿›åº¦: {self.current_penalties['progress']:.2%}")
        print(f"  æƒ©ç½šå¼ºåº¦: é‡å¤={self.current_penalties['repetition_penalty']:.2f}, "
              f"å¾ªç¯={self.current_penalties['cycle_penalty']:.2f}")
    
    def apply_adaptive_repetition_penalty(self, logits: np.ndarray, generated_tokens: List[int]) -> np.ndarray:
        """è‡ªé€‚åº”é‡å¤æƒ©ç½š"""
        if not generated_tokens:
            return logits
        
        penalty_strength = self.current_penalties['repetition_penalty']
        
        # ç»Ÿè®¡tokenå‡ºç°æ¬¡æ•°
        token_counts = {}
        for token in generated_tokens:
            token_counts[token] = token_counts.get(token, 0) + 1
        
        # åº”ç”¨æƒ©ç½š
        for token, count in token_counts.items():
            if token < len(logits):
                penalty = penalty_strength ** count
                logits[token] /= penalty
        
        return logits
    
    def apply_adaptive_cycle_penalty(self, logits: np.ndarray, generated_tokens: List[int]) -> np.ndarray:
        """è‡ªé€‚åº”å¾ªç¯æƒ©ç½š"""
        if len(generated_tokens) < 6:
            return logits
        
        penalty_strength = self.current_penalties['cycle_penalty']
        
        # æ£€æµ‹å¾ªç¯æ¨¡å¼
        window_size = 3
        recent_tokens = generated_tokens[-window_size:]
        
        for i in range(len(generated_tokens) - window_size * 2, -1, -1):
            if i < 0:
                break
            
            prev_window = generated_tokens[i:i + window_size]
            if prev_window == recent_tokens:
                # å‘ç°å¾ªç¯ï¼Œåº”ç”¨æƒ©ç½š
                for token in set(recent_tokens):
                    if token < len(logits):
                        logits[token] *= penalty_strength
                break
        
        return logits
    
    def apply_adaptive_structural_guidance(self, logits: np.ndarray, generated_tokens: List[int]) -> np.ndarray:
        """è‡ªé€‚åº”ç»“æ„æŒ‡å¯¼"""
        if not generated_tokens:
            return logits
        
        guidance_strength = self.current_penalties['structural_guidance']
        last_token = generated_tokens[-1]
        
        # åªåœ¨é˜¶æ®µ2å’Œ3åº”ç”¨å¼ºç»“æ„æŒ‡å¯¼
        if self.current_penalties['stage'] == 'free_exploration':
            return logits
        
        # ç»“æ„åŒ–è§„åˆ™ï¼ˆå¼ºåº¦å¯è°ƒï¼‰
        if last_token == self.tokenizer.char_to_int.get('-', -1):
            # ç ´æŠ˜å·ååº”è¯¥è·Ÿå¤§äºå·
            gt_token = self.tokenizer.char_to_int.get('>', -1)
            if gt_token >= 0 and gt_token < len(logits):
                logits[gt_token] *= guidance_strength
            
            # æŠ‘åˆ¶å†æ¬¡ç”Ÿæˆç ´æŠ˜å·ï¼ˆå¼ºåº¦å¯è°ƒï¼‰
            dash_token = self.tokenizer.char_to_int.get('-', -1)
            if dash_token >= 0 and dash_token < len(logits):
                logits[dash_token] *= (1.0 / guidance_strength)
        
        elif last_token == self.tokenizer.char_to_int.get('>', -1):
            # å¤§äºå·ååº”è¯¥è·Ÿç©ºæ ¼
            space_token = self.tokenizer.char_to_int.get(' ', -1)
            if space_token >= 0 and space_token < len(logits):
                logits[space_token] *= guidance_strength
        
        return logits
    
    def apply_adaptive_end_token_boost(self, logits: np.ndarray, generated_tokens: List[int]) -> np.ndarray:
        """è‡ªé€‚åº”END_TOKENæå‡"""
        if len(generated_tokens) < 5:
            return logits
        
        boost_strength = self.current_penalties['end_token_boost']
        
        # æ£€æŸ¥æ˜¯å¦æœ‰åŸºæœ¬ç»“æ„
        has_negation = self.tokenizer.char_to_int.get('~', -1) in generated_tokens
        has_arrow = (self.tokenizer.char_to_int.get('-', -1) in generated_tokens and 
                    self.tokenizer.char_to_int.get('>', -1) in generated_tokens)
        
        if has_negation and has_arrow:
            logits[self.tokenizer.END_TOKEN] *= boost_strength
        
        return logits
    
    def generate_with_curriculum(self, input_sequence: List[int], max_steps: int = 20) -> Tuple[List[int], str]:
        """ä½¿ç”¨è¯¾ç¨‹å­¦ä¹ çš„åºåˆ—ç”Ÿæˆ"""
        
        # ç¼–ç è¾“å…¥
        encoded = self.model.encode(input_sequence)
        
        # åˆå§‹åŒ–
        generated_tokens = []
        current_token = self.tokenizer.START_TOKEN
        
        for step in range(max_steps):
            # è§£ç æ­¥éª¤
            hidden_state, raw_logits = self.model.decode_step(encoded, current_token)
            
            # è½¬æ¢ä¸ºæ¦‚ç‡
            logits = raw_logits.copy()
            
            # åº”ç”¨è‡ªé€‚åº”æƒ©ç½šæœºåˆ¶
            logits = self.apply_adaptive_repetition_penalty(logits, generated_tokens)
            logits = self.apply_adaptive_cycle_penalty(logits, generated_tokens)
            logits = self.apply_adaptive_structural_guidance(logits, generated_tokens)
            logits = self.apply_adaptive_end_token_boost(logits, generated_tokens)
            
            # é‡æ–°è®¡ç®—æ¦‚ç‡
            exp_logits = np.exp(logits - np.max(logits))
            probabilities = exp_logits / np.sum(exp_logits)
            
            # é€‰æ‹©ä¸‹ä¸€ä¸ªtoken
            next_token = int(np.argmax(probabilities))
            
            # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
            if next_token == self.tokenizer.END_TOKEN:
                break
            
            # æ£€æŸ¥tokenæœ‰æ•ˆæ€§
            if next_token >= self.tokenizer.vocab_size or next_token < 0:
                break
            
            # æ·»åŠ åˆ°åºåˆ—
            generated_tokens.append(next_token)
            current_token = next_token
            
            # åŸºæœ¬å¾ªç¯æ£€æµ‹ï¼ˆå§‹ç»ˆä¿ç•™ï¼‰
            if len(generated_tokens) >= 6:
                last_3 = generated_tokens[-3:]
                prev_3 = generated_tokens[-6:-3]
                if last_3 == prev_3:
                    break
        
        # è§£ç ç»“æœ
        decoded_text = self.tokenizer.decode(generated_tokens)
        
        return generated_tokens, decoded_text


def test_curriculum_penalty_system():
    """æµ‹è¯•è¯¾ç¨‹å­¦ä¹ æƒ©ç½šç³»ç»Ÿ"""
    print("ğŸ“ æµ‹è¯•è¯¾ç¨‹å­¦ä¹ æƒ©ç½šç³»ç»Ÿ")
    print("=" * 60)
    
    # åŠ è½½æ¨¡å‹å’Œtokenizer
    tokenizer = Tokenizer()
    
    from logic_transformer.models.base_model import ImprovedSimpleModel
    
    model = ImprovedSimpleModel(
        vocab_size=tokenizer.vocab_size,
        hidden_size=128,
        max_length=50,
        learning_rate=0.003
    )
    
    model_path = 'outputs/trained_models/robust_model_Level_1_é²æ£’ç‰ˆ.npz'
    if not model.load_model(model_path):
        print(f"âŒ æ— æ³•åŠ è½½æ¨¡å‹: {model_path}")
        return
    
    # åˆ›å»ºè¯¾ç¨‹å­¦ä¹ ç”Ÿæˆå™¨
    curriculum_generator = CurriculumPenaltyGenerator(model, tokenizer)
    
    # æµ‹è¯•ä¸åŒé˜¶æ®µçš„ç”Ÿæˆæ•ˆæœ
    test_input = "p -> q"
    input_sequence = tokenizer.encode(test_input)
    
    print(f"æµ‹è¯•è¾“å…¥: '{test_input}'")
    print("=" * 40)
    
    # æ¨¡æ‹Ÿä¸åŒè®­ç»ƒé˜¶æ®µ
    test_epochs = [5, 15, 30, 45, 55]
    
    for epoch in test_epochs:
        curriculum_generator.update_epoch(epoch)
        
        # ç”Ÿæˆå¤šä¸ªæ ·æœ¬è§‚å¯Ÿå·®å¼‚
        print(f"\nğŸ§ª è½®æ¬¡ {epoch} çš„ç”Ÿæˆç»“æœ:")
        for i in range(3):
            tokens, text = curriculum_generator.generate_with_curriculum(input_sequence)
            print(f"  æ ·æœ¬ {i+1}: '{text}'")
    
    print(f"\nğŸ“Š è¯¾ç¨‹å­¦ä¹ æ•ˆæœåˆ†æ:")
    print(f"  é˜¶æ®µ1 (è‡ªç”±æ¢ç´¢): åº”è¯¥çœ‹åˆ°æ›´å¤šæ ·åŒ–ä½†å¯èƒ½ä¸å®Œæ•´çš„è¾“å‡º")
    print(f"  é˜¶æ®µ2 (æ¸è¿›çº¦æŸ): åº”è¯¥çœ‹åˆ°é€æ­¥æ”¹å–„çš„ç»“æ„")
    print(f"  é˜¶æ®µ3 (ç²¾ç»†è°ƒä¼˜): åº”è¯¥çœ‹åˆ°æœ€è§„èŒƒçš„è¾“å‡º")


if __name__ == "__main__":
    test_curriculum_penalty_system()
