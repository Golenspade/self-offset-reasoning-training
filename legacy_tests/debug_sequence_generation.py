"""
æ·±åº¦è°ƒè¯•åºåˆ—ç”Ÿæˆæœºåˆ¶
åˆ†æä¸ºä»€ä¹ˆæ¨¡å‹é™·å…¥ "-> -> -> ..." å¾ªç¯
"""

import sys
import os
import numpy as np
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / 'src'))

from logic_transformer.data_utils import Tokenizer, load_dataset
from logic_transformer.models.base_model import ImprovedSimpleModel


def debug_tokenizer():
    """è°ƒè¯•tokenizerçš„tokenæ˜ å°„"""
    print("ğŸ” è°ƒè¯•Tokenizer...")
    print("=" * 50)
    
    tokenizer = Tokenizer()
    
    print(f"è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
    print(f"ç¬¦å·åˆ—è¡¨: {tokenizer.symbols}")
    print(f"PAD_TOKEN: {tokenizer.PAD_TOKEN}")
    print(f"START_TOKEN: {tokenizer.START_TOKEN}")
    print(f"END_TOKEN: {tokenizer.END_TOKEN}")
    
    print(f"\nå­—ç¬¦åˆ°æ•´æ•°æ˜ å°„:")
    for char, idx in tokenizer.char_to_int.items():
        print(f"  '{char}' -> {idx}")
    
    print(f"\næ•´æ•°åˆ°å­—ç¬¦æ˜ å°„:")
    for idx, char in tokenizer.int_to_char.items():
        print(f"  {idx} -> '{char}'")
    
    # æµ‹è¯•ç¼–ç è§£ç 
    test_text = "~p -> q"
    encoded = tokenizer.encode(test_text)
    decoded = tokenizer.decode(encoded)
    
    print(f"\nç¼–ç è§£ç æµ‹è¯•:")
    print(f"  åŸæ–‡: '{test_text}'")
    print(f"  ç¼–ç : {encoded}")
    print(f"  è§£ç : '{decoded}'")
    
    return tokenizer


def debug_model_prediction_step_by_step(model, tokenizer, input_text):
    """é€æ­¥è°ƒè¯•æ¨¡å‹é¢„æµ‹è¿‡ç¨‹"""
    print(f"\nğŸ” é€æ­¥è°ƒè¯•é¢„æµ‹è¿‡ç¨‹...")
    print(f"è¾“å…¥: '{input_text}'")
    print("=" * 60)
    
    # ç¼–ç è¾“å…¥
    input_sequence = tokenizer.encode(input_text)
    print(f"1. è¾“å…¥ç¼–ç : {input_sequence}")
    print(f"   å¯¹åº”å­—ç¬¦: {[tokenizer.int_to_char[token] for token in input_sequence]}")
    
    # ç¼–ç 
    encoded = model.encode(input_sequence)
    print(f"2. ç¼–ç åçš„éšè—çŠ¶æ€å½¢çŠ¶: {encoded.shape}")
    print(f"   ç¼–ç å€¼èŒƒå›´: [{encoded.min():.3f}, {encoded.max():.3f}]")
    
    # å¼€å§‹è§£ç 
    output_sequence = []
    current_token = tokenizer.START_TOKEN
    print(f"3. å¼€å§‹è§£ç ï¼Œåˆå§‹token: {current_token} ('{tokenizer.int_to_char[current_token]}')")
    
    for step in range(10):  # åªè°ƒè¯•å‰10æ­¥
        print(f"\n  æ­¥éª¤ {step + 1}:")
        print(f"    å½“å‰token: {current_token} ('{tokenizer.int_to_char.get(current_token, 'UNKNOWN')}')")
        
        # è§£ç æ­¥éª¤
        new_hidden, output_probs = model.decode_step(encoded, current_token)
        
        print(f"    æ–°éšè—çŠ¶æ€èŒƒå›´: [{new_hidden.min():.3f}, {new_hidden.max():.3f}]")
        print(f"    è¾“å‡ºæ¦‚ç‡å½¢çŠ¶: {output_probs.shape}")
        print(f"    æ¦‚ç‡å’Œ: {output_probs.sum():.6f}")
        
        # åˆ†ææ¦‚ç‡åˆ†å¸ƒ
        top_5_indices = np.argsort(output_probs)[-5:][::-1]
        print(f"    å‰5ä¸ªæœ€é«˜æ¦‚ç‡:")
        for i, idx in enumerate(top_5_indices):
            char = tokenizer.int_to_char.get(idx, 'UNKNOWN')
            prob = output_probs[idx]
            print(f"      {i+1}. token {idx} ('{char}'): {prob:.4f}")
        
        # é€‰æ‹©ä¸‹ä¸€ä¸ªtoken
        next_token = int(np.argmax(output_probs))
        next_char = tokenizer.int_to_char.get(next_token, 'UNKNOWN')
        
        print(f"    é€‰æ‹©çš„ä¸‹ä¸€ä¸ªtoken: {next_token} ('{next_char}')")
        
        # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
        if next_token == tokenizer.END_TOKEN:
            print(f"    é‡åˆ°END_TOKENï¼Œåœæ­¢ç”Ÿæˆ")
            break
        
        # æ£€æŸ¥æœ‰æ•ˆæ€§
        if next_token >= tokenizer.vocab_size or next_token < 0:
            print(f"    æ— æ•ˆtokenï¼Œæ›¿æ¢ä¸ºPAD_TOKEN")
            next_token = tokenizer.PAD_TOKEN
        
        # æ·»åŠ åˆ°åºåˆ—
        output_sequence.append(next_token)
        current_token = next_token
        
        # æ£€æŸ¥æ˜¯å¦é™·å…¥å¾ªç¯
        if len(output_sequence) >= 3:
            last_3 = output_sequence[-3:]
            if len(set(last_3)) == 1:  # æœ€å3ä¸ªtokenéƒ½ç›¸åŒ
                print(f"    âš ï¸  æ£€æµ‹åˆ°å¾ªç¯æ¨¡å¼: {last_3}")
                break
    
    # è§£ç è¾“å‡ºåºåˆ—
    decoded_output = tokenizer.decode(output_sequence)
    print(f"\n4. æœ€ç»ˆè¾“å‡ºåºåˆ—: {output_sequence}")
    print(f"   è§£ç ç»“æœ: '{decoded_output}'")
    
    return output_sequence, decoded_output


def analyze_weight_patterns(model, tokenizer):
    """åˆ†ææ¨¡å‹æƒé‡æ¨¡å¼"""
    print(f"\nğŸ” åˆ†ææ¨¡å‹æƒé‡æ¨¡å¼...")
    print("=" * 50)
    
    print(f"æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
    print(f"  embeddingå½¢çŠ¶: {model.embedding.shape}")
    print(f"  encoder_weightså½¢çŠ¶: {model.encoder_weights.shape}")
    print(f"  decoder_weightså½¢çŠ¶: {model.decoder_weights.shape}")
    print(f"  output_weightså½¢çŠ¶: {model.output_weights.shape}")
    
    # åˆ†æè¾“å‡ºæƒé‡
    print(f"\nè¾“å‡ºæƒé‡åˆ†æ:")
    print(f"  æƒé‡èŒƒå›´: [{model.output_weights.min():.3f}, {model.output_weights.max():.3f}]")
    print(f"  æƒé‡å‡å€¼: {model.output_weights.mean():.3f}")
    print(f"  æƒé‡æ ‡å‡†å·®: {model.output_weights.std():.3f}")
    
    # æŸ¥çœ‹ç‰¹å®štokençš„è¾“å‡ºæƒé‡
    arrow_token = tokenizer.char_to_int.get('>', -1)
    dash_token = tokenizer.char_to_int.get('-', -1)
    end_token = tokenizer.END_TOKEN
    
    if arrow_token >= 0:
        print(f"\n'>' token ({arrow_token}) çš„è¾“å‡ºæƒé‡:")
        arrow_weights = model.output_weights[:, arrow_token]
        print(f"  èŒƒå›´: [{arrow_weights.min():.3f}, {arrow_weights.max():.3f}]")
        print(f"  å‡å€¼: {arrow_weights.mean():.3f}")
    
    if dash_token >= 0:
        print(f"\n'-' token ({dash_token}) çš„è¾“å‡ºæƒé‡:")
        dash_weights = model.output_weights[:, dash_token]
        print(f"  èŒƒå›´: [{dash_weights.min():.3f}, {dash_weights.max():.3f}]")
        print(f"  å‡å€¼: {dash_weights.mean():.3f}")
    
    print(f"\nEND_TOKEN ({end_token}) çš„è¾“å‡ºæƒé‡:")
    end_weights = model.output_weights[:, end_token]
    print(f"  èŒƒå›´: [{end_weights.min():.3f}, {end_weights.max():.3f}]")
    print(f"  å‡å€¼: {end_weights.mean():.3f}")


def main():
    """ä¸»è°ƒè¯•å‡½æ•°"""
    print("ğŸ› æ·±åº¦è°ƒè¯•åºåˆ—ç”Ÿæˆæœºåˆ¶")
    print("=" * 60)
    
    # 1. è°ƒè¯•tokenizer
    tokenizer = debug_tokenizer()
    
    # 2. åŠ è½½æ¨¡å‹
    print(f"\nğŸ” åŠ è½½æ¨¡å‹...")
    model = ImprovedSimpleModel(
        vocab_size=tokenizer.vocab_size,
        hidden_size=128,
        max_length=50,
        learning_rate=0.003
    )
    
    model_path = 'outputs/trained_models/robust_model_Level_1_é²æ£’ç‰ˆ.npz'
    if not model.load_model(model_path):
        print(f"âŒ æ— æ³•åŠ è½½æ¨¡å‹: {model_path}")
        return
    
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # 3. åˆ†ææƒé‡æ¨¡å¼
    analyze_weight_patterns(model, tokenizer)
    
    # 4. é€æ­¥è°ƒè¯•é¢„æµ‹è¿‡ç¨‹
    test_inputs = [
        "p -> q",
        "~p -> r"
    ]
    
    for input_text in test_inputs:
        debug_model_prediction_step_by_step(model, tokenizer, input_text)
    
    print(f"\nğŸ¯ è°ƒè¯•æ€»ç»“:")
    print(f"é€šè¿‡ä»¥ä¸Šåˆ†æï¼Œæˆ‘ä»¬å¯ä»¥ç¡®å®šåºåˆ—ç”Ÿæˆå¾ªç¯çš„å…·ä½“åŸå› ...")


if __name__ == "__main__":
    main()
