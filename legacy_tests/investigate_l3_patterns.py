"""
æ–‡ä»¶å: investigate_l3_patterns.py
æ·±å…¥è°ƒæŸ¥Level 3æ•°æ®ä¸­çš„æ½œåœ¨ä½œå¼Šæ¨¡å¼
å¯»æ‰¾noisy_propå’Œtarget_contrapositiveä¹‹é—´çš„ç®€å•å¯¹åº”å…³ç³»
"""

import json
import random
import re


def load_and_sample_l3_data(filename='data/train_L3_complex.json', num_samples=10):
    """åŠ è½½å¹¶éšæœºé‡‡æ ·Level 3æ•°æ®"""
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            data = []
            for line in f:
                data.append(json.loads(line.strip()))
        
        # éšæœºé‡‡æ ·
        random.seed(42)  # ç¡®ä¿å¯é‡ç°
        samples = random.sample(data, min(num_samples, len(data)))
        return samples
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½æ•°æ®: {e}")
        return []


def analyze_pattern_complexity(samples):
    """åˆ†ææ ·æœ¬çš„æ¨¡å¼å¤æ‚åº¦"""
    print("ğŸ” åˆ†æLevel 3æ•°æ®çš„æ¨¡å¼å¤æ‚åº¦...")
    print("=" * 80)
    
    for i, sample in enumerate(samples):
        noisy = sample.get('noisy_prop', '')
        target = sample.get('target_contrapositive', '')
        original = sample.get('original_prop', '')
        
        print(f"\nğŸ“ æ ·æœ¬ {i+1}:")
        print(f"  åŸå§‹å‘½é¢˜: {original}")
        print(f"  å™ªå£°å‘½é¢˜: {noisy}")
        print(f"  ç›®æ ‡è¾“å‡º: {target}")
        
        # åˆ†æé•¿åº¦å…³ç³»
        print(f"  é•¿åº¦æ¯”è¾ƒ: åŸå§‹({len(original)}) -> å™ªå£°({len(noisy)}) -> ç›®æ ‡({len(target)})")
        
        # å¯»æ‰¾ç®€å•çš„å­—ç¬¦ä¸²å¯¹åº”å…³ç³»
        analyze_string_patterns(noisy, target, i+1)


def analyze_string_patterns(noisy, target, sample_num):
    """åˆ†æä¸¤ä¸ªå­—ç¬¦ä¸²ä¹‹é—´çš„ç®€å•å¯¹åº”å…³ç³»"""
    
    # 1. æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç›´æ¥çš„å­å­—ç¬¦ä¸²å¤åˆ¶
    common_substrings = find_common_substrings(noisy, target, min_length=5)
    if common_substrings:
        print(f"  âš ï¸  å‘ç°å…±åŒå­å­—ç¬¦ä¸²: {common_substrings}")
    
    # 2. æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç®€å•çš„å˜æ¢è§„å¾‹
    check_simple_transformations(noisy, target)
    
    # 3. æ£€æŸ¥å˜é‡å‡ºç°æ¨¡å¼
    check_variable_patterns(noisy, target)


def find_common_substrings(str1, str2, min_length=5):
    """æ‰¾åˆ°ä¸¤ä¸ªå­—ç¬¦ä¸²çš„å…±åŒå­å­—ç¬¦ä¸²"""
    common = []
    for i in range(len(str1) - min_length + 1):
        for j in range(min_length, len(str1) - i + 1):
            substring = str1[i:i+j]
            if substring in str2 and len(substring) >= min_length:
                common.append(substring)
    
    # å»é‡å¹¶æŒ‰é•¿åº¦æ’åº
    common = list(set(common))
    common.sort(key=len, reverse=True)
    return common[:3]  # åªè¿”å›å‰3ä¸ªæœ€é•¿çš„


def check_simple_transformations(noisy, target):
    """æ£€æŸ¥ç®€å•çš„å˜æ¢è§„å¾‹"""
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç®€å•çš„å‰ç¼€/åç¼€å…³ç³»
    if target.startswith('~') and noisy.endswith(')'):
        # æ£€æŸ¥æ˜¯å¦æ˜¯ ~A -> B çš„æ¨¡å¼
        if ' -> ' in target:
            parts = target.split(' -> ', 1)
            if len(parts) == 2:
                neg_consequent = parts[0]  # ~B
                neg_antecedent = parts[1]  # ~A
                
                # æ£€æŸ¥neg_consequentæ˜¯å¦èƒ½ä»noisyçš„æŸéƒ¨åˆ†ç®€å•å¾—åˆ°
                if '|' in noisy:
                    noisy_parts = noisy.split('|')
                    if len(noisy_parts) >= 2:
                        last_part = noisy_parts[-1].strip().rstrip(')')
                        if neg_consequent == f"~{last_part}" or neg_consequent == f"~({last_part})":
                            print(f"  ğŸš¨ å‘ç°ä½œå¼Šæ¨¡å¼: ç›®æ ‡å¼€å¤´'{neg_consequent}'å¯èƒ½ç›´æ¥æ¥è‡ªå™ªå£°ç»“å°¾'{last_part}'")


def check_variable_patterns(noisy, target):
    """æ£€æŸ¥å˜é‡å‡ºç°æ¨¡å¼"""
    
    # æå–æ‰€æœ‰å˜é‡
    noisy_vars = set(re.findall(r'\b[pqrst]\b', noisy))
    target_vars = set(re.findall(r'\b[pqrst]\b', target))
    
    print(f"  å˜é‡åˆ†æ: å™ªå£°ä¸­æœ‰{noisy_vars}, ç›®æ ‡ä¸­æœ‰{target_vars}")
    
    if noisy_vars == target_vars:
        print(f"  âœ“ å˜é‡é›†åˆç›¸åŒ")
    else:
        print(f"  âš ï¸  å˜é‡é›†åˆä¸åŒ")


def analyze_noise_effectiveness(samples):
    """åˆ†æå™ªå£°çš„æœ‰æ•ˆæ€§"""
    print(f"\nğŸ¯ åˆ†æå™ªå£°æœ‰æ•ˆæ€§...")
    print("=" * 50)
    
    noise_types_found = {
        'type1_implication_to_disjunction': 0,
        'type2_double_negation': 0,
        'type3_redundant_parentheses': 0,
        'minimal_change': 0,
        'no_change': 0
    }
    
    for i, sample in enumerate(samples):
        original = sample.get('original_prop', '')
        noisy = sample.get('noisy_prop', '')
        
        # æ£€æŸ¥å™ªå£°ç±»å‹
        if '->' in original and '|' in noisy and '->' not in noisy:
            noise_types_found['type1_implication_to_disjunction'] += 1
        elif '~~' in noisy:
            noise_types_found['type2_double_negation'] += 1
        elif noisy.count('(') > original.count('('):
            noise_types_found['type3_redundant_parentheses'] += 1
        elif len(noisy) - len(original) <= 2:
            noise_types_found['minimal_change'] += 1
        elif noisy == original:
            noise_types_found['no_change'] += 1
    
    print("å™ªå£°ç±»å‹åˆ†å¸ƒ:")
    for noise_type, count in noise_types_found.items():
        percentage = count / len(samples) * 100
        print(f"  {noise_type}: {count} ({percentage:.1f}%)")
    
    # è¯„ä¼°å™ªå£°å¤šæ ·æ€§
    total_effective = sum(noise_types_found.values()) - noise_types_found['no_change']
    if noise_types_found['no_change'] > len(samples) * 0.1:
        print(f"  âš ï¸  è­¦å‘Š: {noise_types_found['no_change']} ä¸ªæ ·æœ¬æ²¡æœ‰åº”ç”¨å™ªå£°")
    
    if noise_types_found['minimal_change'] > len(samples) * 0.3:
        print(f"  âš ï¸  è­¦å‘Š: {noise_types_found['minimal_change']} ä¸ªæ ·æœ¬çš„å™ªå£°å˜åŒ–å¾ˆå°")


def suggest_improvements(samples):
    """åŸºäºåˆ†æç»“æœæå‡ºæ”¹è¿›å»ºè®®"""
    print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®...")
    print("=" * 50)
    
    print("åŸºäºåˆ†æç»“æœï¼Œå»ºè®®ä»¥ä¸‹æ”¹è¿›æªæ–½:")
    print("1. å¢åŠ å™ªå£°åº”ç”¨æ¬¡æ•°: ä»1æ¬¡å¢åŠ åˆ°2-3æ¬¡")
    print("2. ç»„åˆå¤šç§å™ªå£°ç±»å‹: åŒæ—¶åº”ç”¨type1, type2, type3")
    print("3. å¢åŠ ç»“æ„å¤šæ ·æ€§: ç¡®ä¿ä¸»è•´å«ç¬¦ä¸æ€»æ˜¯åœ¨æœ€å¤–å±‚")
    print("4. æ·»åŠ æ›´å¤šå™ªå£°ç±»å‹: å¦‚äº¤æ¢å¾‹ã€ç»“åˆå¾‹å˜æ¢")
    print("5. å¢åŠ éšæœºæ€§: åœ¨å™ªå£°åº”ç”¨ä¸­å¼•å…¥æ›´å¤šéšæœºå› ç´ ")


def cross_evaluation_test():
    """äº¤å‰è¯„ä¼°æµ‹è¯•å»ºè®®"""
    print(f"\nğŸ§ª äº¤å‰è¯„ä¼°æµ‹è¯•å»ºè®®...")
    print("=" * 50)
    
    print("ä¸ºäº†éªŒè¯Level 3æ¨¡å‹æ˜¯å¦å­¦åˆ°äº†çœŸæ­£çš„é€»è¾‘:")
    print("1. åŠ è½½Level 3è®­ç»ƒå¥½çš„æ¨¡å‹")
    print("2. ç”¨å®ƒè¯„ä¼°Level 1çš„éªŒè¯é›†")
    print("3. å¦‚æœå‡†ç¡®ç‡å¾ˆä½(æ¥è¿‘0%)ï¼Œè¯æ˜æ¨¡å‹ç¡®å®åœ¨ä½œå¼Š")
    print("4. å¦‚æœå‡†ç¡®ç‡åˆç†(>30%)ï¼Œè¯´æ˜æ¨¡å‹å­¦åˆ°äº†ä¸€äº›é€šç”¨è§„å¾‹")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ•µï¸ Level 3æ•°æ®æ¨¡å¼è°ƒæŸ¥æŠ¥å‘Š")
    print("=" * 80)
    
    # åŠ è½½æ ·æœ¬æ•°æ®
    samples = load_and_sample_l3_data(num_samples=8)
    
    if not samples:
        print("âŒ æ— æ³•åŠ è½½æ•°æ®ï¼Œè¯·ç¡®ä¿æ•°æ®æ–‡ä»¶å­˜åœ¨")
        return
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(samples)} ä¸ªæ ·æœ¬è¿›è¡Œåˆ†æ")
    
    # è¿›è¡Œå„ç§åˆ†æ
    analyze_pattern_complexity(samples)
    analyze_noise_effectiveness(samples)
    suggest_improvements(samples)
    cross_evaluation_test()
    
    print(f"\nğŸ¯ è°ƒæŸ¥æ€»ç»“:")
    print("å¦‚æœå‘ç°äº†æ˜æ˜¾çš„ä½œå¼Šæ¨¡å¼ï¼Œéœ€è¦:")
    print("1. é‡æ–°è®¾è®¡æ•°æ®ç”Ÿæˆç­–ç•¥")
    print("2. å¢åŠ å™ªå£°çš„å¤æ‚åº¦å’Œéšæœºæ€§") 
    print("3. é‡æ–°ç”ŸæˆLevel 3æ•°æ®é›†")
    print("4. é‡æ–°è®­ç»ƒå¹¶éªŒè¯ç»“æœ")


if __name__ == "__main__":
    main()
