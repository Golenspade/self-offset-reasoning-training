"""
æ··åˆé€»è¾‘ç³»ç»Ÿ
ç¥ç»ç½‘ç»œè´Ÿè´£é€»è¾‘æ¨ç†ï¼Œè§„åˆ™ç³»ç»Ÿè´Ÿè´£è¯­æ³•è§„èŒƒ
å®ç°æ‚¨å»ºè®®çš„"è®©æ¯ä¸ªç³»ç»Ÿåšè‡ªå·±æœ€æ“…é•¿çš„äº‹"
"""

import numpy as np
from typing import List, Tuple, Dict, Optional
import sys
import os
from pathlib import Path
import re

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / 'src'))

from logic_transformer.data_utils import Tokenizer


class LogicIntentExtractor:
    """é€»è¾‘æ„å›¾æå–å™¨ - ä»ç¥ç»ç½‘ç»œçš„"æ··ä¹±"è¾“å‡ºä¸­æå–é€»è¾‘æ„å›¾"""
    
    def __init__(self, tokenizer: Tokenizer):
        self.tokenizer = tokenizer
    
    def extract_variables(self, text: str) -> List[str]:
        """æå–æ–‡æœ¬ä¸­çš„å˜é‡"""
        variables = []
        for char in text:
            if char in 'pqrst':
                variables.append(char)
        return list(set(variables))  # å»é‡
    
    def extract_negations(self, text: str) -> List[str]:
        """æå–å¦å®šçš„å˜é‡"""
        negated_vars = []
        i = 0
        while i < len(text):
            if text[i] == '~' and i + 1 < len(text) and text[i + 1] in 'pqrst':
                negated_vars.append(text[i + 1])
                i += 2
            else:
                i += 1
        return negated_vars
    
    def analyze_logic_intent(self, neural_output: str, input_text: str) -> Dict:
        """åˆ†æç¥ç»ç½‘ç»œè¾“å‡ºçš„é€»è¾‘æ„å›¾"""
        
        # æå–è¾“å…¥çš„ç»“æ„
        input_vars = self.extract_variables(input_text)
        input_negated = self.extract_negations(input_text)
        
        # æå–è¾“å‡ºçš„ç»“æ„
        output_vars = self.extract_variables(neural_output)
        output_negated = self.extract_negations(neural_output)
        
        # åˆ†ææ„å›¾
        intent = {
            'input_variables': input_vars,
            'input_negated': input_negated,
            'output_variables': output_vars,
            'output_negated': output_negated,
            'has_implication': '->' in neural_output,
            'structure_type': 'unknown'
        }
        
        # åˆ¤æ–­ç»“æ„ç±»å‹
        if intent['has_implication'] and output_negated:
            if len(output_vars) >= 1:
                intent['structure_type'] = 'contrapositive_attempt'
            else:
                intent['structure_type'] = 'incomplete_contrapositive'
        elif intent['has_implication']:
            intent['structure_type'] = 'implication_attempt'
        else:
            intent['structure_type'] = 'fragment'
        
        return intent


class LogicRuleGenerator:
    """é€»è¾‘è§„åˆ™ç”Ÿæˆå™¨ - æ ¹æ®æ„å›¾ç”Ÿæˆæ­£ç¡®çš„é€»è¾‘è¡¨è¾¾å¼"""
    
    def __init__(self):
        pass
    
    def generate_contrapositive(self, original_formula: str) -> str:
        """ç”Ÿæˆæ ‡å‡†çš„é€†å¦å‘½é¢˜"""
        
        # ç®€å•çš„é€†å¦å‘½é¢˜ç”Ÿæˆï¼ˆé’ˆå¯¹ A -> B å½¢å¼ï¼‰
        if '->' not in original_formula:
            return original_formula
        
        parts = original_formula.split('->')
        if len(parts) != 2:
            return original_formula
        
        antecedent = parts[0].strip()
        consequent = parts[1].strip()
        
        # ç”Ÿæˆé€†å¦ï¼š~B -> ~A
        neg_consequent = self.negate_expression(consequent)
        neg_antecedent = self.negate_expression(antecedent)
        
        return f"{neg_consequent} -> {neg_antecedent}"
    
    def negate_expression(self, expr: str) -> str:
        """å¦å®šä¸€ä¸ªè¡¨è¾¾å¼"""
        expr = expr.strip()
        
        # å¦‚æœå·²ç»æ˜¯å¦å®šçš„ï¼Œå»æ‰å¦å®š
        if expr.startswith('~'):
            return expr[1:].strip()
        
        # å¦‚æœæ˜¯å•ä¸ªå˜é‡ï¼Œç›´æ¥å¦å®š
        if len(expr) == 1 and expr in 'pqrst':
            return f"~{expr}"
        
        # å¦‚æœæ˜¯å¤æ‚è¡¨è¾¾å¼ï¼ŒåŠ æ‹¬å·å¦å®š
        if '(' in expr or '&' in expr or '|' in expr:
            return f"~({expr})"
        
        # é»˜è®¤æƒ…å†µ
        return f"~{expr}"
    
    def repair_logic_expression(self, intent: Dict, input_text: str) -> str:
        """æ ¹æ®æ„å›¾ä¿®å¤é€»è¾‘è¡¨è¾¾å¼"""
        
        if intent['structure_type'] == 'contrapositive_attempt':
            # ç¥ç»ç½‘ç»œè¯•å›¾ç”Ÿæˆé€†å¦å‘½é¢˜ï¼Œæˆ‘ä»¬å¸®å®ƒå®Œæˆ
            return self.generate_contrapositive(input_text)
        
        elif intent['structure_type'] == 'incomplete_contrapositive':
            # ä¸å®Œæ•´çš„é€†å¦å‘½é¢˜ï¼Œè¡¥å…¨å®ƒ
            return self.generate_contrapositive(input_text)
        
        elif intent['structure_type'] == 'implication_attempt':
            # è¯•å›¾ç”Ÿæˆè•´å«ï¼Œä½†å¯èƒ½ä¸æ˜¯é€†å¦å‘½é¢˜
            return self.generate_contrapositive(input_text)
        
        else:
            # å…¶ä»–æƒ…å†µï¼Œç”Ÿæˆæ ‡å‡†é€†å¦å‘½é¢˜
            return self.generate_contrapositive(input_text)


class HybridLogicSystem:
    """æ··åˆé€»è¾‘ç³»ç»Ÿ - ç»“åˆç¥ç»ç½‘ç»œå’Œè§„åˆ™ç³»ç»Ÿ"""
    
    def __init__(self, model, tokenizer: Tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.intent_extractor = LogicIntentExtractor(tokenizer)
        self.rule_generator = LogicRuleGenerator()
        
        # ç¥ç»ç½‘ç»œç”Ÿæˆå‚æ•°ï¼ˆå®½æ¾è®¾ç½®ï¼Œå…è®¸æ¢ç´¢ï¼‰
        self.neural_max_steps = 15
        self.neural_temperature = 1.0  # å¢åŠ éšæœºæ€§
        
    def generate_neural_attempt(self, input_sequence: List[int]) -> Tuple[str, Dict]:
        """è®©ç¥ç»ç½‘ç»œè‡ªç”±å°è¯•ï¼Œä¸æ–½åŠ å¼ºçº¦æŸ"""
        
        # ç¼–ç è¾“å…¥
        encoded = self.model.encode(input_sequence)
        
        # åˆå§‹åŒ–
        generated_tokens = []
        current_token = self.tokenizer.START_TOKEN
        
        for step in range(self.neural_max_steps):
            # è§£ç æ­¥éª¤
            hidden_state, raw_logits = self.model.decode_step(encoded, current_token)
            
            # åªåº”ç”¨æœ€åŸºæœ¬çš„çº¦æŸ
            logits = raw_logits.copy()
            
            # åŸºæœ¬çš„å¾ªç¯æ£€æµ‹ï¼ˆé˜²æ­¢å®Œå…¨å¡æ­»ï¼‰
            if len(generated_tokens) >= 6:
                last_3 = generated_tokens[-3:]
                prev_3 = generated_tokens[-6:-3]
                if last_3 == prev_3:
                    # æ£€æµ‹åˆ°å¾ªç¯ï¼Œå¼ºåˆ¶ç»“æŸ
                    break
            
            # æ¸©åº¦é‡‡æ ·ï¼ˆå¢åŠ å¤šæ ·æ€§ï¼‰
            if self.neural_temperature != 1.0:
                logits = logits / self.neural_temperature
            
            # è®¡ç®—æ¦‚ç‡
            exp_logits = np.exp(logits - np.max(logits))
            probabilities = exp_logits / np.sum(exp_logits)
            
            # é€‰æ‹©ä¸‹ä¸€ä¸ªtoken
            next_token = int(np.argmax(probabilities))
            
            # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
            if next_token == self.tokenizer.END_TOKEN:
                break
            
            # æ£€æŸ¥tokenæœ‰æ•ˆæ€§
            if next_token >= self.tokenizer.vocab_size or next_token < 0:
                break
            
            # æ·»åŠ åˆ°åºåˆ—
            generated_tokens.append(next_token)
            current_token = next_token
        
        # è§£ç ç¥ç»ç½‘ç»œçš„å°è¯•
        neural_output = self.tokenizer.decode(generated_tokens)
        
        # åˆ†ææ„å›¾
        input_text = self.tokenizer.decode(input_sequence)
        intent = self.intent_extractor.analyze_logic_intent(neural_output, input_text)
        
        return neural_output, intent
    
    def generate_hybrid_solution(self, input_text: str) -> Tuple[str, str, Dict]:
        """ç”Ÿæˆæ··åˆè§£å†³æ–¹æ¡ˆ"""
        
        print(f"ğŸ§  æ··åˆç³»ç»Ÿå¤„ç†: '{input_text}'")
        
        # 1. ç¥ç»ç½‘ç»œè‡ªç”±å°è¯•
        input_sequence = self.tokenizer.encode(input_text)
        neural_output, intent = self.generate_neural_attempt(input_sequence)
        
        print(f"  ç¥ç»ç½‘ç»œè¾“å‡º: '{neural_output}'")
        print(f"  è¯†åˆ«æ„å›¾: {intent['structure_type']}")
        
        # 2. è§„åˆ™ç³»ç»Ÿä¿®å¤å’Œå®Œå–„
        corrected_output = self.rule_generator.repair_logic_expression(intent, input_text)
        
        print(f"  è§„åˆ™ç³»ç»Ÿä¿®æ­£: '{corrected_output}'")
        
        return neural_output, corrected_output, intent


def test_hybrid_system():
    """æµ‹è¯•æ··åˆé€»è¾‘ç³»ç»Ÿ"""
    print("ğŸ¤– æµ‹è¯•æ··åˆé€»è¾‘ç³»ç»Ÿ")
    print("=" * 60)
    
    # åŠ è½½æ¨¡å‹å’Œtokenizer
    tokenizer = Tokenizer()
    
    from logic_transformer.models.base_model import ImprovedSimpleModel
    
    model = ImprovedSimpleModel(
        vocab_size=tokenizer.vocab_size,
        hidden_size=128,
        max_length=50,
        learning_rate=0.003
    )
    
    model_path = 'outputs/trained_models/robust_model_Level_1_é²æ£’ç‰ˆ.npz'
    if not model.load_model(model_path):
        print(f"âŒ æ— æ³•åŠ è½½æ¨¡å‹: {model_path}")
        return
    
    # åˆ›å»ºæ··åˆç³»ç»Ÿ
    hybrid_system = HybridLogicSystem(model, tokenizer)
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        ("p -> q", "~q -> ~p"),
        ("~p -> r", "~r -> p"),
        ("q -> s", "~s -> ~q"),
        ("(p & q) -> r", "~r -> ~(p & q)")
    ]
    
    print("ğŸ”„ æ··åˆç³»ç»Ÿå·¥ä½œæµç¨‹æ¼”ç¤º:")
    print("=" * 50)
    
    for test_input, expected in test_cases:
        print(f"\nğŸ“ æµ‹è¯•æ¡ˆä¾‹: '{test_input}' â†’ æœŸæœ›: '{expected}'")
        print("-" * 60)
        
        # è¿è¡Œæ··åˆç³»ç»Ÿ
        neural_output, final_output, intent = hybrid_system.generate_hybrid_solution(test_input)
        
        # è¯„ä¼°ç»“æœ
        print(f"  ğŸ“Š è¯„ä¼°:")
        if final_output == expected:
            print(f"    âœ… å®Œå…¨æ­£ç¡®!")
        elif final_output.replace(' ', '') == expected.replace(' ', ''):
            print(f"    âœ… é€»è¾‘æ­£ç¡® (æ ¼å¼ç•¥æœ‰å·®å¼‚)")
        else:
            print(f"    ğŸ”„ éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›")
        
        print(f"    ç¥ç»ç½‘ç»œè´¡çŒ®: æä¾›äº†é€»è¾‘æ–¹å‘å’Œå˜é‡ä¿¡æ¯")
        print(f"    è§„åˆ™ç³»ç»Ÿè´¡çŒ®: ç¡®ä¿äº†è¯­æ³•æ­£ç¡®æ€§å’Œå®Œæ•´æ€§")
    
    print(f"\nğŸ¯ æ··åˆç³»ç»Ÿä¼˜åŠ¿æ€»ç»“:")
    print(f"  1. ç¥ç»ç½‘ç»œä¸“æ³¨äºé€»è¾‘ç†è§£ï¼Œä¸è¢«è¯­æ³•çº¦æŸæŸç¼š")
    print(f"  2. è§„åˆ™ç³»ç»Ÿç¡®ä¿è¾“å‡ºçš„æ­£ç¡®æ€§å’Œå®Œæ•´æ€§")
    print(f"  3. ä¸¤ä¸ªç³»ç»Ÿå„å¸å…¶èŒï¼Œé¿å…äº†å†²çªçš„ä¼˜åŒ–ç›®æ ‡")
    print(f"  4. å³ä½¿ç¥ç»ç½‘ç»œè¾“å‡ºä¸å®Œç¾ï¼Œè§„åˆ™ç³»ç»Ÿä¹Ÿèƒ½ä¿®æ­£")


if __name__ == "__main__":
    test_hybrid_system()
