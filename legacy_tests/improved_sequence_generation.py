"""
æ”¹è¿›çš„åºåˆ—ç”Ÿæˆæœºåˆ¶
å®ç°å¤šç§æƒ©ç½šç­–ç•¥æ¥è§£å†³å¾ªç¯é—®é¢˜
"""

import numpy as np
from typing import List, Tuple, Dict
import sys
import os
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / 'src'))

from logic_transformer.data_utils import Tokenizer


class ImprovedSequenceGenerator:
    """æ”¹è¿›çš„åºåˆ—ç”Ÿæˆå™¨ï¼ŒåŒ…å«å¤šç§æƒ©ç½šæœºåˆ¶"""
    
    def __init__(self, model, tokenizer: Tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        
        # æƒ©ç½šå‚æ•°
        self.repetition_penalty = 1.2  # é‡å¤æƒ©ç½šå¼ºåº¦
        self.length_penalty = 0.1      # é•¿åº¦æƒ©ç½šå¼ºåº¦
        self.end_token_boost = 2.0     # END_TOKENæå‡å€æ•°
        self.max_repeats = 2           # æœ€å¤§é‡å¤æ¬¡æ•°
        self.max_length = 30           # æœ€å¤§åºåˆ—é•¿åº¦
        
        # å¾ªç¯æ£€æµ‹å‚æ•°
        self.cycle_detection_window = 3  # å¾ªç¯æ£€æµ‹çª—å£
        self.cycle_penalty = 0.1         # å¾ªç¯æƒ©ç½šå¼ºåº¦
        
    def apply_repetition_penalty(self, logits: np.ndarray, generated_tokens: List[int]) -> np.ndarray:
        """
        åº”ç”¨é‡å¤æƒ©ç½š
        å¯¹å·²ç”Ÿæˆçš„tokené™ä½æ¦‚ç‡
        """
        if not generated_tokens:
            return logits
        
        # ç»Ÿè®¡tokenå‡ºç°æ¬¡æ•°
        token_counts = {}
        for token in generated_tokens:
            token_counts[token] = token_counts.get(token, 0) + 1
        
        # åº”ç”¨æƒ©ç½š
        for token, count in token_counts.items():
            if token < len(logits):
                # é‡å¤æ¬¡æ•°è¶Šå¤šï¼Œæƒ©ç½šè¶Šé‡
                penalty = self.repetition_penalty ** count
                logits[token] /= penalty
        
        return logits
    
    def apply_cycle_penalty(self, logits: np.ndarray, generated_tokens: List[int]) -> np.ndarray:
        """
        åº”ç”¨å¾ªç¯æ£€æµ‹æƒ©ç½š
        æ£€æµ‹å¹¶æƒ©ç½šå¾ªç¯æ¨¡å¼
        """
        if len(generated_tokens) < self.cycle_detection_window * 2:
            return logits
        
        # æ£€æµ‹æœ€è¿‘çš„å¾ªç¯æ¨¡å¼
        window_size = self.cycle_detection_window
        recent_tokens = generated_tokens[-window_size:]
        
        # æ£€æŸ¥æ˜¯å¦ä¸ä¹‹å‰çš„çª—å£é‡å¤
        for i in range(len(generated_tokens) - window_size * 2, -1, -1):
            if i < 0:
                break
            
            prev_window = generated_tokens[i:i + window_size]
            if prev_window == recent_tokens:
                # å‘ç°å¾ªç¯ï¼Œæƒ©ç½šå¾ªç¯ä¸­çš„æ‰€æœ‰token
                for token in set(recent_tokens):
                    if token < len(logits):
                        logits[token] *= self.cycle_penalty
                break
        
        return logits
    
    def apply_length_penalty(self, logits: np.ndarray, current_length: int) -> np.ndarray:
        """
        åº”ç”¨é•¿åº¦æƒ©ç½š
        éšç€åºåˆ—å˜é•¿ï¼Œå¢åŠ END_TOKENçš„æ¦‚ç‡
        """
        if current_length > 5:  # æœ€å°é•¿åº¦é˜ˆå€¼
            # é•¿åº¦æƒ©ç½šï¼šåºåˆ—è¶Šé•¿ï¼ŒEND_TOKENæ¦‚ç‡è¶Šé«˜
            length_factor = 1 + (current_length - 5) * self.length_penalty
            logits[self.tokenizer.END_TOKEN] *= length_factor
        
        return logits
    
    def apply_end_token_boost(self, logits: np.ndarray, generated_tokens: List[int]) -> np.ndarray:
        """
        åº”ç”¨END_TOKENæå‡
        åœ¨åˆé€‚çš„æ—¶æœºæå‡END_TOKENæ¦‚ç‡
        """
        # å¦‚æœå·²ç»ç”Ÿæˆäº†åŸºæœ¬çš„é€†å¦å‘½é¢˜ç»“æ„ï¼Œæå‡END_TOKEN
        if len(generated_tokens) >= 5:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«åŸºæœ¬ç»“æ„ï¼š~ å˜é‡ -> 
            has_negation = self.tokenizer.char_to_int['~'] in generated_tokens
            has_arrow = (self.tokenizer.char_to_int['-'] in generated_tokens and 
                        self.tokenizer.char_to_int['>'] in generated_tokens)
            has_variable = any(token in [0, 1, 2, 3, 4] for token in generated_tokens)  # p,q,r,s,t
            
            if has_negation and has_arrow and has_variable:
                logits[self.tokenizer.END_TOKEN] *= self.end_token_boost
        
        return logits
    
    def apply_structural_guidance(self, logits: np.ndarray, generated_tokens: List[int]) -> np.ndarray:
        """
        åº”ç”¨ç»“æ„åŒ–æŒ‡å¯¼
        æ ¹æ®é€»è¾‘å‘½é¢˜çš„ç»“æ„è§„å¾‹è°ƒæ•´æ¦‚ç‡
        """
        if not generated_tokens:
            return logits
        
        last_token = generated_tokens[-1]
        
        # ç»“æ„åŒ–è§„åˆ™
        if last_token == self.tokenizer.char_to_int['~']:
            # å¦å®šç¬¦ååº”è¯¥è·Ÿå˜é‡æˆ–æ‹¬å·
            for token in [0, 1, 2, 3, 4, 10]:  # p,q,r,s,t,(
                if token < len(logits):
                    logits[token] *= 2.0
        
        elif last_token == self.tokenizer.char_to_int['-']:
            # ç ´æŠ˜å·ååº”è¯¥è·Ÿå¤§äºå·
            logits[self.tokenizer.char_to_int['>']] *= 3.0
            # å¼ºçƒˆæŠ‘åˆ¶å†æ¬¡ç”Ÿæˆç ´æŠ˜å·
            logits[self.tokenizer.char_to_int['-']] *= 0.1
        
        elif last_token == self.tokenizer.char_to_int['>']:
            # å¤§äºå·ååº”è¯¥è·Ÿç©ºæ ¼æˆ–å˜é‡
            logits[self.tokenizer.char_to_int[' ']] *= 2.0
            for token in [0, 1, 2, 3, 4]:  # p,q,r,s,t
                if token < len(logits):
                    logits[token] *= 1.5
            # æŠ‘åˆ¶ç«‹å³ç”Ÿæˆå¦ä¸€ä¸ªç®­å¤´
            logits[self.tokenizer.char_to_int['-']] *= 0.3
        
        elif last_token == self.tokenizer.char_to_int[' ']:
            # ç©ºæ ¼åçš„è§„åˆ™
            if len(generated_tokens) >= 2:
                prev_token = generated_tokens[-2]
                if prev_token == self.tokenizer.char_to_int['>']:
                    # -> åçš„ç©ºæ ¼ï¼Œåº”è¯¥è·Ÿå˜é‡æˆ–å¦å®š
                    for token in [0, 1, 2, 3, 4, 5]:  # p,q,r,s,t,~
                        if token < len(logits):
                            logits[token] *= 2.0
                    # å¼ºçƒˆæŠ‘åˆ¶å†æ¬¡ç”Ÿæˆç®­å¤´
                    logits[self.tokenizer.char_to_int['-']] *= 0.1
        
        return logits
    
    def detect_completion(self, generated_tokens: List[int]) -> bool:
        """
        æ£€æµ‹åºåˆ—æ˜¯å¦åº”è¯¥å®Œæˆ
        """
        if len(generated_tokens) < 3:
            return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„é€†å¦å‘½é¢˜ç»“æ„
        text = self.tokenizer.decode(generated_tokens)
        
        # åŸºæœ¬å®Œæ•´æ€§æ£€æŸ¥
        has_negation = '~' in text
        has_arrow = '->' in text
        has_variable = any(var in text for var in ['p', 'q', 'r', 's', 't'])
        
        # å¦‚æœæœ‰åŸºæœ¬ç»“æ„ä¸”é•¿åº¦åˆç†ï¼Œå¯ä»¥ç»“æŸ
        if has_negation and has_arrow and has_variable and len(generated_tokens) >= 5:
            return True
        
        # å¦‚æœåºåˆ—è¿‡é•¿ï¼Œå¼ºåˆ¶ç»“æŸ
        if len(generated_tokens) >= self.max_length:
            return True
        
        return False
    
    def generate_sequence(self, input_sequence: List[int], max_steps: int = 20) -> Tuple[List[int], str]:
        """
        ç”Ÿæˆæ”¹è¿›çš„åºåˆ—
        """
        # ç¼–ç è¾“å…¥
        encoded = self.model.encode(input_sequence)
        
        # åˆå§‹åŒ–
        generated_tokens = []
        current_token = self.tokenizer.START_TOKEN
        
        for step in range(max_steps):
            # è§£ç æ­¥éª¤
            hidden_state, raw_logits = self.model.decode_step(encoded, current_token)
            
            # è½¬æ¢ä¸ºæ¦‚ç‡ï¼ˆsoftmaxï¼‰
            logits = raw_logits.copy()
            
            # åº”ç”¨å„ç§æƒ©ç½šæœºåˆ¶
            logits = self.apply_repetition_penalty(logits, generated_tokens)
            logits = self.apply_cycle_penalty(logits, generated_tokens)
            logits = self.apply_length_penalty(logits, len(generated_tokens))
            logits = self.apply_end_token_boost(logits, generated_tokens)
            logits = self.apply_structural_guidance(logits, generated_tokens)
            
            # é‡æ–°è®¡ç®—æ¦‚ç‡
            exp_logits = np.exp(logits - np.max(logits))  # æ•°å€¼ç¨³å®šæ€§
            probabilities = exp_logits / np.sum(exp_logits)
            
            # é€‰æ‹©ä¸‹ä¸€ä¸ªtoken
            next_token = int(np.argmax(probabilities))
            
            # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
            if next_token == self.tokenizer.END_TOKEN:
                break
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥å¼ºåˆ¶å®Œæˆ
            if self.detect_completion(generated_tokens):
                break
            
            # æ£€æŸ¥tokenæœ‰æ•ˆæ€§
            if next_token >= self.tokenizer.vocab_size or next_token < 0:
                next_token = self.tokenizer.END_TOKEN
                break
            
            # æ·»åŠ åˆ°åºåˆ—
            generated_tokens.append(next_token)
            current_token = next_token
            
            # å¾ªç¯æ£€æµ‹ï¼šå¦‚æœæ£€æµ‹åˆ°ä¸¥é‡å¾ªç¯ï¼Œå¼ºåˆ¶ç»“æŸ
            if len(generated_tokens) >= 6:
                last_3 = generated_tokens[-3:]
                prev_3 = generated_tokens[-6:-3]
                if last_3 == prev_3:
                    print(f"æ£€æµ‹åˆ°å¾ªç¯ï¼Œå¼ºåˆ¶ç»“æŸ: {last_3}")
                    break
        
        # è§£ç ç»“æœ
        decoded_text = self.tokenizer.decode(generated_tokens)
        
        return generated_tokens, decoded_text


def test_improved_generation():
    """æµ‹è¯•æ”¹è¿›çš„ç”Ÿæˆæœºåˆ¶"""
    print("ğŸš€ æµ‹è¯•æ”¹è¿›çš„åºåˆ—ç”Ÿæˆæœºåˆ¶")
    print("=" * 60)
    
    # åŠ è½½æ¨¡å‹å’Œtokenizer
    tokenizer = Tokenizer()
    
    # è¿™é‡Œéœ€è¦å¯¼å…¥å®é™…çš„æ¨¡å‹ç±»
    from logic_transformer.models.base_model import ImprovedSimpleModel
    
    model = ImprovedSimpleModel(
        vocab_size=tokenizer.vocab_size,
        hidden_size=128,
        max_length=50,
        learning_rate=0.003
    )
    
    model_path = 'outputs/trained_models/robust_model_Level_1_é²æ£’ç‰ˆ.npz'
    if not model.load_model(model_path):
        print(f"âŒ æ— æ³•åŠ è½½æ¨¡å‹: {model_path}")
        return
    
    # åˆ›å»ºæ”¹è¿›çš„ç”Ÿæˆå™¨
    generator = ImprovedSequenceGenerator(model, tokenizer)
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        "p -> q",
        "~p -> r",
        "(p & q) -> s"
    ]
    
    for test_input in test_cases:
        print(f"\nğŸ§ª æµ‹è¯•è¾“å…¥: '{test_input}'")
        print("-" * 40)
        
        # ç¼–ç è¾“å…¥
        input_sequence = tokenizer.encode(test_input)
        
        # ç”Ÿæˆåºåˆ—
        generated_tokens, decoded_text = generator.generate_sequence(input_sequence)
        
        print(f"ç”Ÿæˆçš„tokens: {generated_tokens}")
        print(f"è§£ç ç»“æœ: '{decoded_text}'")
        
        # åˆ†æç»“æœ
        if '-> -> ->' in decoded_text:
            print("âŒ ä»ç„¶å­˜åœ¨å¾ªç¯é—®é¢˜")
        elif '->' in decoded_text and len(decoded_text.strip()) > 3:
            print("âœ… ç”Ÿæˆäº†åˆç†çš„é€»è¾‘è¡¨è¾¾å¼")
        else:
            print("ğŸ”„ ç”Ÿæˆç»“æœéœ€è¦è¿›ä¸€æ­¥æ”¹è¿›")


if __name__ == "__main__":
    test_improved_generation()
