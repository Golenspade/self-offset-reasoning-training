"""
ä½¿ç”¨é²æ£’æ•°æ®é›†è¿›è¡Œè®­ç»ƒå¯¹æ¯”å®éªŒ
éªŒè¯æ˜¯å¦èƒ½å µæ­»ä½œå¼Šæ·å¾„ï¼Œè¿«ä½¿æ¨¡å‹å­¦ä¹ çœŸæ­£çš„é€»è¾‘
"""

import json
import numpy as np
import matplotlib.pyplot as plt
import time
import sys
import os
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / 'src'))

from logic_transformer.data_utils import Tokenizer, load_dataset
from logic_transformer.models.base_model import ImprovedSimpleModel


def train_robust_model(model, train_data, val_data, tokenizer, epochs=20, model_name="Model"):
    """è®­ç»ƒé²æ£’æ¨¡å‹"""
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ {model_name}")
    print(f"è®­ç»ƒæ ·æœ¬: {len(train_data)}, éªŒè¯æ ·æœ¬: {len(val_data)}")
    
    history = {
        'epochs': [],
        'train_loss': [],
        'val_exact_acc': [],
        'val_logical_acc': []
    }
    
    best_accuracy = 0
    
    for epoch in range(epochs):
        start_time = time.time()
        
        # è®­ç»ƒé˜¶æ®µ
        total_loss = 0
        num_batches = 0
        batch_size = 8  # å‡å°æ‰¹æ¬¡å¤§å°ä»¥é€‚åº”æ›´å¤æ‚çš„æ•°æ®
        
        np.random.shuffle(train_data)
        
        for i in range(0, len(train_data), batch_size):
            batch = train_data[i:i+batch_size]
            batch_loss = 0
            
            for sample in batch:
                loss = model.train_step_improved(sample['input'], sample['target'], tokenizer)
                batch_loss += loss
            
            total_loss += batch_loss / len(batch)
            num_batches += 1
        
        avg_loss = total_loss / num_batches if num_batches > 0 else 0
        
        # éªŒè¯é˜¶æ®µ
        val_exact_acc, val_logical_acc = evaluate_robust_model(model, val_data, tokenizer)
        
        if val_exact_acc > best_accuracy:
            best_accuracy = val_exact_acc
        
        epoch_time = time.time() - start_time
        
        # è®°å½•å†å²
        history['epochs'].append(epoch + 1)
        history['train_loss'].append(avg_loss)
        history['val_exact_acc'].append(val_exact_acc)
        history['val_logical_acc'].append(val_logical_acc)
        
        print(f"Epoch {epoch+1:2d}/{epochs}: "
              f"Loss={avg_loss:.4f}, "
              f"ç²¾ç¡®={val_exact_acc:.1%}, "
              f"é€»è¾‘={val_logical_acc:.1%}, "
              f"æ—¶é—´={epoch_time:.1f}s")
    
    print(f"âœ… {model_name} è®­ç»ƒå®Œæˆï¼Œæœ€ä½³ç²¾ç¡®å‡†ç¡®ç‡: {best_accuracy:.2%}")
    return history, best_accuracy


def evaluate_robust_model(model, data, tokenizer, max_samples=50):
    """è¯„ä¼°é²æ£’æ¨¡å‹æ€§èƒ½"""
    correct_exact = 0
    correct_logical = 0
    total = min(len(data), max_samples)
    
    for i, sample in enumerate(data[:total]):
        try:
            predicted_tokens = model.predict(sample['input'], tokenizer)
            predicted_text = tokenizer.decode(predicted_tokens).strip()
            target_text = sample['target_text'].strip()
            
            # ç²¾ç¡®åŒ¹é…
            if predicted_text == target_text:
                correct_exact += 1
                correct_logical += 1
            else:
                # æ£€æŸ¥æ˜¯å¦è‡³å°‘ç”Ÿæˆäº†åˆç†çš„é€»è¾‘è¡¨è¾¾å¼
                if (len(predicted_text) > 5 and 
                    '->' in predicted_text and 
                    not predicted_text.startswith('-> -> ->')):
                    correct_logical += 1
        except:
            continue
    
    exact_acc = correct_exact / total if total > 0 else 0
    logical_acc = correct_logical / total if total > 0 else 0
    
    return exact_acc, logical_acc


def run_robust_training_experiments():
    """è¿è¡Œé²æ£’è®­ç»ƒå®éªŒ"""
    print("ğŸ›¡ï¸ é²æ£’æ•°æ®é›†è®­ç»ƒå¯¹æ¯”å®éªŒ")
    print("=" * 60)
    
    np.random.seed(42)
    tokenizer = Tokenizer()
    
    # å®šä¹‰å®éªŒé…ç½®
    experiments = [
        {
            'name': 'Level 1 é²æ£’ç‰ˆ',
            'train_file': 'data/train_level_1_é²æ£’ç‰ˆ.json',
            'val_file': 'data/val_level_1_é²æ£’ç‰ˆ.json',
            'color': 'blue',
            'max_samples': 2000
        },
        {
            'name': 'Level 2 é²æ£’ç‰ˆ',
            'train_file': 'data/train_level_2_é²æ£’ç‰ˆ.json',
            'val_file': 'data/val_level_2_é²æ£’ç‰ˆ.json',
            'color': 'green',
            'max_samples': 1500
        },
        {
            'name': 'Level 3 é²æ£’ç‰ˆ',
            'train_file': 'data/train_level_3_é²æ£’ç‰ˆ.json',
            'val_file': 'data/val_level_3_é²æ£’ç‰ˆ.json',
            'color': 'red',
            'max_samples': 1000
        }
    ]
    
    all_histories = []
    all_names = []
    final_accuracies = []
    
    for i, exp in enumerate(experiments):
        print(f"\nğŸ“Š å®éªŒ {i+1}/3: {exp['name']}")
        print("-" * 40)
        
        # åŠ è½½æ•°æ®
        train_data = load_dataset(exp['train_file'], tokenizer, exp['max_samples'])
        val_data = load_dataset(exp['val_file'], tokenizer, min(200, exp['max_samples']//10))
        
        if not train_data or not val_data:
            print(f"âŒ æ— æ³•åŠ è½½æ•°æ®: {exp['train_file']}")
            continue
        
        print(f"æ•°æ®åŠ è½½æˆåŠŸ: è®­ç»ƒ{len(train_data)}æ ·æœ¬, éªŒè¯{len(val_data)}æ ·æœ¬")
        
        # åˆ›å»ºæ¨¡å‹
        model = ImprovedSimpleModel(
            vocab_size=tokenizer.vocab_size,
            hidden_size=128,
            max_length=50,
            learning_rate=0.003  # ç¨å¾®é™ä½å­¦ä¹ ç‡ä»¥æé«˜ç¨³å®šæ€§
        )
        
        # è®­ç»ƒæ¨¡å‹
        history, best_acc = train_robust_model(
            model, train_data, val_data, tokenizer, 
            epochs=20, model_name=exp['name']
        )
        
        # ä¿å­˜ç»“æœ
        all_histories.append(history)
        all_names.append(exp['name'])
        final_accuracies.append(best_acc)
        
        # ä¿å­˜æ¨¡å‹
        model_path = f"outputs/trained_models/robust_model_{exp['name'].replace(' ', '_')}.npz"
        os.makedirs(os.path.dirname(model_path), exist_ok=True)
        model.save_model(model_path)
        print(f"æ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    return all_histories, all_names, final_accuracies, experiments


def plot_robust_training_results(histories, names, final_accuracies):
    """ç»˜åˆ¶é²æ£’è®­ç»ƒç»“æœ"""
    print(f"\nğŸ“ˆ ç»˜åˆ¶é²æ£’è®­ç»ƒç»“æœ...")
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('é²æ£’æ•°æ®é›†è®­ç»ƒç»“æœ - å µæ­»ä½œå¼Šæ·å¾„åçš„çœŸå®å­¦ä¹ ', fontsize=16, fontweight='bold')
    
    colors = ['blue', 'green', 'red']
    
    # 1. è®­ç»ƒæŸå¤±
    ax1.set_title('è®­ç»ƒæŸå¤±æ›²çº¿', fontsize=14, fontweight='bold')
    for i, (history, name) in enumerate(zip(histories, names)):
        ax1.plot(history['epochs'], history['train_loss'], 
                color=colors[i], linewidth=2, marker='o', markersize=3, label=name)
    ax1.set_xlabel('è®­ç»ƒè½®æ¬¡')
    ax1.set_ylabel('è®­ç»ƒæŸå¤±')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. ç²¾ç¡®å‡†ç¡®ç‡
    ax2.set_title('ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡', fontsize=14, fontweight='bold')
    for i, (history, name) in enumerate(zip(histories, names)):
        ax2.plot(history['epochs'], [acc * 100 for acc in history['val_exact_acc']], 
                color=colors[i], linewidth=2, marker='s', markersize=3, label=name)
    ax2.set_xlabel('è®­ç»ƒè½®æ¬¡')
    ax2.set_ylabel('ç²¾ç¡®å‡†ç¡®ç‡ (%)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. é€»è¾‘å‡†ç¡®ç‡
    ax3.set_title('é€»è¾‘ç­‰ä»·å‡†ç¡®ç‡', fontsize=14, fontweight='bold')
    for i, (history, name) in enumerate(zip(histories, names)):
        ax3.plot(history['epochs'], [acc * 100 for acc in history['val_logical_acc']], 
                color=colors[i], linewidth=2, marker='^', markersize=3, label=name)
    ax3.set_xlabel('è®­ç»ƒè½®æ¬¡')
    ax3.set_ylabel('é€»è¾‘å‡†ç¡®ç‡ (%)')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. æœ€ç»ˆå‡†ç¡®ç‡å¯¹æ¯”
    ax4.set_title('æœ€ç»ˆå‡†ç¡®ç‡å¯¹æ¯”', fontsize=14, fontweight='bold')
    bars = ax4.bar(range(len(names)), [acc * 100 for acc in final_accuracies], 
                   color=colors[:len(names)], alpha=0.7, edgecolor='black')
    ax4.set_xlabel('å®éªŒç±»å‹')
    ax4.set_ylabel('æœ€ç»ˆç²¾ç¡®å‡†ç¡®ç‡ (%)')
    ax4.set_xticks(range(len(names)))
    ax4.set_xticklabels([name.replace('é²æ£’ç‰ˆ', '') for name in names], rotation=45)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, acc in zip(bars, final_accuracies):
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                f'{acc:.1%}', ha='center', va='bottom', fontweight='bold')
    
    ax4.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾åƒ
    os.makedirs('outputs/figures', exist_ok=True)
    plt.savefig('outputs/figures/robust_training_results.png', dpi=300, bbox_inches='tight')
    plt.savefig('outputs/figures/robust_training_results.pdf', bbox_inches='tight')
    
    print(f"âœ… é²æ£’è®­ç»ƒç»“æœå›¾å·²ä¿å­˜:")
    print(f"  ğŸ“Š outputs/figures/robust_training_results.png")


def analyze_robust_results(names, final_accuracies, histories):
    """åˆ†æé²æ£’è®­ç»ƒç»“æœ"""
    print(f"\nğŸ¯ é²æ£’è®­ç»ƒç»“æœåˆ†æ")
    print("=" * 60)
    
    print(f"{'å®éªŒåç§°':<15} {'æœ€ç»ˆç²¾ç¡®å‡†ç¡®ç‡':<15} {'å­¦ä¹ è¶‹åŠ¿':<15}")
    print("-" * 50)
    
    for i, (name, acc) in enumerate(zip(names, final_accuracies)):
        history = histories[i]
        
        # åˆ†æå­¦ä¹ è¶‹åŠ¿
        if len(history['val_exact_acc']) >= 10:
            early_acc = np.mean(history['val_exact_acc'][:5])
            late_acc = np.mean(history['val_exact_acc'][-5:])
            trend = "ä¸Šå‡" if late_acc > early_acc + 0.01 else "ç¨³å®š" if abs(late_acc - early_acc) <= 0.01 else "ä¸‹é™"
        else:
            trend = "æ•°æ®ä¸è¶³"
        
        print(f"{name:<15} {acc:<15.1%} {trend:<15}")
    
    # æ€»ä½“åˆ†æ
    print(f"\nğŸ” å…³é”®å‘ç°:")
    
    max_acc = max(final_accuracies)
    best_idx = final_accuracies.index(max_acc)
    
    if max_acc > 0.1:
        print(f"âœ… æˆåŠŸï¼{names[best_idx]} è¾¾åˆ°äº† {max_acc:.1%} çš„å‡†ç¡®ç‡")
        print(f"   è¿™è¡¨æ˜é²æ£’æ•°æ®é›†æˆåŠŸè¿«ä½¿æ¨¡å‹å­¦ä¹ çœŸæ­£çš„é€»è¾‘æ¨ç†")
    elif max_acc > 0.05:
        print(f"ğŸ”„ è¿›å±•ä¸­ï¼šæœ€ä½³å‡†ç¡®ç‡ {max_acc:.1%}ï¼Œéœ€è¦æ›´å¤šè®­ç»ƒ")
    else:
        print(f"âš ï¸  æŒ‘æˆ˜æ€§ï¼šæ‰€æœ‰å®éªŒå‡†ç¡®ç‡éƒ½è¾ƒä½ï¼Œæ•°æ®å¯èƒ½è¿‡äºå¤æ‚")
    
    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å¼‚å¸¸çš„å¿«é€Ÿå­¦ä¹ 
    for i, (name, history) in enumerate(zip(names, histories)):
        if len(history['val_exact_acc']) >= 3:
            early_acc = history['val_exact_acc'][2]  # ç¬¬3è½®çš„å‡†ç¡®ç‡
            if early_acc > 0.8:
                print(f"âš ï¸  {name} ä»ç„¶è¡¨ç°å‡ºå¼‚å¸¸å¿«é€Ÿçš„å­¦ä¹ ï¼Œå¯èƒ½è¿˜æœ‰éšè—çš„æ·å¾„")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ›¡ï¸ å¼€å§‹é²æ£’æ•°æ®é›†è®­ç»ƒå®éªŒ...")
    
    # è¿è¡Œè®­ç»ƒå®éªŒ
    histories, names, final_accuracies, experiments = run_robust_training_experiments()
    
    if not histories:
        print("âŒ æ²¡æœ‰æˆåŠŸå®Œæˆçš„å®éªŒ")
        return
    
    # ç»˜åˆ¶ç»“æœ
    plot_robust_training_results(histories, names, final_accuracies)
    
    # åˆ†æç»“æœ
    analyze_robust_results(names, final_accuracies, histories)
    
    print(f"\nğŸ‰ é²æ£’è®­ç»ƒå®éªŒå®Œæˆï¼")
    print(f"\nğŸ’¡ å¦‚æœç»“æœæ˜¾ç¤ºå¥åº·çš„å­¦ä¹ æ›²çº¿ï¼ˆé€æ­¥æå‡è€Œéç¬é—´è¾¾åˆ°100%ï¼‰ï¼Œ")
    print(f"   é‚£ä¹ˆæˆ‘ä»¬å°±æˆåŠŸåœ°å µæ­»äº†ä½œå¼Šæ·å¾„ï¼Œè¿«ä½¿æ¨¡å‹å­¦ä¹ çœŸæ­£çš„é€»è¾‘æ¨ç†ï¼")


if __name__ == "__main__":
    main()
