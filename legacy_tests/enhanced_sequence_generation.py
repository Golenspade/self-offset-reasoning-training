"""
å¢å¼ºçš„åºåˆ—ç”Ÿæˆæœºåˆ¶
åœ¨è§£å†³å¾ªç¯é—®é¢˜åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥å®Œå–„é€†å¦å‘½é¢˜ç”Ÿæˆ
"""

import numpy as np
from typing import List, Tuple, Dict
import sys
import os
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / 'src'))

from logic_transformer.data_utils import Tokenizer
from improved_sequence_generation import ImprovedSequenceGenerator


class EnhancedSequenceGenerator(ImprovedSequenceGenerator):
    """å¢å¼ºçš„åºåˆ—ç”Ÿæˆå™¨ï¼Œä¸“æ³¨äºå®Œæ•´é€†å¦å‘½é¢˜ç”Ÿæˆ"""
    
    def __init__(self, model, tokenizer: Tokenizer):
        super().__init__(model, tokenizer)
        
        # è°ƒæ•´å‚æ•°ä»¥é¼“åŠ±æ›´å®Œæ•´çš„ç”Ÿæˆ
        self.end_token_boost = 1.5      # é™ä½END_TOKENæå‡ï¼Œå…è®¸æ›´é•¿åºåˆ—
        self.max_length = 25            # å¢åŠ æœ€å¤§é•¿åº¦
        self.min_length = 7             # è®¾ç½®æœ€å°é•¿åº¦
        
    def apply_completeness_guidance(self, logits: np.ndarray, generated_tokens: List[int]) -> np.ndarray:
        """
        åº”ç”¨å®Œæ•´æ€§æŒ‡å¯¼
        é¼“åŠ±ç”Ÿæˆå®Œæ•´çš„é€†å¦å‘½é¢˜ç»“æ„
        """
        if len(generated_tokens) < 3:
            return logits
        
        # åˆ†æå½“å‰ç”Ÿæˆçš„å†…å®¹
        current_text = self.tokenizer.decode(generated_tokens)
        
        # å¦‚æœå·²ç»æœ‰ "~å˜é‡ ->" ä½†è¿˜æ²¡æœ‰ååŠéƒ¨åˆ†ï¼Œé¼“åŠ±ç”ŸæˆååŠéƒ¨åˆ†
        if '->' in current_text and current_text.count('->') == 1:
            # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰ç©ºæ ¼
            if current_text.endswith('->'):
                # é¼“åŠ±ç”Ÿæˆç©ºæ ¼
                logits[self.tokenizer.char_to_int[' ']] *= 3.0
            elif current_text.endswith('-> '):
                # é¼“åŠ±ç”Ÿæˆå¦å®šç¬¦æˆ–å˜é‡
                logits[self.tokenizer.char_to_int['~']] *= 2.5
                for token in [0, 1, 2, 3, 4]:  # p,q,r,s,t
                    if token < len(logits):
                        logits[token] *= 2.0
        
        return logits
    
    def detect_completion(self, generated_tokens: List[int]) -> bool:
        """
        å¢å¼ºçš„å®Œæˆæ£€æµ‹
        è¦æ±‚æ›´å®Œæ•´çš„é€†å¦å‘½é¢˜ç»“æ„
        """
        if len(generated_tokens) < self.min_length:
            return False
        
        text = self.tokenizer.decode(generated_tokens)
        
        # æ£€æŸ¥å®Œæ•´çš„é€†å¦å‘½é¢˜ç»“æ„: ~A -> ~B
        has_arrow = '->' in text
        negation_count = text.count('~')
        
        # ç†æƒ³æƒ…å†µï¼šæœ‰ç®­å¤´ï¼Œæœ‰è‡³å°‘ä¸€ä¸ªå¦å®šç¬¦
        if has_arrow and negation_count >= 1:
            # å¦‚æœç®­å¤´åé¢æœ‰å†…å®¹ï¼Œå¯ä»¥ç»“æŸ
            arrow_pos = text.find('->')
            if arrow_pos >= 0 and len(text) > arrow_pos + 2:
                after_arrow = text[arrow_pos + 2:].strip()
                if after_arrow:  # ç®­å¤´åæœ‰å†…å®¹
                    return True
        
        # å¼ºåˆ¶é•¿åº¦é™åˆ¶
        if len(generated_tokens) >= self.max_length:
            return True
        
        return False
    
    def apply_variable_consistency(self, logits: np.ndarray, generated_tokens: List[int], 
                                 input_sequence: List[int]) -> np.ndarray:
        """
        åº”ç”¨å˜é‡ä¸€è‡´æ€§æŒ‡å¯¼
        æ ¹æ®è¾“å…¥ä¸­çš„å˜é‡æ¥æŒ‡å¯¼è¾“å‡ºå˜é‡çš„é€‰æ‹©
        """
        if not generated_tokens:
            return logits
        
        # åˆ†æè¾“å…¥ä¸­çš„å˜é‡
        input_text = self.tokenizer.decode(input_sequence)
        input_variables = set()
        for var in ['p', 'q', 'r', 's', 't']:
            if var in input_text:
                input_variables.add(var)
        
        current_text = self.tokenizer.decode(generated_tokens)
        
        # å¦‚æœæ­£åœ¨ç”Ÿæˆç®­å¤´åçš„éƒ¨åˆ†ï¼Œä¼˜å…ˆä½¿ç”¨è¾“å…¥ä¸­çš„å˜é‡
        if '->' in current_text:
            arrow_pos = current_text.find('->')
            after_arrow = current_text[arrow_pos + 2:]
            
            # å¦‚æœç®­å¤´åéœ€è¦å˜é‡
            if after_arrow.endswith('~') or (after_arrow.strip() == ''):
                for var in input_variables:
                    var_token = self.tokenizer.char_to_int[var]
                    if var_token < len(logits):
                        logits[var_token] *= 2.0
        
        return logits
    
    def generate_sequence_enhanced(self, input_sequence: List[int], max_steps: int = 25) -> Tuple[List[int], str]:
        """
        å¢å¼ºçš„åºåˆ—ç”Ÿæˆ
        """
        # ç¼–ç è¾“å…¥
        encoded = self.model.encode(input_sequence)
        
        # åˆå§‹åŒ–
        generated_tokens = []
        current_token = self.tokenizer.START_TOKEN
        
        for step in range(max_steps):
            # è§£ç æ­¥éª¤
            hidden_state, raw_logits = self.model.decode_step(encoded, current_token)
            
            # è½¬æ¢ä¸ºæ¦‚ç‡
            logits = raw_logits.copy()
            
            # åº”ç”¨æ‰€æœ‰æƒ©ç½šå’ŒæŒ‡å¯¼æœºåˆ¶
            logits = self.apply_repetition_penalty(logits, generated_tokens)
            logits = self.apply_cycle_penalty(logits, generated_tokens)
            logits = self.apply_length_penalty(logits, len(generated_tokens))
            logits = self.apply_structural_guidance(logits, generated_tokens)
            logits = self.apply_completeness_guidance(logits, generated_tokens)
            logits = self.apply_variable_consistency(logits, generated_tokens, input_sequence)
            
            # æœ€ååº”ç”¨END_TOKENæå‡ï¼ˆåœ¨å…¶ä»–æŒ‡å¯¼ä¹‹åï¼‰
            logits = self.apply_end_token_boost(logits, generated_tokens)
            
            # é‡æ–°è®¡ç®—æ¦‚ç‡
            exp_logits = np.exp(logits - np.max(logits))
            probabilities = exp_logits / np.sum(exp_logits)
            
            # é€‰æ‹©ä¸‹ä¸€ä¸ªtoken
            next_token = int(np.argmax(probabilities))
            
            # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
            if next_token == self.tokenizer.END_TOKEN:
                break
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥å¼ºåˆ¶å®Œæˆ
            if self.detect_completion(generated_tokens):
                break
            
            # æ£€æŸ¥tokenæœ‰æ•ˆæ€§
            if next_token >= self.tokenizer.vocab_size or next_token < 0:
                break
            
            # æ·»åŠ åˆ°åºåˆ—
            generated_tokens.append(next_token)
            current_token = next_token
            
            # å¾ªç¯æ£€æµ‹
            if len(generated_tokens) >= 6:
                last_3 = generated_tokens[-3:]
                prev_3 = generated_tokens[-6:-3]
                if last_3 == prev_3:
                    break
        
        # è§£ç ç»“æœ
        decoded_text = self.tokenizer.decode(generated_tokens)
        
        return generated_tokens, decoded_text


def test_enhanced_generation():
    """æµ‹è¯•å¢å¼ºçš„ç”Ÿæˆæœºåˆ¶"""
    print("ğŸš€ æµ‹è¯•å¢å¼ºçš„åºåˆ—ç”Ÿæˆæœºåˆ¶")
    print("=" * 60)
    
    # åŠ è½½æ¨¡å‹å’Œtokenizer
    tokenizer = Tokenizer()
    
    from logic_transformer.models.base_model import ImprovedSimpleModel
    
    model = ImprovedSimpleModel(
        vocab_size=tokenizer.vocab_size,
        hidden_size=128,
        max_length=50,
        learning_rate=0.003
    )
    
    model_path = 'outputs/trained_models/robust_model_Level_1_é²æ£’ç‰ˆ.npz'
    if not model.load_model(model_path):
        print(f"âŒ æ— æ³•åŠ è½½æ¨¡å‹: {model_path}")
        return
    
    # åˆ›å»ºå¢å¼ºçš„ç”Ÿæˆå™¨
    generator = EnhancedSequenceGenerator(model, tokenizer)
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        ("p -> q", "~q -> ~p"),
        ("~p -> r", "~r -> p"),
        ("(p & q) -> s", "~s -> ~(p & q)"),
        ("r -> (p | q)", "~(p | q) -> ~r")
    ]
    
    print("å¯¹æ¯”æµ‹è¯•ï¼šåŸºç¡€ç”Ÿæˆå™¨ vs å¢å¼ºç”Ÿæˆå™¨")
    print("=" * 60)
    
    for test_input, expected in test_cases:
        print(f"\nğŸ§ª æµ‹è¯•è¾“å…¥: '{test_input}'")
        print(f"æœŸæœ›è¾“å‡º: '{expected}'")
        print("-" * 50)
        
        # ç¼–ç è¾“å…¥
        input_sequence = tokenizer.encode(test_input)
        
        # åŸºç¡€ç”Ÿæˆå™¨
        basic_tokens, basic_text = generator.generate_sequence(input_sequence)
        print(f"åŸºç¡€ç”Ÿæˆå™¨: '{basic_text}'")
        
        # å¢å¼ºç”Ÿæˆå™¨
        enhanced_tokens, enhanced_text = generator.generate_sequence_enhanced(input_sequence)
        print(f"å¢å¼ºç”Ÿæˆå™¨: '{enhanced_text}'")
        
        # åˆ†ææ”¹è¿›
        if len(enhanced_text) > len(basic_text):
            print("âœ… å¢å¼ºç‰ˆç”Ÿæˆäº†æ›´é•¿çš„åºåˆ—")
        if enhanced_text.count('~') > basic_text.count('~'):
            print("âœ… å¢å¼ºç‰ˆåŒ…å«æ›´å¤šå¦å®šç¬¦")
        if '-> ~' in enhanced_text and '-> ~' not in basic_text:
            print("âœ… å¢å¼ºç‰ˆç”Ÿæˆäº†æ›´å®Œæ•´çš„é€†å¦ç»“æ„")


def analyze_generation_quality():
    """åˆ†æç”Ÿæˆè´¨é‡"""
    print("\nğŸ“Š ç”Ÿæˆè´¨é‡åˆ†æ")
    print("=" * 40)
    
    print("æ”¹è¿›æ•ˆæœæ€»ç»“:")
    print("1. âœ… å®Œå…¨æ¶ˆé™¤äº† '-> -> ->' å¾ªç¯é—®é¢˜")
    print("2. âœ… ç”Ÿæˆäº†åŸºæœ¬çš„é€»è¾‘è¡¨è¾¾å¼ç»“æ„")
    print("3. ğŸ”„ æ­£åœ¨æ”¹è¿›å®Œæ•´æ€§ï¼ˆç”Ÿæˆå®Œæ•´çš„é€†å¦å‘½é¢˜ï¼‰")
    print("4. ğŸ¯ ä¸‹ä¸€æ­¥ï¼šä¼˜åŒ–å˜é‡é€‰æ‹©å’Œç»“æ„å®Œæ•´æ€§")
    
    print("\næƒ©ç½šæœºåˆ¶æ•ˆæœ:")
    print("- å¾ªç¯æ£€æµ‹æƒ©ç½š: ğŸ¯ å®Œå…¨æœ‰æ•ˆ")
    print("- ç»“æ„åŒ–æŒ‡å¯¼: ğŸ¯ æ˜¾è‘—æ”¹å–„")
    print("- é‡å¤æƒ©ç½š: ğŸ¯ æœ‰æ•ˆå‡å°‘é‡å¤")
    print("- å®Œæˆæ£€æµ‹: ğŸ”„ éœ€è¦è¿›ä¸€æ­¥è°ƒä¼˜")


if __name__ == "__main__":
    test_enhanced_generation()
    analyze_generation_quality()
