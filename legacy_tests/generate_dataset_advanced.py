"""
é«˜çº§æ•°æ®é›†ç”Ÿæˆè„šæœ¬
å®ç°é…ç½®æ–‡ä»¶é©±åŠ¨ã€è¯¾ç¨‹å­¦ä¹ å’Œå¤æ‚åº¦é€’å¢çš„æ•°æ®ç”Ÿæˆç­–ç•¥
"""

import json
import random
import os
from typing import Dict, List, Callable
from logic_utils import (
    generate_simple_proposition, 
    generate_complex_proposition,
    generate_recursive_implication,
    to_contrapositive, 
    add_noise,
    verify_equivalence
)


def load_config(config_path: str = 'configs/dataset_config.json') -> Dict:
    """åŠ è½½æ•°æ®é›†é…ç½®æ–‡ä»¶"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        print(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")
        return config
    except FileNotFoundError:
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        return None
    except json.JSONDecodeError as e:
        print(f"âŒ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
        return None


def get_generator_function(func_name: str) -> Callable:
    """æ ¹æ®å‡½æ•°åè·å–ç”Ÿæˆå™¨å‡½æ•°"""
    generators = {
        'generate_simple_proposition': generate_simple_proposition,
        'generate_complex_proposition': generate_complex_proposition,
        'generate_recursive_proposition': lambda: generate_recursive_implication(max_depth=3),
    }
    
    if func_name not in generators:
        raise ValueError(f"æœªçŸ¥çš„ç”Ÿæˆå™¨å‡½æ•°: {func_name}")
    
    return generators[func_name]


def generate_training_sample(generator_func: Callable, noise_types: List[str], 
                           num_applications: int = 1, **kwargs) -> Dict:
    """
    ç”Ÿæˆä¸€ä¸ªè®­ç»ƒæ ·æœ¬
    æ”¯æŒä¸åŒçš„ç”Ÿæˆå™¨å‡½æ•°å’Œå™ªå£°é…ç½®
    """
    # ç”ŸæˆåŸå§‹å‘½é¢˜
    if 'max_depth' in kwargs:
        # å¯¹äºé€’å½’ç”Ÿæˆå™¨ï¼Œä¼ é€’max_depthå‚æ•°
        original_prop = generate_recursive_implication(max_depth=kwargs['max_depth'])
    else:
        original_prop = generator_func()
    
    # ç”Ÿæˆé€†å¦å‘½é¢˜
    target_contrapositive = to_contrapositive(original_prop)
    
    # æ·»åŠ å™ªå£°
    noisy_prop = add_noise(original_prop, noise_types, num_applications)
    
    return {
        'original_prop': original_prop,
        'noisy_prop': noisy_prop,
        'target_contrapositive': target_contrapositive,
        'complexity': 'recursive' if 'max_depth' in kwargs else 'simple',
        'noise_applications': num_applications,
        'noise_types': noise_types
    }


def generate_dataset_from_config(dataset_config: Dict) -> List[Dict]:
    """
    æ ¹æ®é…ç½®ç”Ÿæˆæ•°æ®é›†
    å®ç°æ‚¨å»ºè®®çš„é…ç½®é©±åŠ¨æ–¹æ³•
    """
    print(f"ç”Ÿæˆæ•°æ®é›†: {dataset_config.get('description', 'æœªçŸ¥')}")
    
    # è·å–ç”Ÿæˆå™¨å‡½æ•°
    generator_func = get_generator_function(dataset_config['generator_func'])
    
    # æå–å‚æ•°
    num_samples = dataset_config['num_samples']
    noise_types = dataset_config['noise_types']
    num_applications = dataset_config.get('noise_applications', 1)
    max_depth = dataset_config.get('max_depth', 3)
    
    dataset = []
    successful_samples = 0
    attempts = 0
    max_attempts = num_samples * 3
    
    while successful_samples < num_samples and attempts < max_attempts:
        attempts += 1
        
        try:
            sample = generate_training_sample(
                generator_func=generator_func,
                noise_types=noise_types,
                num_applications=num_applications,
                max_depth=max_depth
            )
            
            # åŸºæœ¬éªŒè¯ï¼šç¡®ä¿ç”Ÿæˆçš„æ ·æœ¬ä¸ä¸ºç©ºä¸”ä¸ç›¸åŒ
            if (sample['noisy_prop'].strip() and 
                sample['target_contrapositive'].strip() and
                sample['noisy_prop'] != sample['target_contrapositive']):
                
                dataset.append(sample)
                successful_samples += 1
                
                # æ¯ç”Ÿæˆ1000ä¸ªæ ·æœ¬æ‰“å°è¿›åº¦
                if successful_samples % 1000 == 0:
                    print(f"  å·²ç”Ÿæˆ {successful_samples}/{num_samples} ä¸ªæ ·æœ¬...")
                    
        except Exception as e:
            # print(f"ç”Ÿæˆæ ·æœ¬æ—¶å‡ºé”™: {e}")
            continue
    
    print(f"  âœ… æˆåŠŸç”Ÿæˆ {successful_samples} ä¸ªæ ·æœ¬ï¼Œæ€»å°è¯•æ¬¡æ•°: {attempts}")
    return dataset


def save_dataset_optimized(dataset: List[Dict], filename: str):
    """
    ä¼˜åŒ–çš„æ•°æ®é›†ä¿å­˜å‡½æ•°
    å®ç°æ‚¨å»ºè®®çš„æ‰¹é‡å†™å…¥ä¼˜åŒ–
    """
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    
    # æ‰¹é‡å‡†å¤‡JSONå­—ç¬¦ä¸²
    json_lines = [json.dumps(sample, ensure_ascii=False) + '\n' for sample in dataset]
    
    # ä¸€æ¬¡æ€§å†™å…¥
    with open(filename, 'w', encoding='utf-8') as f:
        f.writelines(json_lines)
    
    print(f"  âœ… æ•°æ®é›†å·²ä¿å­˜åˆ° {filename}")


def analyze_dataset_advanced(dataset: List[Dict], name: str = "æ•°æ®é›†") -> Dict:
    """
    é«˜çº§æ•°æ®é›†åˆ†æ
    åŒ…å«å¤æ‚åº¦å’Œå™ªå£°ç±»å‹çš„è¯¦ç»†ç»Ÿè®¡
    """
    if not dataset:
        return {"error": "æ•°æ®é›†ä¸ºç©º"}
    
    total_samples = len(dataset)
    
    # å¤æ‚åº¦åˆ†å¸ƒ
    complexity_count = {}
    for sample in dataset:
        complexity = sample.get('complexity', 'unknown')
        complexity_count[complexity] = complexity_count.get(complexity, 0) + 1
    
    # å™ªå£°åº”ç”¨æ¬¡æ•°åˆ†å¸ƒ
    noise_app_count = {}
    for sample in dataset:
        apps = sample.get('noise_applications', 1)
        noise_app_count[apps] = noise_app_count.get(apps, 0) + 1
    
    # å™ªå£°ç±»å‹åˆ†å¸ƒ
    noise_type_count = {}
    for sample in dataset:
        types = sample.get('noise_types', ['type1'])
        for noise_type in types:
            noise_type_count[noise_type] = noise_type_count.get(noise_type, 0) + 1
    
    # é•¿åº¦åˆ†å¸ƒ
    input_lengths = [len(sample['noisy_prop']) for sample in dataset]
    target_lengths = [len(sample['target_contrapositive']) for sample in dataset]
    
    stats = {
        "name": name,
        "total_samples": total_samples,
        "complexity_distribution": complexity_count,
        "noise_applications_distribution": noise_app_count,
        "noise_types_distribution": noise_type_count,
        "avg_input_length": round(sum(input_lengths) / len(input_lengths), 2),
        "avg_target_length": round(sum(target_lengths) / len(target_lengths), 2),
        "max_input_length": max(input_lengths),
        "max_target_length": max(target_lengths)
    }
    
    print(f"\nğŸ“Š {name} è¯¦ç»†åˆ†æ:")
    print(f"  æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"  å¤æ‚åº¦åˆ†å¸ƒ: {complexity_count}")
    print(f"  å™ªå£°åº”ç”¨æ¬¡æ•°: {noise_app_count}")
    print(f"  å™ªå£°ç±»å‹åˆ†å¸ƒ: {noise_type_count}")
    print(f"  å¹³å‡è¾“å…¥é•¿åº¦: {stats['avg_input_length']}")
    print(f"  å¹³å‡ç›®æ ‡é•¿åº¦: {stats['avg_target_length']}")
    
    return stats


def show_sample_examples(dataset: List[Dict], num_examples: int = 3):
    """æ˜¾ç¤ºæ•°æ®é›†æ ·æœ¬ç¤ºä¾‹"""
    print(f"\nğŸ“ æ ·æœ¬ç¤ºä¾‹ (å‰{num_examples}ä¸ª):")
    
    for i, sample in enumerate(dataset[:num_examples]):
        print(f"\n  æ ·æœ¬ {i+1}:")
        print(f"    åŸå§‹å‘½é¢˜: {sample['original_prop']}")
        print(f"    å™ªå£°å‘½é¢˜: {sample['noisy_prop']}")
        print(f"    ç›®æ ‡è¾“å‡º: {sample['target_contrapositive']}")
        print(f"    å¤æ‚åº¦: {sample.get('complexity', 'unknown')}")
        print(f"    å™ªå£°æ¬¡æ•°: {sample.get('noise_applications', 1)}")


def generate_curriculum_datasets(config: Dict):
    """
    ç”Ÿæˆè¯¾ç¨‹å­¦ä¹ æ•°æ®é›†
    å®ç°æ‚¨å»ºè®®çš„ä»æ˜“åˆ°éš¾çš„å­¦ä¹ ç­–ç•¥
    """
    print("ğŸ“ å¼€å§‹ç”Ÿæˆè¯¾ç¨‹å­¦ä¹ æ•°æ®é›†...")
    print("=" * 60)
    
    datasets_config = config['datasets']
    curriculum_config = config.get('curriculum_learning', {})
    
    if curriculum_config.get('enabled', False):
        print("ğŸ“š è¯¾ç¨‹å­¦ä¹ æ¨¡å¼å·²å¯ç”¨")
        stages = curriculum_config.get('stages', [])
        
        for stage in stages:
            print(f"\nğŸ¯ {stage['name']}:")
            for dataset_name in stage['datasets']:
                if dataset_name in datasets_config:
                    dataset_config = datasets_config[dataset_name]
                    dataset = generate_dataset_from_config(dataset_config)
                    save_dataset_optimized(dataset, dataset_config['output_file'])
                    analyze_dataset_advanced(dataset, dataset_name)
                    show_sample_examples(dataset, 2)
    else:
        print("ğŸ“– æ ‡å‡†æ¨¡å¼ï¼šç”Ÿæˆæ‰€æœ‰æ•°æ®é›†")
        for dataset_name, dataset_config in datasets_config.items():
            print(f"\nğŸ“ ç”Ÿæˆ {dataset_name}:")
            dataset = generate_dataset_from_config(dataset_config)
            save_dataset_optimized(dataset, dataset_config['output_file'])
            analyze_dataset_advanced(dataset, dataset_name)


def main():
    """
    ä¸»å‡½æ•°ï¼šé…ç½®é©±åŠ¨çš„æ•°æ®ç”Ÿæˆ
    å®ç°æ‚¨å»ºè®®çš„æ‰€æœ‰ä¼˜åŒ–
    """
    print("ğŸš€ é«˜çº§è‡ªåç§»æ¨ç†è®­ç»ƒæ•°æ®é›†ç”Ÿæˆå™¨")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡ç°æ€§
    random.seed(42)
    
    # åŠ è½½é…ç½®
    config = load_config()
    if config is None:
        print("âŒ æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ï¼Œé€€å‡ºç¨‹åº")
        return
    
    # ç”Ÿæˆæ•°æ®é›†
    generate_curriculum_datasets(config)
    
    print("\nğŸ‰ æ‰€æœ‰æ•°æ®é›†ç”Ÿæˆå®Œæˆï¼")
    print("\nğŸ“‹ ç”Ÿæˆçš„æ–‡ä»¶:")
    for dataset_name, dataset_config in config['datasets'].items():
        output_file = dataset_config['output_file']
        if os.path.exists(output_file):
            file_size = os.path.getsize(output_file) / 1024 / 1024  # MB
            print(f"  âœ… {output_file} ({file_size:.2f} MB)")


if __name__ == "__main__":
    main()
