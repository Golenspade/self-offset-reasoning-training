"""
æ–‡ä»¶å: train_cuda.py
CUDAåŠ é€Ÿè®­ç»ƒä¸»è„šæœ¬
æ”¯æŒGPUåŠ é€Ÿçš„è‡ªåç§»æ¨ç†è®­ç»ƒ
"""
import os
import sys
import json
import time
import logging
import argparse
from pathlib import Path
from typing import Dict, List

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'src'))

from cuda_training_system import CUDABreakthroughTraining
from cuda_utils import CUDAManager, print_cuda_summary


def setup_logging(log_level: str = "INFO", log_file: str = None):
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    
    handlers = [logging.StreamHandler()]
    
    if log_file:
        os.makedirs(os.path.dirname(log_file), exist_ok=True)
        handlers.append(logging.FileHandler(log_file, encoding='utf-8'))
    
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format=log_format,
        handlers=handlers
    )


def load_training_data(data_dir: str = "data") -> tuple:
    """
    åŠ è½½è®­ç»ƒæ•°æ®
    
    Args:
        data_dir: æ•°æ®ç›®å½•
        
    Returns:
        (train_data, val_data): è®­ç»ƒå’ŒéªŒè¯æ•°æ®
    """
    # å°è¯•åŠ è½½ä¸åŒçº§åˆ«çš„æ•°æ®
    data_files = [
        ("train_level_3_é²æ£’ç‰ˆ.json", "val_level_3_é²æ£’ç‰ˆ.json"),
        ("train_level_2_é²æ£’ç‰ˆ.json", "val_level_2_é²æ£’ç‰ˆ.json"),
        ("train_level_1_é²æ£’ç‰ˆ.json", "val_level_1_é²æ£’ç‰ˆ.json"),
        ("train_data.json", "val_data.json")
    ]
    
    for train_file, val_file in data_files:
        train_path = os.path.join(data_dir, train_file)
        val_path = os.path.join(data_dir, val_file)
        
        if os.path.exists(train_path) and os.path.exists(val_path):
            print(f"ğŸ“Š åŠ è½½æ•°æ®æ–‡ä»¶: {train_file}, {val_file}")
            
            with open(train_path, 'r', encoding='utf-8') as f:
                train_data = json.load(f)
            
            with open(val_path, 'r', encoding='utf-8') as f:
                val_data = json.load(f)
            
            return train_data, val_data
    
    raise FileNotFoundError(f"åœ¨ {data_dir} ç›®å½•ä¸­æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶")


def create_training_config(args) -> Dict:
    """
    åˆ›å»ºè®­ç»ƒé…ç½®
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
        
    Returns:
        è®­ç»ƒé…ç½®å­—å…¸
    """
    # åŸºç¡€é…ç½®
    config = {
        # æ¨¡å‹å‚æ•°
        'hidden_size': args.hidden_size,
        'num_heads': args.num_heads,
        'num_encoder_layers': args.num_encoder_layers,
        'num_decoder_layers': args.num_decoder_layers,
        'dim_feedforward': args.dim_feedforward,
        'max_length': args.max_length,
        
        # è®­ç»ƒå‚æ•°
        'batch_size': args.batch_size,
        'learning_rate': args.learning_rate,
        'weight_decay': args.weight_decay,
        'epochs': args.epochs,
        'gradient_accumulation_steps': args.gradient_accumulation_steps,
        'max_grad_norm': args.max_grad_norm,
        
        # CUDAå‚æ•°
        'use_mixed_precision': args.use_mixed_precision,
        'gpu_memory_fraction': args.gpu_memory_fraction,
        
        # è°ƒåº¦å™¨å‚æ•°
        'lr_decay_factor': args.lr_decay_factor,
        'lr_patience': args.lr_patience,
        
        # æ—©åœå‚æ•°
        'early_stopping_patience': args.early_stopping_patience,
        
        # ä¿å­˜å‚æ•°
        'save_frequency': args.save_frequency,
        'log_frequency': args.log_frequency,
        
        # æ­£åˆ™åŒ–
        'label_smoothing': args.label_smoothing
    }
    
    return config


def optimize_batch_size(cuda_manager: CUDAManager, initial_batch_size: int, 
                       model_size_estimate: float = 0.1) -> int:
    """
    æ ¹æ®GPUå†…å­˜è‡ªåŠ¨ä¼˜åŒ–æ‰¹æ¬¡å¤§å°
    
    Args:
        cuda_manager: CUDAç®¡ç†å™¨
        initial_batch_size: åˆå§‹æ‰¹æ¬¡å¤§å°
        model_size_estimate: æ¨¡å‹å¤§å°ä¼°è®¡(GB)
        
    Returns:
        ä¼˜åŒ–åçš„æ‰¹æ¬¡å¤§å°
    """
    if cuda_manager.device.type != 'cuda':
        return initial_batch_size
    
    optimal_batch_size = cuda_manager.get_optimal_batch_size(
        model_memory_gb=model_size_estimate,
        max_batch_size=initial_batch_size * 2
    )
    
    # ç¡®ä¿æ‰¹æ¬¡å¤§å°æ˜¯åˆç†çš„
    optimal_batch_size = max(4, min(optimal_batch_size, 128))
    
    if optimal_batch_size != initial_batch_size:
        print(f"ğŸ¯ æ‰¹æ¬¡å¤§å°ä¼˜åŒ–: {initial_batch_size} -> {optimal_batch_size}")
    
    return optimal_batch_size


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="CUDAåŠ é€Ÿçš„è‡ªåç§»æ¨ç†è®­ç»ƒ")
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--data-dir', type=str, default='data', help='æ•°æ®ç›®å½•')
    parser.add_argument('--output-dir', type=str, default='outputs/cuda_training', help='è¾“å‡ºç›®å½•')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--hidden-size', type=int, default=256, help='éšè—å±‚å¤§å°')
    parser.add_argument('--num-heads', type=int, default=8, help='æ³¨æ„åŠ›å¤´æ•°')
    parser.add_argument('--num-encoder-layers', type=int, default=4, help='ç¼–ç å™¨å±‚æ•°')
    parser.add_argument('--num-decoder-layers', type=int, default=4, help='è§£ç å™¨å±‚æ•°')
    parser.add_argument('--dim-feedforward', type=int, default=1024, help='å‰é¦ˆç½‘ç»œç»´åº¦')
    parser.add_argument('--max-length', type=int, default=128, help='æœ€å¤§åºåˆ—é•¿åº¦')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--batch-size', type=int, default=32, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--learning-rate', type=float, default=0.001, help='å­¦ä¹ ç‡')
    parser.add_argument('--weight-decay', type=float, default=1e-5, help='æƒé‡è¡°å‡')
    parser.add_argument('--epochs', type=int, default=100, help='è®­ç»ƒè½®æ¬¡')
    parser.add_argument('--gradient-accumulation-steps', type=int, default=1, help='æ¢¯åº¦ç´¯ç§¯æ­¥æ•°')
    parser.add_argument('--max-grad-norm', type=float, default=1.0, help='æ¢¯åº¦è£å‰ªé˜ˆå€¼')
    
    # CUDAå‚æ•°
    parser.add_argument('--use-mixed-precision', action='store_true', default=True, help='ä½¿ç”¨æ··åˆç²¾åº¦')
    parser.add_argument('--gpu-memory-fraction', type=float, default=0.8, help='GPUå†…å­˜ä½¿ç”¨æ¯”ä¾‹')
    parser.add_argument('--auto-batch-size', action='store_true', help='è‡ªåŠ¨ä¼˜åŒ–æ‰¹æ¬¡å¤§å°')
    
    # è°ƒåº¦å™¨å‚æ•°
    parser.add_argument('--lr-decay-factor', type=float, default=0.5, help='å­¦ä¹ ç‡è¡°å‡å› å­')
    parser.add_argument('--lr-patience', type=int, default=3, help='å­¦ä¹ ç‡è°ƒåº¦å™¨è€å¿ƒå€¼')
    
    # æ—©åœå‚æ•°
    parser.add_argument('--early-stopping-patience', type=int, default=15, help='æ—©åœè€å¿ƒå€¼')
    
    # ä¿å­˜å’Œæ—¥å¿—å‚æ•°
    parser.add_argument('--save-frequency', type=int, default=10, help='ä¿å­˜é¢‘ç‡')
    parser.add_argument('--log-frequency', type=int, default=50, help='æ—¥å¿—é¢‘ç‡')
    parser.add_argument('--log-level', type=str, default='INFO', help='æ—¥å¿—çº§åˆ«')
    
    # æ­£åˆ™åŒ–å‚æ•°
    parser.add_argument('--label-smoothing', type=float, default=0.1, help='æ ‡ç­¾å¹³æ»‘')
    
    # æ¢å¤è®­ç»ƒ
    parser.add_argument('--resume', type=str, help='æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    log_file = os.path.join(args.output_dir, 'training.log')
    setup_logging(args.log_level, log_file)
    
    logger = logging.getLogger(__name__)
    
    print("ğŸš€ å¯åŠ¨CUDAåŠ é€Ÿè®­ç»ƒç³»ç»Ÿ")
    print("=" * 60)
    
    # æ£€æŸ¥CUDAç¯å¢ƒ
    print_cuda_summary()
    
    # åˆ›å»ºCUDAç®¡ç†å™¨
    cuda_manager = CUDAManager(
        memory_fraction=args.gpu_memory_fraction,
        auto_optimize=True
    )
    
    print(f"\nğŸ“ ä½¿ç”¨è®¾å¤‡: {cuda_manager.device}")
    
    # åŠ è½½æ•°æ®
    print(f"\nğŸ“Š åŠ è½½è®­ç»ƒæ•°æ®...")
    try:
        train_data, val_data = load_training_data(args.data_dir)
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ:")
        print(f"  è®­ç»ƒæ ·æœ¬: {len(train_data):,}")
        print(f"  éªŒè¯æ ·æœ¬: {len(val_data):,}")
    except FileNotFoundError as e:
        print(f"âŒ {e}")
        return 1
    
    # åˆ›å»ºè®­ç»ƒé…ç½®
    config = create_training_config(args)
    
    # è‡ªåŠ¨ä¼˜åŒ–æ‰¹æ¬¡å¤§å°
    if args.auto_batch_size:
        config['batch_size'] = optimize_batch_size(
            cuda_manager, 
            config['batch_size'],
            model_size_estimate=config['hidden_size'] * config['num_encoder_layers'] / 1000
        )
    
    print(f"\nğŸ”§ è®­ç»ƒé…ç½®:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # ä¿å­˜é…ç½®
    config_path = os.path.join(args.output_dir, 'training_config.json')
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    # åˆ›å»ºè®­ç»ƒç³»ç»Ÿ
    print(f"\nğŸ—ï¸ åˆå§‹åŒ–CUDAè®­ç»ƒç³»ç»Ÿ...")
    trainer = CUDABreakthroughTraining(config)
    
    # æ¢å¤è®­ç»ƒï¼ˆå¦‚æœæŒ‡å®šï¼‰
    start_epoch = 0
    if args.resume and os.path.exists(args.resume):
        print(f"ğŸ“‚ æ¢å¤è®­ç»ƒ: {args.resume}")
        start_epoch = trainer.load_checkpoint(args.resume)
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\nğŸ¯ å¼€å§‹CUDAåŠ é€Ÿè®­ç»ƒ...")
    print(f"èµ·å§‹epoch: {start_epoch}")
    
    try:
        # è®°å½•å¼€å§‹æ—¶é—´
        training_start_time = time.time()
        
        # è¿è¡Œè®­ç»ƒ
        results = trainer.run_cuda_training(
            train_data=train_data,
            val_data=val_data,
            output_dir=args.output_dir
        )
        
        # è®¡ç®—æ€»æ—¶é—´
        total_time = time.time() - training_start_time
        
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“Š æœ€ç»ˆç»“æœ:")
        print(f"  æœ€ä½³éªŒè¯æŸå¤±: {results['best_val_loss']:.4f}")
        print(f"  æ€»è®­ç»ƒè½®æ¬¡: {results['total_epochs']}")
        print(f"  æ€»è€—æ—¶: {total_time:.2f}s ({total_time/3600:.2f}h)")
        print(f"  æœ€ç»ˆæ¨¡å‹: {results['final_model_path']}")
        print(f"  è®­ç»ƒå†å²: {results['history_path']}")
        
        return 0
        
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        return 1
        
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
