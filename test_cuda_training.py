"""
æ–‡ä»¶å: test_cuda_training.py
CUDAè®­ç»ƒç³»ç»Ÿæµ‹è¯•è„šæœ¬
éªŒè¯GPUåŠ é€ŸåŠŸèƒ½å’Œæ€§èƒ½
"""
import os
import sys
import json
import time
import tempfile
import logging
from pathlib import Path
from typing import Dict, List

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'src'))

try:
    import torch
    from cuda_utils import CUDAManager, print_cuda_summary
    from cuda_training_system import CUDABreakthroughTraining
    from model import create_cuda_model
    CUDA_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ CUDAç›¸å…³æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    CUDA_AVAILABLE = False


class CUDATrainingTester:
    """CUDAè®­ç»ƒç³»ç»Ÿæµ‹è¯•å™¨"""
    
    def __init__(self):
        self.test_dir = None
        self.logger = self._setup_logging()
        self.test_results = {}
    
    def _setup_logging(self):
        """è®¾ç½®æµ‹è¯•æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        return logging.getLogger(__name__)
    
    def test_cuda_environment(self) -> bool:
        """æµ‹è¯•CUDAç¯å¢ƒ"""
        self.logger.info("ğŸ” æµ‹è¯•CUDAç¯å¢ƒ...")
        
        try:
            if not CUDA_AVAILABLE:
                self.logger.warning("âš ï¸ CUDAæ¨¡å—ä¸å¯ç”¨")
                self.test_results['cuda_environment'] = False
                return False
            
            # æ£€æŸ¥PyTorch CUDAæ”¯æŒ
            assert torch.cuda.is_available(), "CUDAä¸å¯ç”¨"
            
            device_count = torch.cuda.device_count()
            assert device_count > 0, "æœªæ£€æµ‹åˆ°GPUè®¾å¤‡"
            
            # æµ‹è¯•åŸºæœ¬GPUæ“ä½œ
            device = torch.device('cuda:0')
            test_tensor = torch.randn(100, 100, device=device)
            result = torch.matmul(test_tensor, test_tensor.T)
            
            assert result.device == device, "GPUè®¡ç®—å¤±è´¥"
            
            self.logger.info(f"âœ… CUDAç¯å¢ƒæµ‹è¯•é€šè¿‡ ({device_count} GPU)")
            self.test_results['cuda_environment'] = True
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ CUDAç¯å¢ƒæµ‹è¯•å¤±è´¥: {e}")
            self.test_results['cuda_environment'] = False
            return False
    
    def test_cuda_manager(self) -> bool:
        """æµ‹è¯•CUDAç®¡ç†å™¨"""
        self.logger.info("ğŸ”§ æµ‹è¯•CUDAç®¡ç†å™¨...")
        
        try:
            # åˆ›å»ºCUDAç®¡ç†å™¨
            cuda_manager = CUDAManager()
            
            # æµ‹è¯•è®¾å¤‡é€‰æ‹©
            assert cuda_manager.device is not None, "è®¾å¤‡é€‰æ‹©å¤±è´¥"
            
            # æµ‹è¯•å†…å­˜ä¿¡æ¯è·å–
            memory_info = cuda_manager.get_memory_info()
            if cuda_manager.device.type == 'cuda':
                assert 'total_memory' in memory_info, "å†…å­˜ä¿¡æ¯è·å–å¤±è´¥"
                assert memory_info['total_memory'] > 0, "GPUå†…å­˜ä¿¡æ¯æ— æ•ˆ"
            
            # æµ‹è¯•å†…å­˜ç›‘æ§
            with cuda_manager.memory_monitor("æµ‹è¯•æ“ä½œ"):
                if cuda_manager.device.type == 'cuda':
                    test_tensor = torch.randn(1000, 1000, device=cuda_manager.device)
                    del test_tensor
            
            # æµ‹è¯•ç¼“å­˜æ¸…ç†
            cuda_manager.clear_cache()
            
            self.logger.info("âœ… CUDAç®¡ç†å™¨æµ‹è¯•é€šè¿‡")
            self.test_results['cuda_manager'] = True
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ CUDAç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
            self.test_results['cuda_manager'] = False
            return False
    
    def test_cuda_model(self) -> bool:
        """æµ‹è¯•CUDAæ¨¡å‹"""
        self.logger.info("ğŸ¤– æµ‹è¯•CUDAæ¨¡å‹...")
        
        try:
            # åˆ›å»ºCUDAæ¨¡å‹
            model, device = create_cuda_model(
                vocab_size=1000,
                device='auto',
                d_model=64,
                nhead=4,
                num_encoder_layers=2,
                num_decoder_layers=2
            )
            
            # éªŒè¯æ¨¡å‹åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            for param in model.parameters():
                assert param.device == device, f"æ¨¡å‹å‚æ•°è®¾å¤‡ä¸åŒ¹é…: {param.device} vs {device}"
            
            # æµ‹è¯•å‰å‘ä¼ æ’­
            batch_size = 4
            seq_len = 10
            
            src = torch.randint(0, 1000, (seq_len, batch_size), device=device)
            tgt = torch.randint(0, 1000, (seq_len, batch_size), device=device)
            
            with torch.no_grad():
                output = model(src, tgt[:-1])
                assert output.device == device, "æ¨¡å‹è¾“å‡ºè®¾å¤‡ä¸åŒ¹é…"
                assert output.shape == (seq_len-1, batch_size, 1000), f"è¾“å‡ºå½¢çŠ¶é”™è¯¯: {output.shape}"
            
            self.logger.info("âœ… CUDAæ¨¡å‹æµ‹è¯•é€šè¿‡")
            self.test_results['cuda_model'] = True
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ CUDAæ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
            self.test_results['cuda_model'] = False
            return False
    
    def test_mixed_precision(self) -> bool:
        """æµ‹è¯•æ··åˆç²¾åº¦è®­ç»ƒ"""
        self.logger.info("ğŸ”¥ æµ‹è¯•æ··åˆç²¾åº¦è®­ç»ƒ...")
        
        try:
            from torch.cuda.amp import GradScaler, autocast
            
            # åˆ›å»ºç®€å•æ¨¡å‹
            model, device = create_cuda_model(
                vocab_size=100,
                device='auto',
                d_model=32,
                nhead=2,
                num_encoder_layers=1,
                num_decoder_layers=1
            )
            
            if device.type != 'cuda':
                self.logger.info("â„¹ï¸ CPUæ¨¡å¼ï¼Œè·³è¿‡æ··åˆç²¾åº¦æµ‹è¯•")
                self.test_results['mixed_precision'] = True
                return True
            
            # æ£€æŸ¥æ˜¯å¦æ”¯æŒæ··åˆç²¾åº¦
            cuda_manager = CUDAManager()
            if not cuda_manager.supports_mixed_precision():
                self.logger.info("â„¹ï¸ GPUä¸æ”¯æŒæ··åˆç²¾åº¦ï¼Œè·³è¿‡æµ‹è¯•")
                self.test_results['mixed_precision'] = True
                return True
            
            # åˆ›å»ºä¼˜åŒ–å™¨å’Œscaler
            optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
            scaler = GradScaler()
            criterion = torch.nn.CrossEntropyLoss()
            
            # æµ‹è¯•æ··åˆç²¾åº¦è®­ç»ƒæ­¥éª¤
            model.train()
            
            src = torch.randint(0, 100, (5, 2), device=device)
            tgt = torch.randint(0, 100, (5, 2), device=device)
            
            optimizer.zero_grad()
            
            with autocast():
                output = model(src, tgt[:-1])
                loss = criterion(output.reshape(-1, 100), tgt[1:].reshape(-1))
            
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            assert not torch.isnan(loss), "æ··åˆç²¾åº¦è®­ç»ƒäº§ç”ŸNaN"
            
            self.logger.info("âœ… æ··åˆç²¾åº¦è®­ç»ƒæµ‹è¯•é€šè¿‡")
            self.test_results['mixed_precision'] = True
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ··åˆç²¾åº¦è®­ç»ƒæµ‹è¯•å¤±è´¥: {e}")
            self.test_results['mixed_precision'] = False
            return False
    
    def test_cuda_training_system(self) -> bool:
        """æµ‹è¯•CUDAè®­ç»ƒç³»ç»Ÿ"""
        self.logger.info("ğŸš€ æµ‹è¯•CUDAè®­ç»ƒç³»ç»Ÿ...")
        
        try:
            # åˆ›å»ºæµ‹è¯•é…ç½®
            config = {
                'hidden_size': 64,
                'num_heads': 4,
                'num_encoder_layers': 2,
                'num_decoder_layers': 2,
                'dim_feedforward': 128,
                'max_length': 20,
                'batch_size': 4,
                'learning_rate': 0.01,
                'weight_decay': 1e-5,
                'use_mixed_precision': True,
                'gradient_accumulation_steps': 1,
                'max_grad_norm': 1.0,
                'early_stopping_patience': 5,
                'epochs': 3
            }
            
            # åˆ›å»ºè®­ç»ƒç³»ç»Ÿ
            trainer = CUDABreakthroughTraining(config)
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            test_data = []
            for i in range(20):
                test_data.append({
                    'noisy_prop': f"p{i} -> q{i}",
                    'target_contrapositive': f"~q{i} -> ~p{i}",
                    'complexity': 'simple'
                })
            
            # æµ‹è¯•æ‰¹æ¬¡å‡†å¤‡
            batch = test_data[:4]
            src_batch, tgt_input, tgt_output = trainer.prepare_batch_cuda(batch)
            
            assert src_batch is not None, "æ‰¹æ¬¡å‡†å¤‡å¤±è´¥"
            assert src_batch.device == trainer.device, "æ‰¹æ¬¡æ•°æ®è®¾å¤‡ä¸åŒ¹é…"
            
            # æµ‹è¯•è®­ç»ƒæ­¥éª¤
            metrics = trainer.train_step_cuda(batch)
            
            assert 'loss' in metrics, "è®­ç»ƒæ­¥éª¤æœªè¿”å›æŸå¤±"
            assert not torch.isnan(torch.tensor(metrics['loss'])), "è®­ç»ƒæŸå¤±ä¸ºNaN"
            
            # æµ‹è¯•éªŒè¯
            val_metrics = trainer.validate_cuda(test_data[:10])
            assert 'val_loss' in val_metrics, "éªŒè¯æœªè¿”å›æŸå¤±"
            
            self.logger.info("âœ… CUDAè®­ç»ƒç³»ç»Ÿæµ‹è¯•é€šè¿‡")
            self.test_results['cuda_training_system'] = True
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ CUDAè®­ç»ƒç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}")
            self.test_results['cuda_training_system'] = False
            return False
    
    def test_performance_comparison(self) -> bool:
        """æµ‹è¯•æ€§èƒ½å¯¹æ¯”"""
        self.logger.info("âš¡ æµ‹è¯•æ€§èƒ½å¯¹æ¯”...")
        
        try:
            if not torch.cuda.is_available():
                self.logger.info("â„¹ï¸ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡æ€§èƒ½å¯¹æ¯”")
                self.test_results['performance_comparison'] = True
                return True
            
            # åˆ›å»ºæµ‹è¯•æ¨¡å‹
            vocab_size = 1000
            batch_size = 16
            seq_len = 50
            
            # CPUæ¨¡å‹
            cpu_model, _ = create_cuda_model(vocab_size, device='cpu', d_model=128)
            
            # GPUæ¨¡å‹
            gpu_model, _ = create_cuda_model(vocab_size, device='cuda', d_model=128)
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            cpu_src = torch.randint(0, vocab_size, (seq_len, batch_size))
            cpu_tgt = torch.randint(0, vocab_size, (seq_len, batch_size))
            
            gpu_src = cpu_src.cuda()
            gpu_tgt = cpu_tgt.cuda()
            
            # CPUæ€§èƒ½æµ‹è¯•
            cpu_model.eval()
            with torch.no_grad():
                cpu_start = time.time()
                for _ in range(10):
                    _ = cpu_model(cpu_src, cpu_tgt[:-1])
                cpu_time = time.time() - cpu_start
            
            # GPUæ€§èƒ½æµ‹è¯•
            gpu_model.eval()
            with torch.no_grad():
                # é¢„çƒ­
                _ = gpu_model(gpu_src, gpu_tgt[:-1])
                torch.cuda.synchronize()
                
                gpu_start = time.time()
                for _ in range(10):
                    _ = gpu_model(gpu_src, gpu_tgt[:-1])
                torch.cuda.synchronize()
                gpu_time = time.time() - gpu_start
            
            speedup = cpu_time / gpu_time
            
            self.logger.info(f"ğŸ“Š æ€§èƒ½å¯¹æ¯”ç»“æœ:")
            self.logger.info(f"  CPUæ—¶é—´: {cpu_time:.3f}s")
            self.logger.info(f"  GPUæ—¶é—´: {gpu_time:.3f}s")
            self.logger.info(f"  åŠ é€Ÿæ¯”: {speedup:.2f}x")
            
            # GPUåº”è¯¥æ¯”CPUå¿«ï¼ˆè‡³å°‘ä¸èƒ½æ…¢å¤ªå¤šï¼‰
            assert speedup > 0.5, f"GPUæ€§èƒ½å¼‚å¸¸ï¼ŒåŠ é€Ÿæ¯”ä»…ä¸º{speedup:.2f}x"
            
            self.test_results['performance_comparison'] = True
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ€§èƒ½å¯¹æ¯”æµ‹è¯•å¤±è´¥: {e}")
            self.test_results['performance_comparison'] = False
            return False
    
    def run_all_tests(self) -> Dict[str, bool]:
        """è¿è¡Œæ‰€æœ‰CUDAæµ‹è¯•"""
        self.logger.info("ğŸ§ª å¼€å§‹CUDAè®­ç»ƒç³»ç»Ÿå®Œæ•´æµ‹è¯•")
        self.logger.info("=" * 60)
        
        # é¦–å…ˆæ‰“å°CUDAç¯å¢ƒä¿¡æ¯
        print_cuda_summary()
        
        # è¿è¡Œæµ‹è¯•
        tests = [
            ('CUDAç¯å¢ƒ', self.test_cuda_environment),
            ('CUDAç®¡ç†å™¨', self.test_cuda_manager),
            ('CUDAæ¨¡å‹', self.test_cuda_model),
            ('æ··åˆç²¾åº¦', self.test_mixed_precision),
            ('CUDAè®­ç»ƒç³»ç»Ÿ', self.test_cuda_training_system),
            ('æ€§èƒ½å¯¹æ¯”', self.test_performance_comparison)
        ]
        
        for test_name, test_func in tests:
            self.logger.info(f"\nğŸ” æµ‹è¯•: {test_name}")
            test_func()
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        self._generate_test_report()
        
        return self.test_results
    
    def _generate_test_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        self.logger.info("\n" + "=" * 60)
        self.logger.info("ğŸ“Š CUDAæµ‹è¯•æŠ¥å‘Š")
        self.logger.info("=" * 60)
        
        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results.values() if result)
        
        for test_name, result in self.test_results.items():
            status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
            self.logger.info(f"{test_name}: {status}")
        
        self.logger.info("-" * 60)
        self.logger.info(f"æ€»è®¡: {passed_tests}/{total_tests} æµ‹è¯•é€šè¿‡")
        
        if passed_tests == total_tests:
            self.logger.info("ğŸ‰ æ‰€æœ‰CUDAæµ‹è¯•é€šè¿‡ï¼GPUåŠ é€Ÿè®­ç»ƒç³»ç»Ÿå‡†å¤‡å°±ç»ª")
        else:
            self.logger.warning("âš ï¸ éƒ¨åˆ†CUDAæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥GPUç¯å¢ƒå’Œé©±åŠ¨")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª CUDAè®­ç»ƒç³»ç»Ÿæµ‹è¯•å·¥å…·")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = CUDATrainingTester()
    
    # è¿è¡Œæµ‹è¯•
    results = tester.run_all_tests()
    
    # è¿”å›ç»“æœ
    all_passed = all(results.values())
    exit_code = 0 if all_passed else 1
    
    print(f"\nğŸ CUDAæµ‹è¯•å®Œæˆï¼Œé€€å‡ºç : {exit_code}")
    return exit_code


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
