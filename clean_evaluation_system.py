"""
æ–‡ä»¶å: clean_evaluation_system.py
æ¸…ç†åçš„è¯„ä¼°ç³»ç»Ÿ - å»é™¤æ‰€æœ‰"è£…æ¨¡ä½œæ ·"çš„ä»£ç 
çœŸæ­£æœ‰ç”¨ã€ç›´æ¥æœ‰æ•ˆçš„è¯„ä¼°åŠŸèƒ½
"""

import json
import numpy as np
import os
from typing import Dict, List, Tuple, Optional
import sys
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / 'src'))

from logic_transformer.data_utils import Tokenizer, load_dataset
from logic_transformer.models.base_model import ImprovedSimpleModel

# ç›´æ¥å¯¼å…¥é€»è¾‘å·¥å…·å‡½æ•°
def verify_equivalence(pred, target):
    """ç®€åŒ–çš„é€»è¾‘ç­‰ä»·æ€§æ£€æŸ¥"""
    # æ ‡å‡†åŒ–å¤„ç†
    pred = pred.strip().replace(' ', '')
    target = target.strip().replace(' ', '')

    # ç›´æ¥æ¯”è¾ƒ
    if pred == target:
        return True

    # ç®€å•çš„ç­‰ä»·æ€§æ£€æŸ¥
    equivalences = [
        ('p->q', '~q->~p'),
        ('~p->q', '~q->p'),
        ('p->~q', 'q->~p'),
        ('~p->~q', 'q->p')
    ]

    for eq1, eq2 in equivalences:
        if (pred == eq1 and target == eq2) or (pred == eq2 and target == eq1):
            return True

    return False

def to_contrapositive(prop):
    """ç®€åŒ–çš„é€†å¦å‘½é¢˜è½¬æ¢"""
    prop = prop.strip().replace(' ', '')

    # åŸºæœ¬çš„é€†å¦å‘½é¢˜è½¬æ¢è§„åˆ™
    if prop == 'p->q':
        return '~q->~p'
    elif prop == '~p->q':
        return '~q->p'
    elif prop == 'p->~q':
        return 'q->~p'
    elif prop == '~p->~q':
        return 'q->p'
    else:
        return prop  # æ— æ³•è½¬æ¢æ—¶è¿”å›åŸå‘½é¢˜


class CleanEvaluationSystem:
    """æ¸…ç†åçš„è¯„ä¼°ç³»ç»Ÿ - æ²¡æœ‰è£…æ¨¡ä½œæ ·çš„ä»£ç """
    
    def __init__(self):
        self.tokenizer = Tokenizer()
        self.model = None
        
    def load_model(self, model_path: str) -> bool:
        """åŠ è½½æ¨¡å‹"""
        try:
            self.model = ImprovedSimpleModel(
                vocab_size=self.tokenizer.vocab_size,
                hidden_size=128,
                max_length=50
            )
            self.model.load_model(model_path)
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
            return True
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def evaluate_model_performance(self, test_data: List[Dict], max_samples: int = 100) -> Dict:
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½ - ç›´æ¥ã€æœ‰æ•ˆçš„è¯„ä¼°"""
        if not self.model:
            print("âŒ è¯·å…ˆåŠ è½½æ¨¡å‹")
            return {}
        
        print(f"\nğŸ¯ è¯„ä¼°æ¨¡å‹æ€§èƒ½ (æ ·æœ¬æ•°: {min(len(test_data), max_samples)})")
        
        exact_correct = 0
        logical_correct = 0
        total_samples = min(len(test_data), max_samples)
        
        detailed_results = []
        
        for i, sample in enumerate(test_data[:total_samples]):
            try:
                # æ¨¡å‹é¢„æµ‹
                prediction = self.model.predict(sample['input'], self.tokenizer)
                pred_text = self.tokenizer.decode(prediction).strip()
                target_text = sample['target_text'].strip()
                
                # ç²¾ç¡®åŒ¹é…
                exact_match = pred_text == target_text
                if exact_match:
                    exact_correct += 1
                
                # é€»è¾‘ç­‰ä»·æ€§æ£€æŸ¥
                logical_match = verify_equivalence(pred_text, target_text)
                if logical_match:
                    logical_correct += 1
                
                # è®°å½•è¯¦ç»†ç»“æœ
                detailed_results.append({
                    'input': sample.get('input_text', ''),
                    'target': target_text,
                    'prediction': pred_text,
                    'exact_match': exact_match,
                    'logical_match': logical_match
                })
                
                # è¿›åº¦æ˜¾ç¤º
                if (i + 1) % 20 == 0:
                    print(f"  è¿›åº¦: {i + 1}/{total_samples}")
                    
            except Exception as e:
                print(f"  æ ·æœ¬ {i} è¯„ä¼°å¤±è´¥: {e}")
                continue
        
        # è®¡ç®—å‡†ç¡®ç‡
        exact_accuracy = exact_correct / total_samples
        logical_accuracy = logical_correct / total_samples
        
        results = {
            'exact_accuracy': exact_accuracy,
            'logical_accuracy': logical_accuracy,
            'exact_correct': exact_correct,
            'logical_correct': logical_correct,
            'total_samples': total_samples,
            'detailed_results': detailed_results
        }
        
        print(f"\nğŸ“Š è¯„ä¼°ç»“æœ:")
        print(f"  ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡: {exact_accuracy:.2%} ({exact_correct}/{total_samples})")
        print(f"  é€»è¾‘ç­‰ä»·å‡†ç¡®ç‡: {logical_accuracy:.2%} ({logical_correct}/{total_samples})")
        
        return results
    
    def compare_with_real_baseline(self, test_data: List[Dict], max_samples: int = 100) -> Dict:
        """ä¸çœŸæ­£çš„åŸºçº¿æ–¹æ³•æ¯”è¾ƒ - ä¸æ˜¯è£…æ¨¡ä½œæ ·çš„ç‰ˆæœ¬"""
        print(f"\nğŸ” çœŸå®åŸºçº¿æ¯”è¾ƒ (æ ·æœ¬æ•°: {min(len(test_data), max_samples)})")
        
        total_samples = min(len(test_data), max_samples)
        
        # åŸºçº¿1: éšæœºé¢„æµ‹
        random_correct = 0
        
        # åŸºçº¿2: è§„åˆ™æ–¹æ³• (ä½¿ç”¨çœŸå®çš„è¾“å…¥æ•°æ®)
        rule_correct = 0
        rule_attempted = 0
        
        for sample in test_data[:total_samples]:
            target_text = sample['target_text'].strip()
            input_text = sample.get('input_text', '').strip()
            
            # éšæœºåŸºçº¿ - ä»å¸¸è§çš„é€†å¦å‘½é¢˜æ¨¡å¼ä¸­éšæœºé€‰æ‹©
            common_patterns = ['~p -> ~q', 'q -> p', '~q -> ~p', 'p -> q']
            random_prediction = np.random.choice(common_patterns)
            if random_prediction == target_text:
                random_correct += 1
            
            # è§„åˆ™åŸºçº¿ - ä½¿ç”¨çœŸå®çš„è¾“å…¥æ•°æ®
            if input_text:
                try:
                    rule_prediction = to_contrapositive(input_text)
                    rule_attempted += 1
                    if rule_prediction.strip() == target_text:
                        rule_correct += 1
                except Exception:
                    # è§„åˆ™æ–¹æ³•å¤±è´¥æ—¶ä¸è®¡å…¥
                    pass
        
        # è®¡ç®—åŸºçº¿å‡†ç¡®ç‡
        random_accuracy = random_correct / total_samples
        rule_accuracy = rule_correct / rule_attempted if rule_attempted > 0 else 0.0
        
        baseline_results = {
            'random_accuracy': random_accuracy,
            'rule_accuracy': rule_accuracy,
            'random_correct': random_correct,
            'rule_correct': rule_correct,
            'rule_attempted': rule_attempted,
            'total_samples': total_samples
        }
        
        print(f"ğŸ“Š åŸºçº¿ç»“æœ:")
        print(f"  éšæœºé¢„æµ‹å‡†ç¡®ç‡: {random_accuracy:.2%} ({random_correct}/{total_samples})")
        print(f"  è§„åˆ™æ–¹æ³•å‡†ç¡®ç‡: {rule_accuracy:.2%} ({rule_correct}/{rule_attempted})")
        
        return baseline_results
    
    def analyze_errors(self, evaluation_results: Dict, max_errors: int = 10) -> Dict:
        """é”™è¯¯åˆ†æ - çœŸæ­£æœ‰ç”¨çš„åˆ†æ"""
        print(f"\nğŸ” é”™è¯¯åˆ†æ (æ˜¾ç¤ºå‰{max_errors}ä¸ªé”™è¯¯)")
        
        detailed_results = evaluation_results.get('detailed_results', [])
        errors = [r for r in detailed_results if not r['logical_match']]
        
        error_patterns = {
            'format_error': 0,      # æ ¼å¼é”™è¯¯
            'logic_error': 0,       # é€»è¾‘é”™è¯¯
            'symbol_error': 0,      # ç¬¦å·é”™è¯¯
            'complete_wrong': 0     # å®Œå…¨é”™è¯¯
        }
        
        print(f"é”™è¯¯æ ·æœ¬åˆ†æ:")
        for i, error in enumerate(errors[:max_errors]):
            print(f"\n  é”™è¯¯ {i+1}:")
            print(f"    è¾“å…¥: {error['input']}")
            print(f"    ç›®æ ‡: {error['target']}")
            print(f"    é¢„æµ‹: {error['prediction']}")
            
            # ç®€å•çš„é”™è¯¯åˆ†ç±»
            if not error['prediction']:
                error_patterns['format_error'] += 1
                print(f"    ç±»å‹: æ ¼å¼é”™è¯¯ (ç©ºé¢„æµ‹)")
            elif error['exact_match']:
                error_patterns['logic_error'] += 1
                print(f"    ç±»å‹: é€»è¾‘é”™è¯¯ (æ ¼å¼æ­£ç¡®ä½†é€»è¾‘ä¸ç­‰ä»·)")
            elif any(sym in error['prediction'] for sym in ['p', 'q', '~', '->']):
                error_patterns['symbol_error'] += 1
                print(f"    ç±»å‹: ç¬¦å·é”™è¯¯ (åŒ…å«é€»è¾‘ç¬¦å·ä½†ä¸æ­£ç¡®)")
            else:
                error_patterns['complete_wrong'] += 1
                print(f"    ç±»å‹: å®Œå…¨é”™è¯¯")
        
        # ç»Ÿè®¡æ‰€æœ‰é”™è¯¯çš„æ¨¡å¼
        for error in errors:
            if not error['prediction']:
                error_patterns['format_error'] += 1
            elif error['exact_match']:
                error_patterns['logic_error'] += 1
            elif any(sym in error['prediction'] for sym in ['p', 'q', '~', '->']):
                error_patterns['symbol_error'] += 1
            else:
                error_patterns['complete_wrong'] += 1
        
        print(f"\nğŸ“Š é”™è¯¯æ¨¡å¼ç»Ÿè®¡:")
        total_errors = len(errors)
        for pattern, count in error_patterns.items():
            percentage = (count / total_errors * 100) if total_errors > 0 else 0
            print(f"  {pattern.replace('_', ' ').title()}: {count} ({percentage:.1f}%)")
        
        return {
            'error_patterns': error_patterns,
            'total_errors': total_errors,
            'error_examples': errors[:max_errors]
        }
    
    def comprehensive_evaluation(self, model_path: str, test_data_path: str) -> Dict:
        """ç»¼åˆè¯„ä¼° - ä¸€æ¬¡æ€§å®Œæˆæ‰€æœ‰æœ‰ç”¨çš„è¯„ä¼°"""
        print("ğŸ¯ å¼€å§‹ç»¼åˆè¯„ä¼°")
        print("=" * 60)
        
        # 1. åŠ è½½æ¨¡å‹
        if not self.load_model(model_path):
            return {}
        
        # 2. åŠ è½½æµ‹è¯•æ•°æ®
        try:
            test_data = load_dataset(test_data_path, self.tokenizer, max_samples=200)
            if not test_data:
                print("âŒ æ— æ³•åŠ è½½æµ‹è¯•æ•°æ®")
                return {}
            print(f"âœ… åŠ è½½äº† {len(test_data)} ä¸ªæµ‹è¯•æ ·æœ¬")
        except Exception as e:
            print(f"âŒ æµ‹è¯•æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return {}
        
        # 3. æ¨¡å‹æ€§èƒ½è¯„ä¼°
        model_results = self.evaluate_model_performance(test_data, max_samples=100)
        
        # 4. åŸºçº¿æ¯”è¾ƒ
        baseline_results = self.compare_with_real_baseline(test_data, max_samples=100)
        
        # 5. é”™è¯¯åˆ†æ
        error_analysis = self.analyze_errors(model_results, max_errors=5)
        
        # 6. ç»¼åˆæŠ¥å‘Š
        comprehensive_results = {
            'model_performance': model_results,
            'baseline_comparison': baseline_results,
            'error_analysis': error_analysis,
            'summary': {
                'model_vs_random': model_results['logical_accuracy'] / baseline_results['random_accuracy'] if baseline_results['random_accuracy'] > 0 else float('inf'),
                'model_vs_rule': model_results['logical_accuracy'] / baseline_results['rule_accuracy'] if baseline_results['rule_accuracy'] > 0 else float('inf'),
                'improvement_over_random': (model_results['logical_accuracy'] - baseline_results['random_accuracy']) * 100,
                'improvement_over_rule': (model_results['logical_accuracy'] - baseline_results['rule_accuracy']) * 100
            }
        }
        
        print(f"\nğŸ† ç»¼åˆè¯„ä¼°æ€»ç»“:")
        print(f"  æ¨¡å‹é€»è¾‘å‡†ç¡®ç‡: {model_results['logical_accuracy']:.2%}")
        print(f"  ç›¸æ¯”éšæœºæå‡: {comprehensive_results['summary']['improvement_over_random']:.1f}ä¸ªç™¾åˆ†ç‚¹")
        print(f"  ç›¸æ¯”è§„åˆ™æå‡: {comprehensive_results['summary']['improvement_over_rule']:.1f}ä¸ªç™¾åˆ†ç‚¹")
        
        return comprehensive_results


def main():
    """ä¸»å‡½æ•° - ç›´æ¥æ‰§è¡Œæœ‰ç”¨çš„è¯„ä¼°ï¼Œä¸è£…æ¨¡ä½œæ ·"""
    print("ğŸ§¹ æ¸…ç†åçš„è¯„ä¼°ç³»ç»Ÿ")
    print("å»é™¤æ‰€æœ‰è£…æ¨¡ä½œæ ·çš„ä»£ç ï¼Œåªä¿ç•™çœŸæ­£æœ‰ç”¨çš„åŠŸèƒ½")
    print("=" * 60)
    
    # åˆ›å»ºè¯„ä¼°ç³»ç»Ÿ
    evaluator = CleanEvaluationSystem()
    
    # å¯»æ‰¾æœ€ä½³æ¨¡å‹
    model_candidates = [
        'outputs/breakthrough_training/models/best_breakthrough_model_epoch_23.npz',
        'outputs/trained_models/best_model.npz',
        'outputs/formal_training/models/formal_model_epoch_10.npz'
    ]
    
    best_model = None
    for model_path in model_candidates:
        if os.path.exists(model_path):
            best_model = model_path
            break
    
    if not best_model:
        print("âŒ æ‰¾ä¸åˆ°å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶")
        return
    
    # å¯»æ‰¾æµ‹è¯•æ•°æ®
    test_data_candidates = [
        'data/val_level_3_é²æ£’ç‰ˆ.json',
        'data/val.json',
        'data/val_level_1_é²æ£’ç‰ˆ.json'
    ]
    
    test_data_path = None
    for data_path in test_data_candidates:
        if os.path.exists(data_path):
            test_data_path = data_path
            break
    
    if not test_data_path:
        print("âŒ æ‰¾ä¸åˆ°å¯ç”¨çš„æµ‹è¯•æ•°æ®")
        return
    
    # æ‰§è¡Œç»¼åˆè¯„ä¼°
    results = evaluator.comprehensive_evaluation(best_model, test_data_path)
    
    # ä¿å­˜ç»“æœ
    if results:
        output_path = 'outputs/clean_evaluation_results.json'
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… è¯„ä¼°ç»“æœå·²ä¿å­˜: {output_path}")
        print("\nğŸ‰ æ¸…ç†åçš„è¯„ä¼°å®Œæˆï¼æ²¡æœ‰ä»»ä½•è£…æ¨¡ä½œæ ·çš„ä»£ç ã€‚")


if __name__ == "__main__":
    main()
