package main

import (
	"encoding/json"
	"fmt"
	"log"
	"os"
	"regexp"
	"strings"
	"sync"
	"time"

	"github.com/gocolly/colly/v2"
)

// Judgment ä»£è¡¨ä¸€æ¡"æ¡ˆæƒ…-åˆ¤å†³"å¯¹
type Judgment struct {
	Case     string    `json:"case"`      // å¸–å­æ ‡é¢˜ + æ­£æ–‡
	Verdict  string    `json:"verdict"`   // é«˜èµå›å¤
	Source   string    `json:"source"`    // æ¥æºURL
	Upvotes  int       `json:"upvotes"`   // ç‚¹èµæ•°
	Keywords []string  `json:"keywords"`  // å…³é”®è¯
	CrawlAt  time.Time `json:"crawl_at"`  // æŠ“å–æ—¶é—´
}

// Config çˆ¬è™«é…ç½®
type Config struct {
	TargetForums []string // ç›®æ ‡è´´å§
	MaxPages     int      // æœ€å¤§é¡µæ•°
	Concurrency  int      // å¹¶å‘æ•°
	OutputFile   string   // è¾“å‡ºæ–‡ä»¶
}

var (
	// å…³é”®è¯è¿‡æ»¤å™¨
	keywordPatterns = []string{
		"é‰´å®šä¸º", "çº¯çº¯çš„", "æœ‰ä¸€è¯´ä¸€", "å±äºæ˜¯",
		"é©³å›ä¸Šè¯‰", "å»ºè®®", "èµ›åš", "å…¸ä¸­å…¸",
	}
	
	// å¹¿å‘Šè¿‡æ»¤æ­£åˆ™
	adPattern = regexp.MustCompile(`(åŠ å¾®ä¿¡|æ‰«ç |å¹¿å‘Š|æ¨å¹¿|ä»£ç†)`)
)

func main() {
	config := Config{
		TargetForums: []string{
			"weakintellect",  // å¼±æ™ºå§
			"anti_pressure",  // æŠ—å‹èƒŒé”…å§
			"sunxiaochuan",   // å­™ç¬‘å·å§
		},
		MaxPages:    50,
		Concurrency: 10,
		OutputFile:  "../data/raw/raw_judgments.json",
	}

	log.Println("ğŸš€ èµ›åšè£åˆ¤é•¿ - è¯­æ–™æ å¤ºæ¨¡å—å¯åŠ¨")
	log.Printf("ç›®æ ‡è´´å§: %v\n", config.TargetForums)
	log.Printf("å¹¶å‘æ•°: %d\n", config.Concurrency)

	judgments := crawlJudgments(config)
	
	log.Printf("âœ… æŠ“å–å®Œæˆï¼Œå…±è·å– %d æ¡åˆ¤ä¾‹\n", len(judgments))
	
	if err := saveJudgments(judgments, config.OutputFile); err != nil {
		log.Fatalf("âŒ ä¿å­˜å¤±è´¥: %v", err)
	}
	
	log.Printf("ğŸ’¾ æ•°æ®å·²ä¿å­˜è‡³: %s\n", config.OutputFile)
}

func crawlJudgments(config Config) []Judgment {
	var (
		judgments []Judgment
		mu        sync.Mutex
		wg        sync.WaitGroup
	)

	// åˆ›å»ºæ”¶é›†å™¨
	c := colly.NewCollector(
		colly.Async(true),
		colly.UserAgent("Mozilla/5.0 (compatible; CyberJudge/1.0)"),
	)

	// é™åˆ¶å¹¶å‘
	c.Limit(&colly.LimitRule{
		DomainGlob:  "*",
		Parallelism: config.Concurrency,
		Delay:       1 * time.Second,
	})

	// è§£æå¸–å­åˆ—è¡¨
	c.OnHTML(".thread-item", func(e *colly.HTMLElement) {
		title := e.ChildText(".thread-title")
		link := e.ChildAttr(".thread-link", "href")
		
		if title != "" && link != "" {
			wg.Add(1)
			go func() {
				defer wg.Done()
				if judgment := scrapeThread(link); judgment != nil {
					mu.Lock()
					judgments = append(judgments, *judgment)
					mu.Unlock()
					log.Printf("ğŸ“ æŠ“å–æˆåŠŸ: %s\n", title)
				}
			}()
		}
	})

	// é”™è¯¯å¤„ç†
	c.OnError(func(r *colly.Response, err error) {
		log.Printf("âš ï¸  è¯·æ±‚å¤±è´¥ [%d]: %s\n", r.StatusCode, r.Request.URL)
	})

	// è®¿é—®ç›®æ ‡é¡µé¢
	for _, forum := range config.TargetForums {
		for page := 1; page <= config.MaxPages; page++ {
			url := fmt.Sprintf("https://tieba.baidu.com/f?kw=%s&pn=%d", forum, (page-1)*50)
			c.Visit(url)
		}
	}

	c.Wait()
	wg.Wait()

	return judgments
}

func scrapeThread(url string) *Judgment {
	// TODO: å®ç°å…·ä½“çš„å¸–å­æŠ“å–é€»è¾‘
	// 1. æå–æ ‡é¢˜å’Œæ­£æ–‡ä½œä¸º Case
	// 2. æå–é«˜èµå›å¤ä½œä¸º Verdict
	// 3. è¿‡æ»¤å¹¿å‘Šå’Œæ— æ•ˆå†…å®¹
	// 4. æå–å…³é”®è¯
	
	// è¿™é‡Œæ˜¯ç¤ºä¾‹å®ç°
	return &Judgment{
		Case:     "ç¤ºä¾‹æ¡ˆæƒ…",
		Verdict:  "é‰´å®šä¸ºçº¯çº¯çš„èµ›åšä¹ä¸",
		Source:   url,
		Upvotes:  100,
		Keywords: extractKeywords("é‰´å®šä¸ºçº¯çº¯çš„èµ›åšä¹ä¸"),
		CrawlAt:  time.Now(),
	}
}

func extractKeywords(text string) []string {
	var keywords []string
	for _, pattern := range keywordPatterns {
		if strings.Contains(text, pattern) {
			keywords = append(keywords, pattern)
		}
	}
	return keywords
}

func saveJudgments(judgments []Judgment, filename string) error {
	// ç¡®ä¿ç›®å½•å­˜åœ¨
	os.MkdirAll("../data/raw", 0755)
	
	data, err := json.MarshalIndent(judgments, "", "  ")
	if err != nil {
		return err
	}
	
	return os.WriteFile(filename, data, 0644)
}

