#!/usr/bin/env python3
"""
æ·±åº¦çˆ¬å–é‚¦å¤šåˆ©æ€€å­•å§ - åŒ…å«å›å¤å†…å®¹
é‡ç‚¹ï¼šè·å–åˆ¤æ–­ã€åæ§½ã€è¯„ä»·çš„è¯­æ–™èŒƒå¼
"""

import asyncio
import json
from datetime import datetime
from playwright.async_api import async_playwright


async def wait_for_user(page, message="æŒ‰ Enter ç»§ç»­"):
    """ç­‰å¾…ç”¨æˆ·ç¡®è®¤"""
    print(f"\n{'='*60}")
    print(f"âš ï¸  {message}")
    print(f"{'='*60}\n")
    await asyncio.get_event_loop().run_in_executor(None, input, ">>> ")
    return True


async def extract_thread_detail(context, thread_url):
    """åœ¨æ–°æ ‡ç­¾é¡µä¸­æå–å¸–å­è¯¦æƒ…"""
    detail_page = None
    try:
        # åœ¨æ–°æ ‡ç­¾é¡µæ‰“å¼€
        detail_page = await context.new_page()
        await detail_page.goto(thread_url, timeout=30000)
        await asyncio.sleep(1.5)
        
        # æå–æ¥¼ä¸»å†…å®¹
        op_content = ""
        op_elem = await detail_page.query_selector('.d_post_content')
        if op_elem:
            op_content = await op_elem.inner_text()

        # æå–å›å¤ï¼ˆå‰20æ¡ï¼‰
        replies = []
        reply_elems = await detail_page.query_selector_all('.l_post')
        
        for i, reply_elem in enumerate(reply_elems[:20]):  # åªå–å‰20æ¡å›å¤
            try:
                # æå–å›å¤å†…å®¹
                content_elem = await reply_elem.query_selector('.d_post_content')
                if not content_elem:
                    continue
                
                content = await content_elem.inner_text()
                content = content.strip()
                
                if not content or len(content) < 5:
                    continue
                
                # æå–ä½œè€…
                author_elem = await reply_elem.query_selector('.p_author_name')
                author = await author_elem.inner_text() if author_elem else "åŒ¿å"
                
                replies.append({
                    'author': author.strip(),
                    'content': content,
                    'floor': i + 1
                })
                
            except Exception as e:
                continue
        
        return {
            'op_content': op_content.strip(),
            'replies': replies
        }

    except Exception as e:
        print(f"    âš ï¸  æå–è¯¦æƒ…å¤±è´¥: {e}")
        return None
    finally:
        # å…³é—­è¯¦æƒ…é¡µæ ‡ç­¾
        if detail_page:
            await detail_page.close()


async def extract_threads_from_page(page, context, page_num, crawl_replies=True):
    """ä»å½“å‰é¡µé¢æå–å¸–å­"""
    print(f"\nğŸ” ç¬¬ {page_num} é¡µ - å¼€å§‹æå–å¸–å­...")
    
    await asyncio.sleep(2)
    
    # æŸ¥æ‰¾å¸–å­åˆ—è¡¨
    threads = await page.query_selector_all('li[class*="thread"]')
    
    if not threads:
        print("âŒ æœªæ‰¾åˆ°å¸–å­åˆ—è¡¨ï¼")
        return []
    
    print(f"âœ… æ‰¾åˆ° {len(threads)} ä¸ªå¸–å­")
    
    judgments = []
    
    for i, thread in enumerate(threads[:15]):  # æ¯é¡µå–15ä¸ª
        try:
            # æå–æ ‡é¢˜
            title_elem = await thread.query_selector('a.j_th_tit')
            if not title_elem:
                continue
            
            title = await title_elem.inner_text()
            title = title.strip()
            
            if not title:
                continue
            
            # è·å–å¸–å­é“¾æ¥
            thread_link = await title_elem.get_attribute('href')
            if thread_link and not thread_link.startswith('http'):
                thread_link = f"https://tieba.baidu.com{thread_link}"
            
            # æå–ä½œè€…
            author_elem = await thread.query_selector('.tb_icon_author')
            author = await author_elem.inner_text() if author_elem else "åŒ¿å"
            
            # æå–å›å¤æ•°
            reply_elem = await thread.query_selector('.threadlist_rep_num')
            reply_count = await reply_elem.inner_text() if reply_elem else "0"
            reply_count_int = int(reply_count.strip()) if reply_count.strip().isdigit() else 0
            
            print(f"  [{i+1}] {title[:40]}... (å›å¤: {reply_count})")
            
            # æ„å»ºåŸºç¡€æ•°æ®
            judgment = {
                'title': title,
                'author': author.strip(),
                'reply_count': reply_count_int,
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'source': 'é‚¦å¤šåˆ©æ€€å­•å§',
                'url': thread_link
            }
            
            # å¦‚æœéœ€è¦çˆ¬å–å›å¤ï¼Œåœ¨æ–°æ ‡ç­¾é¡µæ‰“å¼€è¯¦æƒ…
            if crawl_replies and thread_link and reply_count_int > 0:
                print(f"    ğŸ”— åœ¨æ–°æ ‡ç­¾é¡µæ‰“å¼€è¯¦æƒ…...")
                detail = await extract_thread_detail(context, thread_link)

                if detail:
                    judgment['content'] = detail['op_content']
                    judgment['replies'] = detail['replies']
                    print(f"    âœ… è·å–åˆ° {len(detail['replies'])} æ¡å›å¤")
            else:
                judgment['content'] = title
                judgment['replies'] = []
            
            judgments.append(judgment)
            
        except Exception as e:
            print(f"  âš ï¸  æå–ç¬¬ {i+1} ä¸ªå¸–å­å¤±è´¥: {e}")
            continue
    
    print(f"\nâœ… æœ¬é¡µæˆåŠŸæå– {len(judgments)} æ¡æ•°æ®")
    return judgments


async def crawl_bandori_tieba(max_pages=5):
    """æ·±åº¦çˆ¬å–é‚¦å¤šåˆ©æ€€å­•å§"""
    print(f"\nğŸ¯ å¼€å§‹æ·±åº¦çˆ¬å–ï¼šé‚¦å¤šåˆ©æ€€å­•å§")
    print(f"ğŸ“„ è®¡åˆ’çˆ¬å– {max_pages} é¡µï¼ˆåŒ…å«å›å¤å†…å®¹ï¼‰\n")

    async with async_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨ï¼ˆé headlessï¼‰
        browser = await p.chromium.launch(
            headless=False,
            slow_mo=50,
        )

        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            locale='zh-CN',
        )

        page = await context.new_page()

        # ç¬¬ä¸€æ¬¡è®¿é—®
        url = "https://tieba.baidu.com/f?ie=utf-8&kw=é‚¦å¤šåˆ©æ€€å­•"
        print(f"ğŸŒ è®¿é—®: {url}")
        await page.goto(url, timeout=60000)
        await asyncio.sleep(2)

        # ç­‰å¾…ç”¨æˆ·å®ŒæˆéªŒè¯
        await wait_for_user(page, "è¯·å®ŒæˆéªŒè¯ç /ç™»å½•ï¼Œç¡®ä¿èƒ½çœ‹åˆ°å¸–å­åˆ—è¡¨åæŒ‰ Enter")

        all_judgments = []

        # çˆ¬å–å¤šé¡µ
        for page_num in range(1, max_pages + 1):
            if page_num > 1:
                # ç¿»é¡µ
                next_url = f"https://tieba.baidu.com/f?ie=utf-8&kw=é‚¦å¤šåˆ©æ€€å­•&pn={(page_num-1) * 50}"
                print(f"\nğŸ“„ ç¿»åˆ°ç¬¬ {page_num} é¡µ...")
                await page.goto(next_url, timeout=30000)
                await asyncio.sleep(2)

            # æå–æ•°æ®ï¼ˆä¼ å…¥ context ç”¨äºæ‰“å¼€æ–°æ ‡ç­¾é¡µï¼‰
            judgments = await extract_threads_from_page(page, context, page_num, crawl_replies=True)
            all_judgments.extend(judgments)

            # ç»Ÿè®¡
            total_replies = sum(len(j.get('replies', [])) for j in all_judgments)
            print(f"\nğŸ“Š å½“å‰è¿›åº¦: {len(all_judgments)} ä¸ªå¸–å­, {total_replies} æ¡å›å¤")

            # å»¶æ—¶
            if page_num < max_pages:
                delay = 3 + page_num
                print(f"â³ ç­‰å¾… {delay} ç§’åç»§ç»­...")
                await asyncio.sleep(delay)

        # ä¿å­˜ç»“æœ
        output_file = '../data/raw/bandori_judgments.json'
        print(f"\nğŸ’¾ ä¿å­˜æ•°æ®åˆ°: {output_file}")

        import os
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_judgments, f, ensure_ascii=False, indent=2)

        # ç»Ÿè®¡ä¿¡æ¯
        total_replies = sum(len(j.get('replies', [])) for j in all_judgments)
        print(f"\n{'='*60}")
        print(f"ğŸ‰ çˆ¬å–å®Œæˆï¼")
        print(f"ğŸ“Š ç»Ÿè®¡:")
        print(f"   - å¸–å­æ•°: {len(all_judgments)}")
        print(f"   - å›å¤æ•°: {total_replies}")
        print(f"   - æ€»è¯­æ–™: {len(all_judgments) + total_replies} æ¡")
        print(f"   - ä¿å­˜ä½ç½®: {output_file}")
        print(f"{'='*60}\n")

        # ä¿æŒæµè§ˆå™¨æ‰“å¼€
        print("â¸ï¸  æµè§ˆå™¨å°†åœ¨ 10 ç§’åå…³é—­...")
        await asyncio.sleep(10)

        await browser.close()

        return all_judgments


async def main():
    """ä¸»å‡½æ•°"""
    await crawl_bandori_tieba(max_pages=5)


if __name__ == '__main__':
    asyncio.run(main())


