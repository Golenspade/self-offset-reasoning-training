#!/usr/bin/env python3
"""
èµ›åšè£åˆ¤é•¿ - Playwright çˆ¬è™«æ¨¡å—
ä½¿ç”¨çœŸå®æµè§ˆå™¨ç»•è¿‡åçˆ¬è™«æœºåˆ¶
"""

import asyncio
import json
import random
from pathlib import Path
from typing import List, Dict
from playwright.async_api import async_playwright, Page, Browser
from datetime import datetime


class TiebaCrawler:
    """è´´å§çˆ¬è™« - ä½¿ç”¨ Playwright"""
    
    def __init__(self, headless: bool = True):
        self.headless = headless
        self.browser: Browser = None
        self.judgments: List[Dict] = []
        
    async def random_delay(self, min_sec: float = 1.0, max_sec: float = 3.0):
        """éšæœºå»¶æ—¶ï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º"""
        await asyncio.sleep(random.uniform(min_sec, max_sec))
    
    async def init_browser(self, playwright):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        print("ğŸŒ å¯åŠ¨æµè§ˆå™¨...")
        self.browser = await playwright.chromium.launch(
            headless=self.headless,
            args=[
                '--disable-blink-features=AutomationControlled',  # éšè—è‡ªåŠ¨åŒ–ç‰¹å¾
            ]
        )

        # åˆ›å»ºä¸Šä¸‹æ–‡ï¼Œè®¾ç½®çœŸå®çš„æµè§ˆå™¨æŒ‡çº¹
        context = await self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            locale='zh-CN',
            timezone_id='Asia/Shanghai',
        )

        # åŠ è½½ä¿å­˜çš„ Cookieï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        import os
        import json
        cookie_file = 'baidu_cookies.json'
        if os.path.exists(cookie_file):
            print("ğŸª åŠ è½½å·²ä¿å­˜çš„ Cookie...")
            with open(cookie_file, 'r') as f:
                cookies = json.load(f)
                await context.add_cookies(cookies)
                print(f"âœ… å·²åŠ è½½ {len(cookies)} ä¸ª Cookie")

        # æ³¨å…¥åæ£€æµ‹è„šæœ¬
        await context.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
        """)

        return context
    
    async def crawl_tieba_page(self, page: Page, tieba_name: str, page_num: int = 0) -> List[Dict]:
        """çˆ¬å–å•ä¸ªè´´å§é¡µé¢"""
        url = f"https://tieba.baidu.com/f?ie=utf-8&kw={tieba_name}&pn={page_num}"
        
        print(f"ğŸ“„ æ­£åœ¨çˆ¬å–: {tieba_name} ç¬¬ {page_num // 50 + 1} é¡µ")
        
        try:
            # è®¿é—®é¡µé¢
            await page.goto(url, wait_until='networkidle', timeout=30000)
            await self.random_delay(0.5, 1.5)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦éªŒè¯ç 
            page_title = await page.title()
            if "å®‰å…¨éªŒè¯" in page_title or "éªŒè¯" in page_title:
                print(f"  âš ï¸  é‡åˆ°éªŒè¯ç ï¼è¯·å…ˆè¿è¡Œ handle_captcha.py è·å– Cookie")
                return []

            # ç­‰å¾…å†…å®¹åŠ è½½ï¼ˆä½¿ç”¨æ­£ç¡®çš„é€‰æ‹©å™¨ï¼‰
            await page.wait_for_selector('li[class*="thread"]', timeout=10000)

            # æå–å¸–å­åˆ—è¡¨ï¼ˆä½¿ç”¨æ­£ç¡®çš„é€‰æ‹©å™¨ï¼‰
            threads = await page.query_selector_all('li[class*="thread"]')
            
            judgments = []
            for thread in threads[:10]:  # æ¯é¡µåªå–å‰10ä¸ª
                try:
                    # æå–æ ‡é¢˜ï¼ˆä½¿ç”¨æ­£ç¡®çš„é€‰æ‹©å™¨ï¼‰
                    title_elem = await thread.query_selector('a.j_th_tit')
                    if not title_elem:
                        continue
                    title = await title_elem.inner_text()
                    title = title.strip()

                    # è·³è¿‡ç©ºæ ‡é¢˜
                    if not title:
                        continue

                    # æå–ä½œè€…ï¼ˆä½¿ç”¨æ­£ç¡®çš„é€‰æ‹©å™¨ï¼‰
                    author_elem = await thread.query_selector('.tb_icon_author')
                    author = await author_elem.inner_text() if author_elem else 'unknown'
                    author = author.strip()
                    
                    # æå–å›å¤æ•°ï¼ˆä½œä¸ºçƒ­åº¦æŒ‡æ ‡ï¼‰
                    reply_elem = await thread.query_selector('.threadlist_rep_num')
                    replies = await reply_elem.inner_text() if reply_elem else '0'
                    
                    # ç®€å•è¿‡æ»¤ï¼šæ ‡é¢˜ä¸­åŒ…å«åˆ¤æ–­æ€§è¯æ±‡
                    judgment_keywords = ['å—', 'ï¼Ÿ', 'èƒ½', 'ä¼š', 'æ˜¯', 'ä¸', 'æ²¡', 'åˆ«', 'å¤ª']
                    if any(kw in title for kw in judgment_keywords):
                        judgments.append({
                            'title': title.strip(),
                            'content': f"{title.strip()}ï¼ˆéœ€è¦è¿›å…¥è¯¦æƒ…é¡µè·å–å®Œæ•´å†…å®¹ï¼‰",
                            'author': author,
                            'upvotes': int(replies) if replies.isdigit() else 0,
                            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                            'source': url
                        })
                        
                except Exception as e:
                    print(f"  âš ï¸  æå–å¸–å­å¤±è´¥: {e}")
                    continue
            
            print(f"  âœ… æå–åˆ° {len(judgments)} æ¡æ•°æ®")
            return judgments
            
        except Exception as e:
            print(f"  âŒ é¡µé¢çˆ¬å–å¤±è´¥: {e}")
            return []
    
    async def crawl_tieba(self, tieba_name: str, max_pages: int = 3):
        """çˆ¬å–æ•´ä¸ªè´´å§"""
        async with async_playwright() as playwright:
            context = await self.init_browser(playwright)
            page = await context.new_page()
            
            for page_num in range(0, max_pages * 50, 50):
                judgments = await self.crawl_tieba_page(page, tieba_name, page_num)
                self.judgments.extend(judgments)
                
                # éšæœºå»¶æ—¶ï¼Œé¿å…è¢«å°
                await self.random_delay(2.0, 4.0)
            
            await self.browser.close()
    
    async def crawl_multiple_tiebas(self, tieba_list: List[str], pages_per_tieba: int = 2):
        """çˆ¬å–å¤šä¸ªè´´å§"""
        for tieba in tieba_list:
            print(f"\nğŸ¯ å¼€å§‹çˆ¬å–è´´å§: {tieba}")
            await self.crawl_tieba(tieba, max_pages=pages_per_tieba)
            print(f"âœ… {tieba} çˆ¬å–å®Œæˆï¼Œå½“å‰æ€»æ•°æ®: {len(self.judgments)}")
    
    def save_to_json(self, output_file: Path):
        """ä¿å­˜ä¸º JSON"""
        output_file.parent.mkdir(parents=True, exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.judgments, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜: {output_file}")
        print(f"ğŸ“Š æ€»è®¡: {len(self.judgments)} æ¡")


async def main():
    """ä¸»å‡½æ•°"""
    crawler = TiebaCrawler(headless=True)
    
    # ç›®æ ‡è´´å§åˆ—è¡¨ï¼ˆä½¿ç”¨ URL ç¼–ç åçš„åç§°ï¼‰
    tieba_list = [
        "é‚¦å¤šåˆ©æ€€å­•",  # Playwright ä¼šè‡ªåŠ¨å¤„ç† URL ç¼–ç 
        "å¼±æ™º",
        "æŠ—å‹",
    ]
    
    await crawler.crawl_multiple_tiebas(tieba_list, pages_per_tieba=2)
    
    # ä¿å­˜æ•°æ®
    output_file = Path(__file__).parent.parent / 'data' / 'raw' / 'raw_judgments.json'
    crawler.save_to_json(output_file)


if __name__ == '__main__':
    asyncio.run(main())

