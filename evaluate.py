"""
æ–‡ä»¶å: evaluate.py
è¯„ä¼°è„šæœ¬
ç”¨äºå…¨é¢è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹æ€§èƒ½
"""

import json
import numpy as np
import matplotlib.pyplot as plt
from logic_utils import Tokenizer, verify_equivalence, to_contrapositive
from simple_model import load_data
from train import ImprovedSimpleModel


def analyze_predictions(model, data, tokenizer, num_samples=50):
    """
    åˆ†ææ¨¡å‹é¢„æµ‹çš„è¯¦ç»†æƒ…å†µ
    """
    print(f"\n=== é¢„æµ‹åˆ†æ (åˆ†æ {num_samples} ä¸ªæ ·æœ¬) ===")
    
    categories = {
        'perfect_match': [],      # å®Œå…¨åŒ¹é…
        'logical_equivalent': [], # é€»è¾‘ç­‰ä»·ä½†ä¸å®Œå…¨åŒ¹é…
        'partial_correct': [],    # éƒ¨åˆ†æ­£ç¡®
        'completely_wrong': []    # å®Œå…¨é”™è¯¯
    }
    
    for i, sample in enumerate(data[:num_samples]):
        predicted_tokens = model.predict(sample['input'], tokenizer)
        predicted_text = tokenizer.decode(predicted_tokens).strip()
        target_text = sample['target_text'].strip()
        input_text = sample['input_text'].strip()
        
        # åˆ†ç±»é¢„æµ‹ç»“æœ
        if predicted_text == target_text:
            categories['perfect_match'].append((input_text, target_text, predicted_text))
        elif verify_equivalence(predicted_text, target_text):
            categories['logical_equivalent'].append((input_text, target_text, predicted_text))
        elif any(token in predicted_text for token in ['~', '->', '&', '|']):
            categories['partial_correct'].append((input_text, target_text, predicted_text))
        else:
            categories['completely_wrong'].append((input_text, target_text, predicted_text))
    
    # æ‰“å°åˆ†æç»“æœ
    for category, samples in categories.items():
        print(f"\n{category.replace('_', ' ').title()}: {len(samples)} æ ·æœ¬")
        
        # æ˜¾ç¤ºæ¯ä¸ªç±»åˆ«çš„å‰å‡ ä¸ªä¾‹å­
        for j, (inp, target, pred) in enumerate(samples[:3]):
            print(f"  ä¾‹å­ {j+1}:")
            print(f"    è¾“å…¥: {inp}")
            print(f"    ç›®æ ‡: {target}")
            print(f"    é¢„æµ‹: {pred}")
    
    return categories


def test_specific_patterns(model, tokenizer):
    """
    æµ‹è¯•æ¨¡å‹å¯¹ç‰¹å®šé€»è¾‘æ¨¡å¼çš„å¤„ç†èƒ½åŠ›
    """
    print(f"\n=== ç‰¹å®šæ¨¡å¼æµ‹è¯• ===")
    
    test_cases = [
        # ç®€å•å¦å®š
        ("(~p | q)", "p -> q", "~q -> ~p"),
        ("(~q | r)", "q -> r", "~r -> ~q"),
        
        # åŒé‡å¦å®š
        ("(p | ~~q)", "~p -> ~~q", "~q -> p"),
        
        # å¤æ‚è¡¨è¾¾å¼
        ("(~p | ~q)", "p -> ~q", "q -> ~p"),
        ("(~~p | q)", "~p -> q", "~q -> p"),
    ]
    
    correct = 0
    total = len(test_cases)
    
    for i, (noisy_input, original, expected_output) in enumerate(test_cases):
        input_tokens = tokenizer.encode(noisy_input)
        predicted_tokens = model.predict(input_tokens, tokenizer)
        predicted_text = tokenizer.decode(predicted_tokens).strip()
        
        is_correct = (predicted_text == expected_output or 
                     verify_equivalence(predicted_text, expected_output))
        
        if is_correct:
            correct += 1
        
        print(f"\næµ‹è¯• {i+1}:")
        print(f"  å™ªå£°è¾“å…¥: {noisy_input}")
        print(f"  åŸå§‹å‘½é¢˜: {original}")
        print(f"  æœŸæœ›è¾“å‡º: {expected_output}")
        print(f"  æ¨¡å‹é¢„æµ‹: {predicted_text}")
        print(f"  ç»“æœ: {'âœ“' if is_correct else 'âœ—'}")
    
    accuracy = correct / total
    print(f"\nç‰¹å®šæ¨¡å¼æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.2%} ({correct}/{total})")
    
    return accuracy


def error_analysis(model, data, tokenizer, num_samples=100):
    """
    é”™è¯¯åˆ†æï¼šæ‰¾å‡ºæ¨¡å‹å¸¸çŠ¯çš„é”™è¯¯ç±»å‹
    """
    print(f"\n=== é”™è¯¯åˆ†æ ===")
    
    error_types = {
        'wrong_negation': 0,      # å¦å®šé”™è¯¯
        'wrong_direction': 0,     # æ–¹å‘é”™è¯¯ (A->B vs B->A)
        'missing_symbols': 0,     # ç¼ºå°‘ç¬¦å·
        'extra_symbols': 0,       # å¤šä½™ç¬¦å·
        'format_error': 0,        # æ ¼å¼é”™è¯¯
        'other': 0               # å…¶ä»–é”™è¯¯
    }
    
    total_errors = 0
    
    for sample in data[:num_samples]:
        predicted_tokens = model.predict(sample['input'], tokenizer)
        predicted_text = tokenizer.decode(predicted_tokens).strip()
        target_text = sample['target_text'].strip()
        
        if predicted_text != target_text:
            total_errors += 1
            
            # åˆ†æé”™è¯¯ç±»å‹
            if '~' in target_text and '~' not in predicted_text:
                error_types['missing_symbols'] += 1
            elif '~' not in target_text and '~' in predicted_text:
                error_types['extra_symbols'] += 1
            elif '->' not in predicted_text:
                error_types['format_error'] += 1
            else:
                error_types['other'] += 1
    
    print(f"æ€»é”™è¯¯æ•°: {total_errors}/{num_samples}")
    print("é”™è¯¯ç±»å‹åˆ†å¸ƒ:")
    for error_type, count in error_types.items():
        if total_errors > 0:
            percentage = count / total_errors * 100
            print(f"  {error_type.replace('_', ' ').title()}: {count} ({percentage:.1f}%)")
    
    return error_types


# åˆ é™¤äº†è£…æ¨¡ä½œæ ·çš„ benchmark_against_baseline å‡½æ•°
# åŸå‡½æ•°ç¡¬ç¼–ç ç¦ç”¨äº†è§„åˆ™åŸºçº¿ (original_prop = "")ï¼Œæ°¸è¿œè¿”å›0%ï¼Œå®Œå…¨æ— æ•ˆ
# å¦‚æœéœ€è¦çœŸå®çš„åŸºçº¿æ¯”è¾ƒï¼Œè¯·ä½¿ç”¨ clean_evaluation_system.py

def benchmark_against_baseline_REMOVED():
    """
    è¿™ä¸ªå‡½æ•°å·²è¢«åˆ é™¤ï¼Œå› ä¸ºå®ƒæ˜¯è£…æ¨¡ä½œæ ·çš„ä»£ç ï¼š
    - ç¡¬ç¼–ç  original_prop = ""ï¼Œè§„åˆ™åŸºçº¿æ°¸è¿œæ˜¯0%
    - æ²¡æœ‰æä¾›ä»»ä½•æœ‰ç”¨çš„æ¯”è¾ƒä¿¡æ¯
    - è¯·ä½¿ç”¨ clean_evaluation_system.py è¿›è¡ŒçœŸå®çš„åŸºçº¿æ¯”è¾ƒ
    """
    print("âŒ æ­¤å‡½æ•°å·²è¢«åˆ é™¤ï¼Œå› ä¸ºå®ƒæ˜¯è£…æ¨¡ä½œæ ·çš„ä»£ç ")
    print("ğŸ’¡ è¯·ä½¿ç”¨ clean_evaluation_system.py è¿›è¡ŒçœŸå®çš„è¯„ä¼°")
    return 0.0, 0.0


def visualize_training_history():
    """
    å¯è§†åŒ–è®­ç»ƒå†å²
    """
    try:
        with open('training_history.json', 'r') as f:
            history = json.load(f)
        
        epochs = [h['epoch'] for h in history]
        losses = [h['loss'] for h in history]
        
        plt.figure(figsize=(10, 6))
        
        plt.subplot(1, 2, 1)
        plt.plot(epochs, losses, 'b-', linewidth=2)
        plt.title('è®­ç»ƒæŸå¤±')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.grid(True)
        
        # å¦‚æœæœ‰éªŒè¯å‡†ç¡®ç‡æ•°æ®
        val_accuracies = [h.get('val_accuracy') for h in history if h.get('val_accuracy') is not None]
        val_epochs = [h['epoch'] for h in history if h.get('val_accuracy') is not None]
        
        if val_accuracies:
            plt.subplot(1, 2, 2)
            plt.plot(val_epochs, val_accuracies, 'r-', linewidth=2)
            plt.title('éªŒè¯å‡†ç¡®ç‡')
            plt.xlabel('Epoch')
            plt.ylabel('Accuracy')
            plt.grid(True)
        
        plt.tight_layout()
        plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("è®­ç»ƒæ›²çº¿å·²ä¿å­˜ä¸º training_curves.png")
        
    except FileNotFoundError:
        print("æœªæ‰¾åˆ°è®­ç»ƒå†å²æ–‡ä»¶")


def comprehensive_evaluation():
    """
    ç»¼åˆè¯„ä¼°å‡½æ•°
    """
    print("=== è‡ªåç§»æ¨ç†è®­ç»ƒ - ç»¼åˆè¯„ä¼° ===\n")
    
    # åˆå§‹åŒ–
    tokenizer = Tokenizer()
    
    # åˆ›å»ºæ¨¡å‹å¹¶åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
    model = ImprovedSimpleModel(
        vocab_size=tokenizer.vocab_size,
        hidden_size=128,
        max_length=50,
        learning_rate=0.005
    )

    # å°è¯•åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡
    if not model.load_model('trained_model.npz'):
        print("è­¦å‘Š: æ— æ³•åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æƒé‡")
        print("è¯·å…ˆè¿è¡Œ train.py æ¥è®­ç»ƒæ¨¡å‹")
    
    # åŠ è½½æ•°æ®
    print("åŠ è½½è¯„ä¼°æ•°æ®...")
    val_data = load_data('data/val.json', tokenizer, max_samples=200)
    print(f"è¯„ä¼°æ•°æ®: {len(val_data)} æ ·æœ¬")
    
    # 1. åŸºæœ¬æ€§èƒ½è¯„ä¼°
    print("\n1. åŸºæœ¬æ€§èƒ½è¯„ä¼°")
    correct_exact = 0
    correct_logical = 0
    
    for sample in val_data[:100]:
        predicted_tokens = model.predict(sample['input'], tokenizer)
        predicted_text = tokenizer.decode(predicted_tokens).strip()
        target_text = sample['target_text'].strip()
        
        if predicted_text == target_text:
            correct_exact += 1
            correct_logical += 1
        elif verify_equivalence(predicted_text, target_text):
            correct_logical += 1
    
    exact_accuracy = correct_exact / 100
    logical_accuracy = correct_logical / 100
    
    print(f"ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡: {exact_accuracy:.2%}")
    print(f"é€»è¾‘ç­‰ä»·å‡†ç¡®ç‡: {logical_accuracy:.2%}")
    
    # 2. é¢„æµ‹åˆ†æ
    categories = analyze_predictions(model, val_data, tokenizer, 50)
    
    # 3. ç‰¹å®šæ¨¡å¼æµ‹è¯•
    pattern_accuracy = test_specific_patterns(model, tokenizer)
    
    # 4. é”™è¯¯åˆ†æ
    error_types = error_analysis(model, val_data, tokenizer, 100)
    
    # 5. åŸºçº¿æ¯”è¾ƒ (å·²åˆ é™¤è£…æ¨¡ä½œæ ·çš„å‡½æ•°)
    print("\nâš ï¸ åŸåŸºçº¿æ¯”è¾ƒå‡½æ•°å·²åˆ é™¤ (è£…æ¨¡ä½œæ ·çš„ä»£ç )")
    print("ğŸ’¡ å¦‚éœ€çœŸå®åŸºçº¿æ¯”è¾ƒï¼Œè¯·ä½¿ç”¨: python3 clean_evaluation_system.py")
    random_acc, rule_acc = 0.0, 0.0  # å ä½ç¬¦
    
    # 6. å¯è§†åŒ–è®­ç»ƒå†å²
    visualize_training_history()
    
    # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
    report = {
        'exact_accuracy': exact_accuracy,
        'logical_accuracy': logical_accuracy,
        'pattern_accuracy': pattern_accuracy,
        'random_baseline': random_acc,
        'rule_baseline': rule_acc,
        'error_analysis': error_types,
        'prediction_categories': {k: len(v) for k, v in categories.items()}
    }
    
    with open('evaluation_report.json', 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"\n=== è¯„ä¼°æ€»ç»“ ===")
    print(f"æ¨¡å‹åœ¨é€»è¾‘ç­‰ä»·å‡†ç¡®ç‡ä¸Šè¾¾åˆ°äº† {logical_accuracy:.2%}")
    print(f"è¿™æ¯”éšæœºåŸºçº¿ ({random_acc:.2%}) é«˜å‡º {logical_accuracy - random_acc:.2%}")
    print(f"è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ° evaluation_report.json")
    
    return report


if __name__ == "__main__":
    comprehensive_evaluation()
