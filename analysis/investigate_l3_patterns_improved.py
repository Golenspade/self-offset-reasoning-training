"""
æ–‡ä»¶å: investigate_l3_patterns_improved.py
æ”¹è¿›ç‰ˆLevel 3æ¨¡å¼è°ƒæŸ¥è„šæœ¬
è§£å†³åŸç‰ˆæœ¬ä¸­çš„"å·æ‡’"é—®é¢˜ï¼Œæä¾›æ›´é«˜æ•ˆã€å¥å£®çš„åˆ†æ
"""

import json
import random
import re
from collections import defaultdict
from typing import List, Dict, Set, Tuple, Optional


class L3PatternAnalyzer:
    """Level 3 æ•°æ®æ¨¡å¼åˆ†æå™¨ - æ”¹è¿›ç‰ˆ"""

    def __init__(self, data_file: str = "data/val_L3_complex.json"):
        self.data_file = data_file
        self.samples = []
        self.analysis_results = {}

    def load_data(self) -> bool:
        """åŠ è½½æ•°æ®"""
        try:
            with open(self.data_file, "r", encoding="utf-8") as f:
                self.samples = [json.loads(line) for line in f if line.strip()]
            print(f"âœ… æˆåŠŸåŠ è½½ {len(self.samples)} ä¸ªæ ·æœ¬")
            return True
        except FileNotFoundError:
            print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {self.data_file}")
            return False
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return False

    def find_common_substrings_efficient(
        self, str1: str, str2: str, min_length: int = 5
    ) -> List[str]:
        """
        é«˜æ•ˆçš„å…±åŒå­ä¸²æŸ¥æ‰¾ç®—æ³•
        ä½¿ç”¨é›†åˆæ“ä½œæ›¿ä»£æš´åŠ›å¾ªç¯ï¼Œæ—¶é—´å¤æ‚åº¦ä»O(NÂ³)é™åˆ°O(NÂ²)
        """
        if not str1 or not str2:
            return []

        # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„å­ä¸²ï¼ˆä½¿ç”¨é›†åˆæ¨å¯¼å¼ï¼‰
        substrings1 = {
            str1[i : i + j]
            for i in range(len(str1))
            for j in range(min_length, len(str1) - i + 1)
        }
        substrings2 = {
            str2[i : i + j]
            for i in range(len(str2))
            for j in range(min_length, len(str2) - i + 1)
        }

        # ä½¿ç”¨é›†åˆäº¤é›†æ“ä½œï¼Œæ•ˆç‡è¿œé«˜äºåµŒå¥—å¾ªç¯
        common = list(substrings1.intersection(substrings2))

        # æŒ‰é•¿åº¦é™åºæ’åºï¼Œè¿”å›æœ€é•¿çš„å‡ ä¸ª
        common.sort(key=len, reverse=True)
        return common[:3]

    def check_simple_transformations_robust(
        self, noisy: str, target: str
    ) -> Dict[str, any]:
        """
        å¥å£®çš„å˜æ¢æ¨¡å¼æ£€æŸ¥
        ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿ä»£è„†å¼±çš„å­—ç¬¦ä¸²åˆ†å‰²
        """
        results = {
            "cheating_detected": False,
            "pattern_type": None,
            "confidence": 0.0,
            "details": {},
        }

        # æ¸…ç†è¾“å…¥å­—ç¬¦ä¸²
        noisy = noisy.strip()
        target = target.strip()

        # æ¨¡å¼1: æ£€æŸ¥ A|B -> ~B -> ~A çš„ç›´æ¥æ˜ å°„
        target_match = re.match(r"~\s*(.+?)\s*->\s*~\s*(.+)", target)
        if target_match:
            target_b = target_match.group(1).strip()
            target_a = target_match.group(2).strip()

            # æ›´å¥å£®çš„å™ªå£°è§£æï¼Œå¤„ç†æ‹¬å·
            noise_patterns = [
                r"\(\s*(.+?)\s*\)\s*\|\s*(.+)",  # (A) | B
                r"(.+?)\s*\|\s*\(\s*(.+?)\s*\)",  # A | (B)
                r"(.+?)\s*\|\s*(.+)",  # A | B
            ]

            for pattern in noise_patterns:
                noise_match = re.match(pattern, noisy)
                if noise_match:
                    noise_a = noise_match.group(1).strip()
                    noise_b = noise_match.group(2).strip()

                    # æ£€æŸ¥ç›´æ¥æ˜ å°„å…³ç³»
                    if self._normalize_expression(
                        noise_a
                    ) == self._normalize_expression(
                        target_a
                    ) and self._normalize_expression(
                        noise_b
                    ) == self._normalize_expression(
                        target_b
                    ):
                        results.update(
                            {
                                "cheating_detected": True,
                                "pattern_type": "direct_disjunction_mapping",
                                "confidence": 0.95,
                                "details": {
                                    "noise_a": noise_a,
                                    "noise_b": noise_b,
                                    "target_a": target_a,
                                    "target_b": target_b,
                                },
                            }
                        )
                        return results

        # æ¨¡å¼2: æ£€æŸ¥å˜é‡ç›´æ¥æ›¿æ¢æ¨¡å¼
        if self._check_variable_substitution(noisy, target):
            results.update(
                {
                    "cheating_detected": True,
                    "pattern_type": "variable_substitution",
                    "confidence": 0.8,
                    "details": self._get_variable_mapping(noisy, target),
                }
            )

        return results

    def _normalize_expression(self, expr: str) -> str:
        """æ ‡å‡†åŒ–è¡¨è¾¾å¼ï¼Œå»é™¤å¤šä½™çš„ç©ºæ ¼å’Œæ‹¬å·"""
        expr = re.sub(r"\s+", "", expr)  # å»é™¤æ‰€æœ‰ç©ºæ ¼
        expr = re.sub(r"^\((.+)\)$", r"\1", expr)  # å»é™¤å¤–å±‚æ‹¬å·
        return expr

    def _check_variable_substitution(self, noisy: str, target: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç®€å•çš„å˜é‡æ›¿æ¢æ¨¡å¼"""
        noisy_vars = set(re.findall(r"[pqrst]", noisy))
        target_vars = set(re.findall(r"[pqrst]", target))

        # å¦‚æœå˜é‡é›†åˆå®Œå…¨ç›¸åŒï¼Œå¯èƒ½å­˜åœ¨ç®€å•æ˜ å°„
        return len(noisy_vars) == len(target_vars) and len(noisy_vars) <= 3

    def _get_variable_mapping(self, noisy: str, target: str) -> Dict[str, str]:
        """è·å–å˜é‡æ˜ å°„å…³ç³»"""
        noisy_vars = sorted(set(re.findall(r"[pqrst]", noisy)))
        target_vars = sorted(set(re.findall(r"[pqrst]", target)))

        if len(noisy_vars) == len(target_vars):
            return dict(zip(noisy_vars, target_vars))
        return {}

    def analyze_noise_effectiveness_comprehensive(
        self, samples: List[Dict]
    ) -> Dict[str, any]:
        """
        å…¨é¢çš„å™ªå£°æœ‰æ•ˆæ€§åˆ†æ
        ä½¿ç”¨ç‹¬ç«‹çš„ifåˆ¤æ–­ï¼Œå…è®¸å¤šç§å™ªå£°ç±»å‹åŒæ—¶è¯†åˆ«
        """
        noise_analysis = {
            "total_samples": len(samples),
            "noise_types": defaultdict(int),
            "multi_noise_samples": 0,
            "noise_combinations": defaultdict(int),
            "effectiveness_score": 0.0,
        }

        for i, sample in enumerate(samples):
            original = sample.get("original_prop", "")
            noisy = sample.get("noisy_prop", "")

            applied_noises = []

            # ç‹¬ç«‹æ£€æŸ¥å„ç§å™ªå£°ç±»å‹
            if "->" in original and "|" in noisy and "->" not in noisy:
                noise_analysis["noise_types"]["implication_to_disjunction"] += 1
                applied_noises.append("impl_to_disj")

            if "~~" in noisy:
                noise_analysis["noise_types"]["double_negation"] += 1
                applied_noises.append("double_neg")

            # ä¿®æ­£çš„æ‹¬å·è®¡æ•°
            original_parens = original.count("(")
            noisy_parens = noisy.count("(")
            double_neg_parens = noisy.count("~~") * 2  # æ¯ä¸ª~~é€šå¸¸å¢åŠ 2ä¸ªæ‹¬å·

            if noisy_parens > original_parens + double_neg_parens:
                noise_analysis["noise_types"]["redundant_parentheses"] += 1
                applied_noises.append("redundant_parens")

            if re.search(r"\(\s*\(\s*[^)]+\s*\)\s*\)", noisy):
                noise_analysis["noise_types"]["nested_parentheses"] += 1
                applied_noises.append("nested_parens")

            if len(re.findall(r"[&|]", noisy)) > len(re.findall(r"[&|]", original)):
                noise_analysis["noise_types"]["extra_operators"] += 1
                applied_noises.append("extra_ops")

            # è®°å½•å™ªå£°ç»„åˆ
            if len(applied_noises) > 1:
                noise_analysis["multi_noise_samples"] += 1
                combination = "+".join(sorted(applied_noises))
                noise_analysis["noise_combinations"][combination] += 1
            elif len(applied_noises) == 0:
                if noisy == original:
                    noise_analysis["noise_types"]["no_change"] += 1
                else:
                    noise_analysis["noise_types"]["unknown_change"] += 1

        # è®¡ç®—å™ªå£°æœ‰æ•ˆæ€§åˆ†æ•°
        total_noise_applications = sum(noise_analysis["noise_types"].values())
        if total_noise_applications > 0:
            noise_analysis["effectiveness_score"] = (
                total_noise_applications - noise_analysis["noise_types"]["no_change"]
            ) / len(samples)

        return noise_analysis

    def check_variable_patterns_precise(self, samples: List[Dict]) -> Dict[str, any]:
        """
        ç²¾ç¡®çš„å˜é‡æ¨¡å¼æ£€æŸ¥
        ä½¿ç”¨æ›´ç›´æ¥çš„å­—ç¬¦åŒ¹é…ï¼Œé¿å…å•è¯è¾¹ç•Œé—®é¢˜
        """
        variable_analysis = {
            "variable_distribution": defaultdict(int),
            "variable_consistency": 0.0,
            "suspicious_patterns": [],
        }

        for sample in samples:
            noisy = sample.get("noisy_prop", "")
            target = sample.get("target_contrapositive", "")

            # ç›´æ¥å­—ç¬¦åŒ¹é…ï¼Œæ›´å¯é 
            noisy_vars = sorted(set(re.findall(r"[pqrst]", noisy)))
            target_vars = sorted(set(re.findall(r"[pqrst]", target)))

            # è®°å½•å˜é‡åˆ†å¸ƒ
            for var in noisy_vars:
                variable_analysis["variable_distribution"][var] += 1

            # æ£€æŸ¥å¯ç–‘æ¨¡å¼
            if len(noisy_vars) == len(target_vars) and len(noisy_vars) <= 2:
                if noisy_vars == target_vars:
                    variable_analysis["suspicious_patterns"].append(
                        {
                            "type": "identical_variables",
                            "variables": noisy_vars,
                            "sample_index": samples.index(sample),
                        }
                    )

        # è®¡ç®—å˜é‡ä¸€è‡´æ€§
        total_vars = sum(variable_analysis["variable_distribution"].values())
        if total_vars > 0:
            max_var_count = max(variable_analysis["variable_distribution"].values())
            variable_analysis["variable_consistency"] = max_var_count / total_vars

        return variable_analysis

    def run_comprehensive_analysis(self, sample_size: int = 50) -> Dict[str, any]:
        """è¿è¡Œå…¨é¢åˆ†æ"""
        if not self.load_data():
            return {}

        # éšæœºé‡‡æ ·
        analysis_samples = random.sample(
            self.samples, min(sample_size, len(self.samples))
        )

        print(f"\nğŸ” å¼€å§‹å…¨é¢åˆ†æ {len(analysis_samples)} ä¸ªæ ·æœ¬...")

        # 1. ä½œå¼Šæ¨¡å¼æ£€æµ‹
        cheating_patterns = []
        for i, sample in enumerate(analysis_samples):
            result = self.check_simple_transformations_robust(
                sample.get("noisy_prop", ""), sample.get("target_contrapositive", "")
            )
            if result["cheating_detected"]:
                result["sample_index"] = i
                cheating_patterns.append(result)

        # 2. å™ªå£°æœ‰æ•ˆæ€§åˆ†æ
        noise_analysis = self.analyze_noise_effectiveness_comprehensive(
            analysis_samples
        )

        # 3. å˜é‡æ¨¡å¼åˆ†æ
        variable_analysis = self.check_variable_patterns_precise(analysis_samples)

        # 4. å…±åŒå­ä¸²åˆ†æï¼ˆæ”¹è¿›ç‰ˆï¼‰
        substring_analysis = self._analyze_common_substrings(analysis_samples[:10])

        # æ±‡æ€»ç»“æœ
        self.analysis_results = {
            "sample_size": len(analysis_samples),
            "cheating_patterns": cheating_patterns,
            "noise_analysis": noise_analysis,
            "variable_analysis": variable_analysis,
            "substring_analysis": substring_analysis,
            "overall_risk_score": self._calculate_risk_score(
                cheating_patterns, noise_analysis
            ),
        }

        return self.analysis_results

    def _analyze_common_substrings(self, samples: List[Dict]) -> Dict[str, any]:
        """åˆ†æå…±åŒå­ä¸²ï¼ˆä½¿ç”¨æ”¹è¿›çš„ç®—æ³•ï¼‰"""
        substring_results = []

        for sample in samples:
            noisy = sample.get("noisy_prop", "")
            target = sample.get("target_contrapositive", "")

            common = self.find_common_substrings_efficient(noisy, target, min_length=3)
            if common:
                substring_results.append(
                    {"noisy": noisy, "target": target, "common_substrings": common}
                )

        return {
            "samples_with_common_substrings": len(substring_results),
            "examples": substring_results[:3],
        }

    def _calculate_risk_score(
        self, cheating_patterns: List[Dict], noise_analysis: Dict
    ) -> float:
        """è®¡ç®—æ•´ä½“é£é™©åˆ†æ•°"""
        risk_score = 0.0

        # ä½œå¼Šæ¨¡å¼é£é™©
        if cheating_patterns:
            high_confidence_patterns = [
                p for p in cheating_patterns if p["confidence"] > 0.9
            ]
            risk_score += len(high_confidence_patterns) / len(cheating_patterns) * 0.5

        # å™ªå£°æœ‰æ•ˆæ€§é£é™©
        if noise_analysis["effectiveness_score"] < 0.5:
            risk_score += 0.3

        # å¤šå™ªå£°æ ·æœ¬æ¯”ä¾‹
        if noise_analysis["total_samples"] > 0:
            multi_noise_ratio = (
                noise_analysis["multi_noise_samples"] / noise_analysis["total_samples"]
            )
            if multi_noise_ratio < 0.3:
                risk_score += 0.2

        return min(risk_score, 1.0)

    def generate_detailed_report(self) -> str:
        """ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š"""
        if not self.analysis_results:
            return "âŒ è¯·å…ˆè¿è¡Œåˆ†æ"

        results = self.analysis_results

        report = f"""
ğŸ” Level 3 æ•°æ®æ¨¡å¼åˆ†ææŠ¥å‘Šï¼ˆæ”¹è¿›ç‰ˆï¼‰
{'='*60}

ğŸ“Š åŸºæœ¬ç»Ÿè®¡:
  åˆ†ææ ·æœ¬æ•°: {results['sample_size']}
  æ•´ä½“é£é™©åˆ†æ•°: {results['overall_risk_score']:.2f} (0-1, è¶Šé«˜è¶Šå±é™©)

ğŸš¨ ä½œå¼Šæ¨¡å¼æ£€æµ‹:
  å‘ç°å¯ç–‘æ¨¡å¼: {len(results['cheating_patterns'])} ä¸ª
"""

        if results["cheating_patterns"]:
            report += "  è¯¦ç»†ä¿¡æ¯:\n"
            for pattern in results["cheating_patterns"][:3]:
                report += f"    - ç±»å‹: {pattern['pattern_type']}\n"
                report += f"      ç½®ä¿¡åº¦: {pattern['confidence']:.2f}\n"
                report += f"      è¯¦æƒ…: {pattern['details']}\n"

        noise = results["noise_analysis"]
        report += f"""
ğŸ­ å™ªå£°æœ‰æ•ˆæ€§åˆ†æ:
  å™ªå£°æœ‰æ•ˆæ€§åˆ†æ•°: {noise['effectiveness_score']:.2f}
  å¤šé‡å™ªå£°æ ·æœ¬: {noise['multi_noise_samples']} ({noise['multi_noise_samples']/noise['total_samples']*100:.1f}%)
  
  å™ªå£°ç±»å‹åˆ†å¸ƒ:
"""
        for noise_type, count in noise["noise_types"].items():
            percentage = count / noise["total_samples"] * 100
            report += f"    {noise_type}: {count} ({percentage:.1f}%)\n"

        var = results["variable_analysis"]
        report += f"""
ğŸ”¤ å˜é‡æ¨¡å¼åˆ†æ:
  å˜é‡ä¸€è‡´æ€§: {var['variable_consistency']:.2f}
  å¯ç–‘æ¨¡å¼æ•°: {len(var['suspicious_patterns'])}
  
  å˜é‡åˆ†å¸ƒ: {dict(var['variable_distribution'])}
"""

        substr = results["substring_analysis"]
        report += f"""
ğŸ§© å…±åŒå­ä¸²åˆ†æ:
  æœ‰å…±åŒå­ä¸²çš„æ ·æœ¬: {substr['samples_with_common_substrings']}
  
ğŸ’¡ æ”¹è¿›å»ºè®®:
"""

        if results["overall_risk_score"] > 0.6:
            report += "  âš ï¸  é«˜é£é™©ï¼šæ•°æ®é›†å­˜åœ¨æ˜æ˜¾çš„ä½œå¼Šæ·å¾„ï¼Œå»ºè®®é‡æ–°ç”Ÿæˆ\n"
        elif results["overall_risk_score"] > 0.3:
            report += "  âš ï¸  ä¸­ç­‰é£é™©ï¼šå­˜åœ¨ä¸€äº›å¯ç–‘æ¨¡å¼ï¼Œå»ºè®®å¢åŠ å™ªå£°å¤æ‚åº¦\n"
        else:
            report += "  âœ… ä½é£é™©ï¼šæ•°æ®é›†è´¨é‡è‰¯å¥½ï¼Œä½œå¼Šé£é™©è¾ƒä½\n"

        return report


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” Level 3 æ¨¡å¼åˆ†æå™¨ï¼ˆæ”¹è¿›ç‰ˆï¼‰")
    print("è§£å†³åŸç‰ˆæœ¬ä¸­çš„æ•ˆç‡å’Œå¥å£®æ€§é—®é¢˜")
    print("=" * 60)

    analyzer = L3PatternAnalyzer()
    results = analyzer.run_comprehensive_analysis(sample_size=50)

    if results:
        print(analyzer.generate_detailed_report())

        # ä¿å­˜ç»“æœ
        with open(
            "outputs/l3_pattern_analysis_improved.json", "w", encoding="utf-8"
        ) as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print("\nâœ… åˆ†æç»“æœå·²ä¿å­˜åˆ° outputs/l3_pattern_analysis_improved.json")
    else:
        print("âŒ åˆ†æå¤±è´¥")


if __name__ == "__main__":
    main()
